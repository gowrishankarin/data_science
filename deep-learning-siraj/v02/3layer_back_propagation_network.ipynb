{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "import numpy as np\n",
    "np.seterr(over = 'ignore')\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # eery time the program runs\n",
    "        np.random.seed(1) # Seed the random number generator\n",
    "        self.weights = {} # Create dict to hold weights\n",
    "        self.num_layers = 1 # Set initial number of lyaer to one (input layer)\n",
    "        self.adjustments = {} # Create dict to hold adjustments\n",
    "\n",
    "    def add_layer(self, shape):\n",
    "        # Create weights with shape specified + biases\n",
    "        self.weights[self.num_layers] = np.vstack((2*np.random.random(shape) - 1, \n",
    "                                                   2*np.random.random((1, shape[1])) - 1))\n",
    "        # Initialize the adjustments for these weights to zero\n",
    "        self.adjustments[self.num_layers] = np.zeros(shape)\n",
    "        self.num_layers += 1        \n",
    "        \n",
    "    # The sigmoid function, which describes an S shaped curve\n",
    "    # We pass the weighted sum of the inputs thtrough this funciton to \n",
    "    # normalize them between 0 and 1\n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1 + exp(-x))\n",
    "    \n",
    "    # The derivative of the Sigmoid function\n",
    "    # This is the gradient of the Sigmoid curve\n",
    "    # It indicates how confident we are about the existing weight\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        # Pass data enough through pre-trained network\n",
    "        for layer in range(1, self.num_layers+1):\n",
    "            data = np.dot(data, self.weights[layer-1][:, :-1]) + self.weights[layer - 1][:, -1] # + self.biases[layer]\n",
    "            data = self.__sigmoid(data)\n",
    "        return(data)\n",
    "    \n",
    "    def __forward_propagate(self, data):\n",
    "        # Propagate through network and hold values for use in back-propagation\n",
    "        activation_values = {}\n",
    "        activation_values[1] = data\n",
    "        for layer in range(2, self.num_layers + 1):\n",
    "            data = np.dot(data.T, self.weights[layer-1][:-1, :]) + self.weights[layer - 1][-1, :].T # + self.biases[layer]\n",
    "            data = self.__sigmoid(data).T\n",
    "            activation_values[layer] =data\n",
    "        return activation_values\n",
    "    \n",
    "    def simple_error(self, outputs, targets):\n",
    "        return targets - outputs\n",
    "    \n",
    "    def sum_squared_error(self, outputs, targets):\n",
    "        return 0.5 * np.mean(np.sum(np.power(outputs - targets, 2), axis=1))\n",
    "    \n",
    "    def __back_propagate(self, output, target):\n",
    "        deltas = {}\n",
    "        # Delta of output layer\n",
    "        deltas[self.num_layers] = output[self.num_layers] - target\n",
    "        \n",
    "        # Delta of hidden layers\n",
    "        for layer in reversed(range(2, self.num_layers)): # All layers except input/output\n",
    "            a_val = output[layer]\n",
    "            weights = self.weights[layer][:-1, :]\n",
    "            prev_deltas = deltas[layer + 1]\n",
    "            deltas[layer] = np.multiply(np.dot(weights, prev_deltas), self.__sigmoid_derivative(a_val))\n",
    "            \n",
    "        # Calculate total adjustments based on deltas\n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.adjustments[layer] += np.dot(deltas[layer+1], output[layer].T).T\n",
    "            \n",
    "    def __gradient_descente(self, batch_size, learning_rate):\n",
    "        # Calculate partial derivative and take a step in that direction\n",
    "        for layer in range(1, self.num_layers):\n",
    "            partial_d = (1/batch_size) * self.adjustments[layer]\n",
    "            self.weights[layer][:-1, :] += learning_rate * -partial_d\n",
    "            self.weights[layer][-1, :] += learning_rate*1e-3 * -partial_d[-1, :]\n",
    "    \n",
    "    # WE train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time\n",
    "    def train(self, inputs, targets, num_epochs, learning_rate=1, stop_accuracy=1e-5):\n",
    "        error = []\n",
    "        for iteration in range(num_epochs):\n",
    "            for i in range(len(inputs)):\n",
    "                x = inputs[i]\n",
    "                y = targets[i]\n",
    "                \n",
    "                # Pass the training set through our neural network\n",
    "                output = self.__forward_propagate(x)\n",
    "                \n",
    "                # Calculate the error\n",
    "                loss = self.sum_squared_error(output[self.num_layers], y)\n",
    "                error.append(loss)\n",
    "                \n",
    "                # Calculate adjustments\n",
    "                self.__back_propagate(output, y)\n",
    "                \n",
    "            self.__gradient_descente(i, learning_rate)\n",
    "            \n",
    "            # Check if accuracy criterion is satisfied\n",
    "            if np.mean(error[-(i+1):]) < stop_accuracy and iteration > 0:\n",
    "                break\n",
    "        return(np.asarray(error), iteration+1)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Error = ', 0.12919882118364129)\n",
      "('Epoches needed to train = ', 5000)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # ----------- XOR Function ----------------\n",
    "    \n",
    "    # initialize a single neuron neural network\n",
    "    nn = NeuralNetwork()\n",
    "    \n",
    "    # Add Layers (Input layer is created by default)\n",
    "    nn.add_layer((2, 9))\n",
    "    nn.add_layer((9, 1))\n",
    "    \n",
    "    # XOR Functions\n",
    "    training_data = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape(4, 2, 1)\n",
    "    training_labels = np.asarray([[0], [1], [1], [0]])\n",
    "    \n",
    "    error, iteration = nn.train(training_data, training_labels, 5000)\n",
    "    print('Error = ', np.mean(error[-4:]))\n",
    "    print('Epoches needed to train = ', iteration)\n",
    "    \n",
    "    # nn.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "np.vstack((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dstack((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[-0.16595599,  0.44064899],\n",
       "        [-0.99977125, -0.39533485],\n",
       "        [-0.70648822, -0.81532281]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[ 0.,  0.],\n",
       "        [ 0.,  0.]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16595599,  0.44064899, -0.99977125, -0.39533485],\n",
       "       [-0.81532281, -0.62747958, -0.30887855, -0.20646505],\n",
       "       [-0.16161097,  0.370439  , -0.5910955 ,  0.75623487],\n",
       "       [ 0.34093502, -0.1653904 ,  0.11737966, -0.71922612],\n",
       "       [ 0.60148914,  0.93652315, -0.37315164,  0.38464523],\n",
       "       [ 0.78921333, -0.82991158, -0.92189043, -0.66033916]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.weights[1][:,:-1]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16595599,  0.44064899, -0.99977125, -0.39533485, -0.70648822],\n",
       "       [-0.81532281, -0.62747958, -0.30887855, -0.20646505,  0.07763347],\n",
       "       [-0.16161097,  0.370439  , -0.5910955 ,  0.75623487, -0.94522481],\n",
       "       [ 0.34093502, -0.1653904 ,  0.11737966, -0.71922612, -0.60379702],\n",
       "       [ 0.60148914,  0.93652315, -0.37315164,  0.38464523,  0.7527783 ],\n",
       "       [ 0.78921333, -0.82991158, -0.92189043, -0.66033916,  0.75628501]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.70648822,  0.07763347, -0.94522481, -0.60379702,  0.7527783 ,\n",
       "        0.75628501])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights[1][:, -1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
