{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # Initialize the list of weights matrices, then store the\n",
    "        # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Start looping from the index of the first layer but\n",
    "        # stop before we reach teh last two layers\n",
    "        for i in np.arange(0, len(layers)-2):\n",
    "            # Randomly initialize a weight matrix conneting the \n",
    "            # number of nodes in each respective layer together\n",
    "            # adding a extra node for the bias\n",
    "            w = np.random.randn(layers[i]+1, layers[i+1]+1)\n",
    "            self.W.append(w/np.sqrt(layers[i]))\n",
    "            \n",
    "        # The last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2]+1, layers[-1])\n",
    "        self.W.append(w/np.sqrt(layers[-2]))\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # Construct and return a string that represents the network architecture\n",
    "        return \"NeuralNetwork: {}\".format(\n",
    "            \"-\".join(str(l) for l in self.layers))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Compute and return the sigmoid activation value for a given input value\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        # Compute the derivative of the sigmoid function assuming that `x` has \n",
    "        # already been passed through the `sigmoid` function\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        # Insert a column of 1s as the last entry in the feature matrix -- this \n",
    "        # little trick alloows us to treat the bias as a trainable parameter within \n",
    "        # the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        \n",
    "        # loop over the desired number of epochs \n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # Loop over each individual data point and train our network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "                \n",
    "            # Check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                    epoch + 1, loss))\n",
    "                \n",
    "    def fit_partial(self, x, y):\n",
    "        # Construct our list of output activations for each layer as our data point\n",
    "        # flows through the network; the first activation is a special case -- its \n",
    "        # just the input feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        # FeedForward:\n",
    "        # Loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Feedforward the activation at the current layer by takeing the dot product\n",
    "            # between the activation and the weight matrix -- this is called the \"net \n",
    "            # input\" to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            \n",
    "            # Computing the \"net output\" is simply applying our non-linear activation \n",
    "            # function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "            \n",
    "            # Once we have the net output, add it to our list of activations\n",
    "            A.append(out)\n",
    "            \n",
    "        # Backpropagation\n",
    "        \"\"\"\n",
    "        The first phase of the BP is to compute the differences between our *prediction*\n",
    "        (the final ouput activation in the activations list) and the true target value\n",
    "        \"\"\"\n",
    "        error = A[-1] - y\n",
    "        \n",
    "        \"\"\"\n",
    "        From here, we need to apply the chain rule and build our list of deltas 'D'; the \n",
    "        first entry in the deltas is simply the error of the output layer times the derivative\n",
    "        of our activation function for the ouput value\n",
    "        \"\"\"\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "        \n",
    "        \"\"\"\n",
    "        Once you understand the chain rule ut becomes super easy to implement with a `for` loop\n",
    "        Simply loop over the layers in reverse order (ignoring the last two since we already have\n",
    "        taken them into account)\n",
    "        \"\"\"\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            \"\"\"\n",
    "            The delta for the current layer is equal to the delta of the *previous layer* dotted \n",
    "            with the weight matrix of the current layer, followed by multiplying the delta by the\n",
    "            derivative of the non-linear activation function for the activations of the current layer\n",
    "            \"\"\"\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "            \n",
    "        # Since we looped over our layers in reverse order we need to reverse the deltas\n",
    "        D = D[::-1]\n",
    "        \n",
    "        # Weight Update Phase\n",
    "        # Loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Update our weights by taking the dot product of the layer activations with \n",
    "            # their respective deltas, then multiplying this value by some small learning\n",
    "            # rate and adding to our weight matrix. -- this is where the actual \"learning\"\n",
    "            # takes place\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "            \n",
    "    def predict(self, X, addBias=True):\n",
    "        # Initialize the output prediction as the input features -- this value will be\n",
    "        # forward propagated through the network to obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # Check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # Insert a column of 1s as the last entry in the feature matrix(bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "            \n",
    "            \n",
    "        # Loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Computing the ouput prediction is as simple as taking the dot product between\n",
    "            # the current activation value `p` and the weight matrix associated with the \n",
    "            # current layer, then passing this value through a non-linear activation function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "            \n",
    "        # Return the predicted value \n",
    "        return p\n",
    "    \n",
    "    def calculate_loss(self, X, targets):\n",
    "        # Make predictions for the input data points then compute the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        \n",
    "        # return the loss\n",
    "        return loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.17879955, -1.77508576,  0.42450692],\n",
       "       [-0.91843769, -1.14079721,  2.28744076],\n",
       "       [-1.26140933,  0.35172004,  1.49340923]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in np.arange(0, 3-2)]\n",
    "w = np.random.randn(2 + 1, 2 + 1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.5040686\n",
      "[INFO] epoch=100, loss=0.5004561\n",
      "[INFO] epoch=200, loss=0.5003506\n",
      "[INFO] epoch=300, loss=0.5002659\n",
      "[INFO] epoch=400, loss=0.5001971\n",
      "[INFO] epoch=500, loss=0.5001408\n",
      "[INFO] epoch=600, loss=0.5000946\n",
      "[INFO] epoch=700, loss=0.5000568\n",
      "[INFO] epoch=800, loss=0.5000259\n",
      "[INFO] epoch=900, loss=0.5000009\n",
      "[INFO] epoch=1000, loss=0.4999808\n",
      "[INFO] epoch=1100, loss=0.4999647\n",
      "[INFO] epoch=1200, loss=0.4999519\n",
      "[INFO] epoch=1300, loss=0.4999418\n",
      "[INFO] epoch=1400, loss=0.4999339\n",
      "[INFO] epoch=1500, loss=0.4999277\n",
      "[INFO] epoch=1600, loss=0.4999227\n",
      "[INFO] epoch=1700, loss=0.4999187\n",
      "[INFO] epoch=1800, loss=0.4999153\n",
      "[INFO] epoch=1900, loss=0.4999124\n",
      "[INFO] epoch=2000, loss=0.4999098\n",
      "[INFO] epoch=2100, loss=0.4999073\n",
      "[INFO] epoch=2200, loss=0.4999049\n",
      "[INFO] epoch=2300, loss=0.4999024\n",
      "[INFO] epoch=2400, loss=0.4998997\n",
      "[INFO] epoch=2500, loss=0.4998969\n",
      "[INFO] epoch=2600, loss=0.4998939\n",
      "[INFO] epoch=2700, loss=0.4998906\n",
      "[INFO] epoch=2800, loss=0.4998870\n",
      "[INFO] epoch=2900, loss=0.4998831\n",
      "[INFO] epoch=3000, loss=0.4998788\n",
      "[INFO] epoch=3100, loss=0.4998742\n",
      "[INFO] epoch=3200, loss=0.4998692\n",
      "[INFO] epoch=3300, loss=0.4998638\n",
      "[INFO] epoch=3400, loss=0.4998579\n",
      "[INFO] epoch=3500, loss=0.4998515\n",
      "[INFO] epoch=3600, loss=0.4998445\n",
      "[INFO] epoch=3700, loss=0.4998369\n",
      "[INFO] epoch=3800, loss=0.4998286\n",
      "[INFO] epoch=3900, loss=0.4998197\n",
      "[INFO] epoch=4000, loss=0.4998099\n",
      "[INFO] epoch=4100, loss=0.4997991\n",
      "[INFO] epoch=4200, loss=0.4997874\n",
      "[INFO] epoch=4300, loss=0.4997746\n",
      "[INFO] epoch=4400, loss=0.4997604\n",
      "[INFO] epoch=4500, loss=0.4997449\n",
      "[INFO] epoch=4600, loss=0.4997277\n",
      "[INFO] epoch=4700, loss=0.4997087\n",
      "[INFO] epoch=4800, loss=0.4996875\n",
      "[INFO] epoch=4900, loss=0.4996639\n",
      "[INFO] epoch=5000, loss=0.4996375\n",
      "[INFO] epoch=5100, loss=0.4996079\n",
      "[INFO] epoch=5200, loss=0.4995745\n",
      "[INFO] epoch=5300, loss=0.4995367\n",
      "[INFO] epoch=5400, loss=0.4994937\n",
      "[INFO] epoch=5500, loss=0.4994445\n",
      "[INFO] epoch=5600, loss=0.4993878\n",
      "[INFO] epoch=5700, loss=0.4993221\n",
      "[INFO] epoch=5800, loss=0.4992454\n",
      "[INFO] epoch=5900, loss=0.4991551\n",
      "[INFO] epoch=6000, loss=0.4990477\n",
      "[INFO] epoch=6100, loss=0.4989185\n",
      "[INFO] epoch=6200, loss=0.4987610\n",
      "[INFO] epoch=6300, loss=0.4985660\n",
      "[INFO] epoch=6400, loss=0.4983201\n",
      "[INFO] epoch=6500, loss=0.4980026\n",
      "[INFO] epoch=6600, loss=0.4975811\n",
      "[INFO] epoch=6700, loss=0.4970012\n",
      "[INFO] epoch=6800, loss=0.4961659\n",
      "[INFO] epoch=6900, loss=0.4948882\n",
      "[INFO] epoch=7000, loss=0.4927678\n",
      "[INFO] epoch=7100, loss=0.4888308\n",
      "[INFO] epoch=7200, loss=0.4803185\n",
      "[INFO] epoch=7300, loss=0.4585300\n",
      "[INFO] epoch=7400, loss=0.4028144\n",
      "[INFO] epoch=7500, loss=0.2828219\n",
      "[INFO] epoch=7600, loss=0.0532950\n",
      "[INFO] epoch=7700, loss=0.0140519\n",
      "[INFO] epoch=7800, loss=0.0070692\n",
      "[INFO] epoch=7900, loss=0.0045308\n",
      "[INFO] epoch=8000, loss=0.0032722\n",
      "[INFO] epoch=8100, loss=0.0025350\n",
      "[INFO] epoch=8200, loss=0.0020559\n",
      "[INFO] epoch=8300, loss=0.0017220\n",
      "[INFO] epoch=8400, loss=0.0014771\n",
      "[INFO] epoch=8500, loss=0.0012903\n",
      "[INFO] epoch=8600, loss=0.0011436\n",
      "[INFO] epoch=8700, loss=0.0010256\n",
      "[INFO] epoch=8800, loss=0.0009286\n",
      "[INFO] epoch=8900, loss=0.0008477\n",
      "[INFO] epoch=9000, loss=0.0007792\n",
      "[INFO] epoch=9100, loss=0.0007206\n",
      "[INFO] epoch=9200, loss=0.0006698\n",
      "[INFO] epoch=9300, loss=0.0006254\n",
      "[INFO] epoch=9400, loss=0.0005864\n",
      "[INFO] epoch=9500, loss=0.0005517\n",
      "[INFO] epoch=9600, loss=0.0005208\n",
      "[INFO] epoch=9700, loss=0.0004930\n",
      "[INFO] epoch=9800, loss=0.0004679\n",
      "[INFO] epoch=9900, loss=0.0004452\n",
      "[INFO] epoch=10000, loss=0.0004245\n",
      "[INFO] epoch=10100, loss=0.0004056\n",
      "[INFO] epoch=10200, loss=0.0003882\n",
      "[INFO] epoch=10300, loss=0.0003722\n",
      "[INFO] epoch=10400, loss=0.0003574\n",
      "[INFO] epoch=10500, loss=0.0003438\n",
      "[INFO] epoch=10600, loss=0.0003310\n",
      "[INFO] epoch=10700, loss=0.0003192\n",
      "[INFO] epoch=10800, loss=0.0003082\n",
      "[INFO] epoch=10900, loss=0.0002978\n",
      "[INFO] epoch=11000, loss=0.0002882\n",
      "[INFO] epoch=11100, loss=0.0002791\n",
      "[INFO] epoch=11200, loss=0.0002705\n",
      "[INFO] epoch=11300, loss=0.0002625\n",
      "[INFO] epoch=11400, loss=0.0002548\n",
      "[INFO] epoch=11500, loss=0.0002477\n",
      "[INFO] epoch=11600, loss=0.0002408\n",
      "[INFO] epoch=11700, loss=0.0002344\n",
      "[INFO] epoch=11800, loss=0.0002282\n",
      "[INFO] epoch=11900, loss=0.0002224\n",
      "[INFO] epoch=12000, loss=0.0002169\n",
      "[INFO] epoch=12100, loss=0.0002116\n",
      "[INFO] epoch=12200, loss=0.0002065\n",
      "[INFO] epoch=12300, loss=0.0002017\n",
      "[INFO] epoch=12400, loss=0.0001971\n",
      "[INFO] epoch=12500, loss=0.0001927\n",
      "[INFO] epoch=12600, loss=0.0001885\n",
      "[INFO] epoch=12700, loss=0.0001845\n",
      "[INFO] epoch=12800, loss=0.0001806\n",
      "[INFO] epoch=12900, loss=0.0001769\n",
      "[INFO] epoch=13000, loss=0.0001733\n",
      "[INFO] epoch=13100, loss=0.0001699\n",
      "[INFO] epoch=13200, loss=0.0001666\n",
      "[INFO] epoch=13300, loss=0.0001634\n",
      "[INFO] epoch=13400, loss=0.0001603\n",
      "[INFO] epoch=13500, loss=0.0001574\n",
      "[INFO] epoch=13600, loss=0.0001545\n",
      "[INFO] epoch=13700, loss=0.0001517\n",
      "[INFO] epoch=13800, loss=0.0001491\n",
      "[INFO] epoch=13900, loss=0.0001465\n",
      "[INFO] epoch=14000, loss=0.0001440\n",
      "[INFO] epoch=14100, loss=0.0001416\n",
      "[INFO] epoch=14200, loss=0.0001393\n",
      "[INFO] epoch=14300, loss=0.0001370\n",
      "[INFO] epoch=14400, loss=0.0001348\n",
      "[INFO] epoch=14500, loss=0.0001327\n",
      "[INFO] epoch=14600, loss=0.0001307\n",
      "[INFO] epoch=14700, loss=0.0001287\n",
      "[INFO] epoch=14800, loss=0.0001267\n",
      "[INFO] epoch=14900, loss=0.0001249\n",
      "[INFO] epoch=15000, loss=0.0001230\n",
      "[INFO] epoch=15100, loss=0.0001213\n",
      "[INFO] epoch=15200, loss=0.0001195\n",
      "[INFO] epoch=15300, loss=0.0001179\n",
      "[INFO] epoch=15400, loss=0.0001162\n",
      "[INFO] epoch=15500, loss=0.0001146\n",
      "[INFO] epoch=15600, loss=0.0001131\n",
      "[INFO] epoch=15700, loss=0.0001116\n",
      "[INFO] epoch=15800, loss=0.0001101\n",
      "[INFO] epoch=15900, loss=0.0001087\n",
      "[INFO] epoch=16000, loss=0.0001073\n",
      "[INFO] epoch=16100, loss=0.0001059\n",
      "[INFO] epoch=16200, loss=0.0001046\n",
      "[INFO] epoch=16300, loss=0.0001033\n",
      "[INFO] epoch=16400, loss=0.0001020\n",
      "[INFO] epoch=16500, loss=0.0001008\n",
      "[INFO] epoch=16600, loss=0.0000996\n",
      "[INFO] epoch=16700, loss=0.0000984\n",
      "[INFO] epoch=16800, loss=0.0000973\n",
      "[INFO] epoch=16900, loss=0.0000961\n",
      "[INFO] epoch=17000, loss=0.0000950\n",
      "[INFO] epoch=17100, loss=0.0000939\n",
      "[INFO] epoch=17200, loss=0.0000929\n",
      "[INFO] epoch=17300, loss=0.0000919\n",
      "[INFO] epoch=17400, loss=0.0000909\n",
      "[INFO] epoch=17500, loss=0.0000899\n",
      "[INFO] epoch=17600, loss=0.0000889\n",
      "[INFO] epoch=17700, loss=0.0000880\n",
      "[INFO] epoch=17800, loss=0.0000870\n",
      "[INFO] epoch=17900, loss=0.0000861\n",
      "[INFO] epoch=18000, loss=0.0000852\n",
      "[INFO] epoch=18100, loss=0.0000844\n",
      "[INFO] epoch=18200, loss=0.0000835\n",
      "[INFO] epoch=18300, loss=0.0000827\n",
      "[INFO] epoch=18400, loss=0.0000819\n",
      "[INFO] epoch=18500, loss=0.0000810\n",
      "[INFO] epoch=18600, loss=0.0000803\n",
      "[INFO] epoch=18700, loss=0.0000795\n",
      "[INFO] epoch=18800, loss=0.0000787\n",
      "[INFO] epoch=18900, loss=0.0000780\n",
      "[INFO] epoch=19000, loss=0.0000772\n",
      "[INFO] epoch=19100, loss=0.0000765\n",
      "[INFO] epoch=19200, loss=0.0000758\n",
      "[INFO] epoch=19300, loss=0.0000751\n",
      "[INFO] epoch=19400, loss=0.0000744\n",
      "[INFO] epoch=19500, loss=0.0000738\n",
      "[INFO] epoch=19600, loss=0.0000731\n",
      "[INFO] epoch=19700, loss=0.0000725\n",
      "[INFO] epoch=19800, loss=0.0000718\n",
      "[INFO] epoch=19900, loss=0.0000712\n",
      "[INFO] epoch=20000, loss=0.0000706\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2, 2, 2, 2, 1], alpha=0.5)\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn.fit(X, y, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0.0076, step=0\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0.9872, step=1\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0.9898, step=1\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0.0127, step=0\n"
     ]
    }
   ],
   "source": [
    "for(x, target) in zip(X, y):\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
    "        x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
