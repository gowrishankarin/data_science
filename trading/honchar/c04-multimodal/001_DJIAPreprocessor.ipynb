{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Modal Learning\n",
    "\n",
    "### Information\n",
    "\n",
    " - Information in the real world coomes as different modalities\n",
    " - Ex. Images are associated with tags and text explanations\n",
    " - Ex. Texts contain images to express an idea clearly\n",
    " - Thus different modalities\n",
    " - Different modalities are characterised by different statistical properties\n",
    " - Ex. Images are represented as Pixel Intensities\n",
    " - Ex. Texts are represented as discrete word count vectors\n",
    " - Thus they have distinct statiscal properties\n",
    " - IT IS IMPORTANT TO DISCOVER THE RELATIONSHIP BETWEEN DIFFERENT MODALITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HELP_PATH = '/Users/shankar/dev/code/ds/studies/data_science/trading/honchar'\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(IMG_HELP_PATH))\n",
    "from common.preprocessing import data_2_percentage_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_csv(filename='Combined_News_DJIA.csv', date_split=date(2014, 12, 31)):\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Combined'] = df.iloc[:, 2:27].apply(lambda row: ''.join(str(row.values)), axis=1)\n",
    "    \n",
    "    train = df.loc[(pd.to_datetime(df[\"Date\"]) <= date_split), ['Label', 'Combined']]\n",
    "    test = df.loc[(pd.to_datetime(df[\"Date\"]) > date_split), ['Label', 'Combined']]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_text_csv(filename='../datasets/stocknews/Combined_News_DJIA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['b\"Georgia \\'downs two Russian warplanes\\' as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"b'Why wont America and Nato help us? If they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[\"b'Remember that adorable 9-year-old who sang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[\"b' U.S. refuses Israel weapons to attack Ira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"b'All the experts admit that we should legal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                           Combined\n",
       "0      0  ['b\"Georgia \\'downs two Russian warplanes\\' as...\n",
       "1      1  [\"b'Why wont America and Nato help us? If they...\n",
       "2      0  [\"b'Remember that adorable 9-year-old who sang...\n",
       "3      0  [\"b' U.S. refuses Israel weapons to attack Ira...\n",
       "4      1  [\"b'All the experts admit that we should legal..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>1</td>\n",
       "      <td>['Most cases of cancer are the result of sheer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>0</td>\n",
       "      <td>['Moscow-&amp;gt;Beijing high speed train will red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>0</td>\n",
       "      <td>['US oil falls below $50 a barrel'\\n \"Toyota g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"'Shots fired' at French magazine HQ\"\\n '90% ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>1</td>\n",
       "      <td>['New Charlie Hebdo issue to come out next wee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                           Combined\n",
       "1611      1  ['Most cases of cancer are the result of sheer...\n",
       "1612      0  ['Moscow-&gt;Beijing high speed train will red...\n",
       "1613      0  ['US oil falls below $50 a barrel'\\n \"Toyota g...\n",
       "1614      1  [\"'Shots fired' at French magazine HQ\"\\n '90% ...\n",
       "1615      1  ['New Charlie Hebdo issue to come out next wee..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
       "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
       "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
       "      <td>b'Russian tanks are moving towards the capital...</td>\n",
       "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
       "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
       "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
       "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Georgia Invades South Ossetia - if Russia ge...</td>\n",
       "      <td>b'Al-Qaeda Faces Islamist Backlash'</td>\n",
       "      <td>b'Condoleezza Rice: \"The US would not act to p...</td>\n",
       "      <td>b'This is a busy day:  The European Union has ...</td>\n",
       "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
       "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
       "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
       "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
       "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
       "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
       "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
       "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
       "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
       "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
       "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
       "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
       "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Israel and the US behind the Georgian aggres...</td>\n",
       "      <td>b'\"Do not believe TV, neither Russian nor Geor...</td>\n",
       "      <td>b'Riots are still going on in Montreal (Canada...</td>\n",
       "      <td>b'China to overtake US as largest manufacturer'</td>\n",
       "      <td>b'War in South Ossetia [PICS]'</td>\n",
       "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
       "      <td>b' Russia has just beaten the United States ov...</td>\n",
       "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
       "      <td>b'Russia is so much better at war'</td>\n",
       "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2008-08-08      0  b\"Georgia 'downs two Russian warplanes' as cou...   \n",
       "1  2008-08-11      1  b'Why wont America and Nato help us? If they w...   \n",
       "\n",
       "                                          Top2  \\\n",
       "0      b'BREAKING: Musharraf to be impeached.'   \n",
       "1  b'Bush puts foot down on Georgian conflict'   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  b'Russia Today: Columns of troops roll into So...   \n",
       "1  b\"Jewish Georgian minister: Thanks to Israeli ...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  b'Russian tanks are moving towards the capital...   \n",
       "1  b'Georgian army flees in disarray as Russians ...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  b\"Afghan children raped with 'impunity,' U.N. ...   \n",
       "1      b\"Olympic opening ceremony fireworks 'faked'\"   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  b'150 Russian tanks have entered South Ossetia...   \n",
       "1  b'What were the Mossad with fraudulent New Zea...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  b\"Breaking: Georgia invades South Ossetia, Rus...   \n",
       "1  b'Russia angered by Israeli military sale to G...   \n",
       "\n",
       "                                                Top8  \\\n",
       "0  b\"The 'enemy combatent' trials are nothing but...   \n",
       "1  b'An American citizen living in S.Ossetia blam...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "\n",
       "                                               Top16  \\\n",
       "0  b'Georgia Invades South Ossetia - if Russia ge...   \n",
       "1  b'Israel and the US behind the Georgian aggres...   \n",
       "\n",
       "                                               Top17  \\\n",
       "0                b'Al-Qaeda Faces Islamist Backlash'   \n",
       "1  b'\"Do not believe TV, neither Russian nor Geor...   \n",
       "\n",
       "                                               Top18  \\\n",
       "0  b'Condoleezza Rice: \"The US would not act to p...   \n",
       "1  b'Riots are still going on in Montreal (Canada...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  b'This is a busy day:  The European Union has ...   \n",
       "1    b'China to overtake US as largest manufacturer'   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  b\"Georgia will withdraw 1,000 soldiers from Ir...   \n",
       "1                     b'War in South Ossetia [PICS]'   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  b'Why the Pentagon Thinks Attacking Iran is a ...   \n",
       "1  b'Israeli Physicians Group Condemns State Tort...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  b'Caucasus in crisis: Georgia invades South Os...   \n",
       "1  b' Russia has just beaten the United States ov...   \n",
       "\n",
       "                                               Top23  \\\n",
       "0  b'Indian shoe manufactory  - And again in a se...   \n",
       "1  b'Perhaps *the* question about the Georgia - R...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  b'Visitors Suffering from Mental Illnesses Ban...   \n",
       "1                 b'Russia is so much better at war'   \n",
       "\n",
       "                                               Top25  \n",
       "0           b\"No Help for Mexico's Kidnapping Surge\"  \n",
       "1  b\"So this is what it's come to: trading sex fo...  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/stocknews/Combined_News_DJIA.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ts_csv(filename='DJIA_table.csv', date_split=date(2014, 12, 31)):\n",
    "    data = pd.read_csv(filename)[::-1]\n",
    "    \n",
    "    train2 = data.loc[(pd.to_datetime(data[\"Date\"]) <= date_split)]\n",
    "    test2 = data.loc[(pd.to_datetime(data[\"Date\"]) > date_split)]\n",
    "    \n",
    "    open_train = train2.loc[:, 'Open']\n",
    "    open_test = test2.loc[:, 'Open']\n",
    "    open_train = data_2_percentage_change(open_train)\n",
    "    open_test = data_2_percentage_change(open_test)\n",
    "    \n",
    "    high_train = train2.loc[:, 'High']\n",
    "    high_test = test2.loc[:, 'High']\n",
    "    high_train = data_2_percentage_change(high_train)\n",
    "    high_test = data_2_percentage_change(high_test)\n",
    "    \n",
    "    low_train = train2.loc[:, 'Low']\n",
    "    low_test = test2.loc[:, 'Low']\n",
    "    low_train = data_2_percentage_change(low_train)\n",
    "    low_test = data_2_percentage_change(low_test)\n",
    "    \n",
    "    close_train = train2.loc[:, 'Close']\n",
    "    close_test = test2.loc[:, 'Close']\n",
    "    close_train = data_2_percentage_change(close_train)\n",
    "    close_test = data_2_percentage_change(close_test)\n",
    "    \n",
    "    volume_train = train2.loc[:, 'Volume']\n",
    "    volume_test = test2.loc[:, 'Volume']\n",
    "    volume_train = data_2_percentage_change(volume_train)\n",
    "    volume_test = data_2_percentage_change(volume_test)\n",
    "    \n",
    "    train = np.column_stack((open_train, high_train, low_train, close_train, volume_train))\n",
    "    test = np.column_stack((open_test, high_test, low_test, close_test, volume_test))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "      <td>18002.380859</td>\n",
       "      <td>17916.910156</td>\n",
       "      <td>17949.369141</td>\n",
       "      <td>82160000</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "      <td>17930.609375</td>\n",
       "      <td>17711.800781</td>\n",
       "      <td>17929.990234</td>\n",
       "      <td>133030000</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17704.509766</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17694.679688</td>\n",
       "      <td>106380000</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>112190000</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17063.080078</td>\n",
       "      <td>17140.240234</td>\n",
       "      <td>138740000</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
       "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
       "2  2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688   \n",
       "3  2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703   \n",
       "4  2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234   \n",
       "\n",
       "      Volume     Adj Close  \n",
       "0   82160000  17949.369141  \n",
       "1  133030000  17929.990234  \n",
       "2  106380000  17694.679688  \n",
       "3  112190000  17409.720703  \n",
       "4  138740000  17140.240234  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/stocknews/DJIA_table.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00],\n",
       "       [ 2.60302431e-02,  9.11145883e-03,  2.52449265e-02,\n",
       "         4.09306170e-03, -1.39266081e-01],\n",
       "       [ 4.43578322e-03, -7.14249219e-03, -6.33896194e-03,\n",
       "        -1.18719854e-02, -5.24046072e-02],\n",
       "       [-1.26374481e-02, -1.26094829e-02, -1.27724378e-02,\n",
       "        -9.40605976e-03,  5.16158765e-02],\n",
       "       [-8.65992496e-03,  7.26333127e-03, -2.13928516e-04,\n",
       "         7.19413986e-03, -1.24678170e-01],\n",
       "       [ 6.86257080e-03, -7.16028701e-04,  1.29981884e-02,\n",
       "         3.78537958e-03,  3.45766318e-01],\n",
       "       [ 4.17186755e-03, -1.66183974e-03, -1.42770862e-02,\n",
       "        -1.54813281e-02, -2.73204985e-01],\n",
       "       [-1.55716973e-02, -1.81567121e-02, -1.01118508e-02,\n",
       "        -1.13978048e-02,  9.78309553e-02],\n",
       "       [-1.15131887e-02, -2.09262724e-03, -2.46675107e-03,\n",
       "         6.06948766e-03, -1.55612542e-01],\n",
       "       [ 6.10703356e-03,  1.92590190e-03,  2.21337025e-03,\n",
       "         1.11936516e-03, -1.02567642e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = load_ts_csv(filename='../datasets/stocknews/DJIA_table.csv')\n",
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following\n",
    "    1. Tokenizes and removes punctuation\n",
    "    2. Removes stopwords\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    '''\n",
    "    if(pd.isnull(text)):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Removing any stopwords\n",
    "    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # Stemming\n",
    "    porterStemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porterStemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "    try:\n",
    "        text_processed.remove('b')\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \" \".join(text_processed)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take string text perform follow take\n",
      "['Takes', 'in', 'a', 'string', 's', 'of', 'text', 'then', 'performs', 'between', 'the', 'ourselves', 'following', 'the', 'who', 'that', 'take']\n",
      "['takes', 'string', 'text', 'performs', 'following', 'take']\n",
      "['take', 'string', 'text', 'perform', 'follow', 'take']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Takes in a string's of text, then performs between the ourselves following the who that take\"\n",
    "text = text_process(sample_text)\n",
    "print(text)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text_processed = tokenizer.tokenize(sample_text)\n",
    "print(text_processed)\n",
    "text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n",
    "print(text_processed)\n",
    "porterStemmer = PorterStemmer()\n",
    "text_processed = [porterStemmer.stem(word) for word in text_processed]\n",
    "print(text_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_2_sentences(train, test, save_train='train_text.p', save_text='test_text.p'):\n",
    "    '''\n",
    "    Transforming raw text into sentences, if @save_train or @save_test in not None - saves\n",
    "    pickles for further use\n",
    "    '''\n",
    "    train_text = []\n",
    "    test_text = []\n",
    "    for each in train['Combined']:\n",
    "        train_text.append(text_process(each))\n",
    "    \n",
    "    for each in test['Combined']:\n",
    "        test_text.append(text_process(each))\n",
    "        \n",
    "    if(save_train != None):\n",
    "        cPickle.dump(train_text, open(save_train, 'wb'))\n",
    "        \n",
    "    if(save_test != None):\n",
    "        cPickle.dump(test_text, open(save_text, 'wb'))\n",
    "        \n",
    "    return train_text, test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_into_vectors(train_text, test_text, embedding_size=100, model_path='../output/word2vec10.model'):\n",
    "    \n",
    "    '''\n",
    "    Transforms sentences into sequences of word2vec vectors. Returns\n",
    "    train, test set and trained word2vec model\n",
    "    '''\n",
    "    \n",
    "    data_for_w2v = []\n",
    "    for text in train_text + test_text:\n",
    "        words = text.split(' ')\n",
    "        data_for_w2v.append(words)\n",
    "        \n",
    "    model = Word2Vect(data_for_w2v, size=embedding_size, window=5, min_count=1, workers=4)\n",
    "    model.save(model_path)\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    train_text_vectors = [[model[x] for x in sentence.split(' ')] for sentence in train_text]\n",
    "    test_text_vectors = [[model[x] for x in sentence.split(' ')] for sentence in test_text]\n",
    "    \n",
    "    \n",
    "    train_text_vectors = [np.mean(x, axis=0) for x in train_text_vectors]\n",
    "    test_text_vectors = [np.mean(x, axis=0) for x in test_text_vectors]\n",
    "    \n",
    "    return train_text_vectors, test_text_vectors, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_XY(data_chng_train, train_text_vectors, step, window, forecast):\n",
    "    '''\n",
    "    Splits textual and time series data into train or test dataset for hybrid model;\n",
    "    Objective y_i is percentage change of price movement for next day\n",
    "    '''\n",
    "    X_train, X_train_text, Y_train, Y_train2 = [], [], [], []\n",
    "    \n",
    "    for i in range(0, len(data_chng_train), step):\n",
    "        try:\n",
    "            x_i =data_chng_train[i:i+WINDOW]\n",
    "            y_i = np.std(data_chng_train[i:i+WINDOW+forecast][3])\n",
    "            \n",
    "            text_average = train_text_vectors[i:i+WINDOW]\n",
    "            last_close = x_i[-1]\n",
    "            \n",
    "            y_i2 = None\n",
    "            if(data_chng_train[i+WINDOW+forecast][3] > 0.0):\n",
    "                y_i2 = 1.\n",
    "            else:\n",
    "                y_i2 = 0.\n",
    "                \n",
    "        except Exception as e:\n",
    "            print('KEK', e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
