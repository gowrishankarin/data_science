{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-16T17:40:29.288949Z",
     "iopub.status.busy": "2020-10-16T17:40:29.288694Z",
     "iopub.status.idle": "2020-10-16T17:40:29.292115Z",
     "shell.execute_reply": "2020-10-16T17:40:29.291393Z",
     "shell.execute_reply.started": "2020-10-16T17:40:29.288916Z"
    },
    "id": "iOprdZxViGkm"
   },
   "outputs": [],
   "source": [
    "#!pip install pyunpack wget patool plotly cufflinks gitpython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AI Hub Notebook: [AI Hub - Interpretable Multi-Horizon Time Series Forecasting with TFT](https://aihub.cloud.google.com/p/products%2F9f39ad8d-ad81-4fd9-8238-5186d36db2ec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MV-P5VziiKMk",
    "outputId": "ca5cf12b-3c09-4a21-d288-c81a7f12ae1e"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-v0e5ESR5vJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YDiJMpM7R1Ed",
    "outputId": "7dbba14e-2471-4601-f147-aee70d853a4f"
   },
   "outputs": [],
   "source": [
    "if(DEVICE == 'tpu'):  \n",
    "    if 'COLAB_TPU_ADDR' not in os.environ:\n",
    "        print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
    "    else:\n",
    "        tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "        print ('TPU address is', tpu_address)\n",
    "\n",
    "        with tf.Session(tpu_address) as session:\n",
    "            devices = session.list_devices()\n",
    "\n",
    "        print('TPU devices:')\n",
    "        pprint.pprint(devices)\n",
    "\n",
    "if(DEVICE == 'gpu'):\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "yxlTZtrn7ETp",
    "outputId": "bba10e6f-52e9-45e5-c8c0-ea498c49389d"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "    print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "    print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "    print('re-execute this cell.')\n",
    "else:\n",
    "    print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdEGlIWRiGku"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmyVVXJniGky"
   },
   "outputs": [],
   "source": [
    "\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xADolIU5iV6E",
    "outputId": "210b343a-4139-4e50-932e-7b0dbc7e05d8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WK5V1o68WkLx",
    "outputId": "05ab0e57-f113-4fb4-d7ae-4db3412d40e2"
   },
   "outputs": [],
   "source": [
    "assets_dir = '/content/drive/My Drive/Colab Notebooks/od/assets'\n",
    "repo_dir = assets_dir + '/repo'\n",
    "print(repo_dir)\n",
    "if not os.path.exists(repo_dir):\n",
    "    os.makedirs(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNJwrD_AiGk3"
   },
   "outputs": [],
   "source": [
    "if not os.listdir(repo_dir):\n",
    "    git_url = \"https://github.com/google-research/google-research.git\"\n",
    "    Repo.clone_from(git_url, repo_dir)\n",
    "    \n",
    "# Sets current directory\n",
    "tft_dir = os.path.join(repo_dir, 'tft')\n",
    "os.chdir(tft_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Td-HgNBXiGk_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from script_download_data import main as download_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Asr942MviGlD",
    "outputId": "775d97bc-cc21-4e74-918c-f23f4c7bdd38"
   },
   "outputs": [],
   "source": [
    "# Download Parameters\n",
    "expt_name = 'traffic'\n",
    "output_folder = os.path.join(os.getcwd(), 'outputs')\n",
    "force_download = False\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "csv_file = os.path.join(output_folder, 'data', 'traffic', 'hourly_data.csv')\n",
    "if not os.path.exists(csv_file):\n",
    "    download_data(expt_name, force_download=True, output_folder=output_folder)\n",
    "df = pd.read_csv(csv_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "_Mp5jVFdiGlI",
    "outputId": "cad4107f-0d20-4f10-e2c4-6af7d33d7875"
   },
   "outputs": [],
   "source": [
    "from data_formatters.base import GenericDataFormatter, DataTypes, InputTypes\n",
    "\n",
    "print('Available data types:')\n",
    "for option in DataTypes:\n",
    "    print(option)\n",
    "    \n",
    "print()\n",
    "print('Available input types')\n",
    "for option in InputTypes:\n",
    "    print(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSqKNBNPiGlP"
   },
   "outputs": [],
   "source": [
    "from libs import utils\n",
    "import sklearn.preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGgEAd8piGlT"
   },
   "source": [
    "### TrafficFormatter\n",
    "- This class defines and formats data for traffic dataset\n",
    "- It performs z-score normalization across the entire dataset\n",
    "- z-score normalization ensures reusable functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oUTtJX3iGlU"
   },
   "outputs": [],
   "source": [
    "class TrafficFormatters(GenericDataFormatter):\n",
    "    \n",
    "    # This defines the types used by each column\n",
    "    _column_definition = [\n",
    "        ('id', DataTypes.REAL_VALUED, InputTypes.ID),\n",
    "        ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.TIME),\n",
    "        ('values', DataTypes.REAL_VALUED, InputTypes.TARGET),\n",
    "        ('time_on_day', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "        ('day_of_week', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "        ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "        ('categorical_id', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "    ]\n",
    "    \n",
    "    def split_data(self, df, valid_boundary=151, test_boundary=166):\n",
    "        \"\"\"\n",
    "        Splits dataframe into train, validation and test frames.\n",
    "        This also calibrates scaling object and transforms data for each split\n",
    "        \n",
    "        Args:\n",
    "            df: Source data frame to split\n",
    "            valid_boundary: Starting year for validation data\n",
    "            test_boundary: Starting year for test data\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of transformed tvt data\n",
    "        \"\"\"\n",
    "        print('Formatting tvt splits')\n",
    "        \n",
    "        index = df['sensor_day']\n",
    "        train = df.loc[index < valid_boundary]\n",
    "        valid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "        test = df.loc[index >= test_boundary - 7]\n",
    "        \n",
    "        self.set_scalers(train)\n",
    "        \n",
    "        return (self.transform_inputs(data) for data in [train, valid, test])\n",
    "    \n",
    "    def set_scalers(self, df):\n",
    "        \"\"\"\n",
    "        Calibrates scalers using the data df supplied\n",
    "        \"\"\"\n",
    "        print('Setting scalers with training data')\n",
    "        \n",
    "        column_definitions = self.get_column_definition()\n",
    "        id_column = utils.get_single_col_by_input_type(InputTypes.ID, column_definition=column_definitions)\n",
    "        target_column = utils.get_single_col_by_input_type(InputTypes.TARGET, column_definitions)\n",
    "        \n",
    "        # Extract identifiers in case required\n",
    "        self.identifiers = list(df[id_column].unique())\n",
    "        \n",
    "        # Format real scalers\n",
    "        real_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.REAL_VALUED,\n",
    "            column_definitions,\n",
    "            {\n",
    "                InputTypes.ID, InputTypes.TIME\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        data = df[real_inputs].values\n",
    "        self._real_scalers = sklearn.preprocessing.StandardScaler().fit(data)\n",
    "        self._target_scaler = sklearn.preprocessing.StandardScaler().fit(df[[target_column]].values)\n",
    "        \n",
    "        # Format categorical scalers\n",
    "        categorical_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.CATEGORICAL, column_definitions,\n",
    "            {\n",
    "                InputTypes.ID, InputTypes.TIME\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        categorical_scalers = {}\n",
    "        num_classes = []\n",
    "        \n",
    "        for col in categorical_inputs:\n",
    "            # Set all to str so that we dont have mixed integer/string columns\n",
    "            srs = df[col].apply(str)\n",
    "            categorical_scalers[col] = sklearn.preprocessing.LabelEncoder().fit(srs.values)\n",
    "            num_classes.append(srs.nunique())\n",
    "            \n",
    "        # Set categorical scaler ouputs\n",
    "        self._cat_scalers = categorical_scalers\n",
    "        self._num_classes_per_cat_input = num_classes\n",
    "        \n",
    "    def transform_inputs(self, df):\n",
    "        \"\"\"\n",
    "        Performs features transformations\n",
    "        \n",
    "        This includes both feature engineering, pre-processing and normalization\n",
    "        \"\"\"\n",
    "        \n",
    "        output = df.copy()\n",
    "        \n",
    "        if(self._real_scalers is None and self._cat_scalers is None):\n",
    "            raise ValueError('Scalers have not been set!')\n",
    "            \n",
    "        column_definitions = self.get_column_definition()\n",
    "        \n",
    "        real_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.REAL_VALUED, column_definitions, {\n",
    "                InputTypes.ID,\n",
    "                InputTypes.TIME\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        \n",
    "        output[real_inputs] = self._real_scalers.transform(\n",
    "            df[real_inputs].values\n",
    "        )\n",
    "        \n",
    "        categorical_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.CATEGORICAL, column_definitions,\n",
    "            {\n",
    "                InputTypes.ID, InputTypes.TIME\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Format Categorical Inputs\n",
    "        for col in categorical_inputs:\n",
    "            string_df = df[col].apply(str)\n",
    "            output[col] = self._cat_scalers[col].transform(string_df)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def format_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Reverts any normalization to give predictions in orignal scale\n",
    "        \n",
    "        Args:\n",
    "            predictions: DF or model predictions\n",
    "            \n",
    "        Returns:\n",
    "            DF of unnormalized predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        output = predictions.copy()\n",
    "        \n",
    "        column_names = predictions.columns\n",
    "        \n",
    "        for col in column_names:\n",
    "            if col not in {'forecast_time', 'identifier'}:\n",
    "                output[col] = self._target_scaler.inverse_transform(predictions[col])\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def get_fixed_params(self):\n",
    "        \"\"\"\n",
    "        Returns fixed model parameters for experiments\n",
    "        \"\"\"\n",
    "        \n",
    "        fixed_params = {\n",
    "            'total_time_steps': 8*24, # Total width of the Temporal Fusion Decoder\n",
    "            'num_encoder_steps': 7*24, # Length of LSTM decoder (ie # historical inputs)\n",
    "            'num_epochs': 50, # 100,\n",
    "            'early_stopping_patience': 5, # Early stopping threshold for # iterations with no loss improvement\n",
    "            'multiprocessing_workers': 5 # Number of multi-processing workers\n",
    "        }\n",
    "        return fixed_params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "pwQ_kALTiGlY",
    "outputId": "7c84dac9-7c51-41a5-99c8-186525e4a12d"
   },
   "outputs": [],
   "source": [
    "### Explore\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpOPqb2jiGlc"
   },
   "source": [
    "### Training and Evaluating the TFT\n",
    "Using the data formatting in TrafficFormatter, we next walk through the procedure for training the TFT\n",
    "\n",
    "First, we get all data-related parameters from the data formatter and define TFT model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "VG7SOw4AiGld",
    "outputId": "37757606-0da5-4082-fbfa-51b923909432"
   },
   "outputs": [],
   "source": [
    "# Create a data formatter\n",
    "data_formatter = TrafficFormatters()\n",
    "\n",
    "# Split Data\n",
    "train, valid, test = data_formatter.split_data(df)\n",
    "\n",
    "print(f'Train: {train.shape}, Test: {test.shape}, Val: {valid.shape}')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "KiW5_xlmiGli",
    "outputId": "ea3b8025-17f7-4d52-a71b-09b00fa78e34"
   },
   "outputs": [],
   "source": [
    "data_params = data_formatter.get_experiment_params()\n",
    "data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFsa9jrJiGlm"
   },
   "outputs": [],
   "source": [
    "# Model parameters for calbration\n",
    "model_params = {\n",
    "    'dropout_rate': 0.3, # Dropout discard rate\n",
    "    'hidden_layer_size': 320, # Internal state size of TFT\n",
    "    'learning_rate': 0.001, # ADAM initial learning rate\n",
    "    'minibatch_size': 128, # Minibatch size of training\n",
    "    'max_gradient_norm': 100.0, # Max norm for gradient clipping\n",
    "    'num_heads': 4, # Number of heads for multi-head attention\n",
    "    'stack_size': 1 # Number of stacks (default 1 for interpretability)\n",
    "}\n",
    "\n",
    "# Folder to save network weights during training\n",
    "model_folder = os.path.join(output_folder, 'saved_models', 'traffic', 'fixed')\n",
    "model_params['model_folder'] = model_folder\n",
    "\n",
    "model_params.update(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dPvoU4rBiGlq"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from libs.tft_model import TemporalFusionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "vg16LKTliGlu",
    "outputId": "e86e9d3c-917e-4bb8-d87c-9222479c2c2a"
   },
   "outputs": [],
   "source": [
    "tf_config = utils.get_default_tensorflow_config(tf_device=DEVICE, gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lma6_Ef_iGly",
    "outputId": "4f60e62b-0bec-4e7d-be9b-e56c92c2ff62"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    model = TemporalFusionTransformer(model_params, use_cudnn=USE_CUDNN)\n",
    "    \n",
    "    if(not model.training_data_cached()):\n",
    "        model.cache_batched_data(train, \"train\", num_samples=450000)\n",
    "        model.cache_batched_data(valid, \"valid\", num_samples=50000)\n",
    "\n",
    "    print(\"Data acquisition completed, training starts...\")\n",
    "    model.fit()\n",
    "    model.save(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhyrvRYIqkpt"
   },
   "outputs": [],
   "source": [
    "!ls /content/repo/tft/outputs/saved_models/\n",
    "!zip -r /content/repo/tft/outputs/saved_models.zip /content/repo/tft/outputs/saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fmNAv2WniGl3",
    "outputId": "4d5f81a0-ee58-417e-83d0-cbb03bd4d4aa"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    \n",
    "    # Create a new model and load weights\n",
    "    model = TemporalFusionTransformer(model_params, use_cudnn=USE_CUDNN)\n",
    "    model.load(model_folder)\n",
    "    \n",
    "    # Make forecasts\n",
    "    output_map = model.predict(test, return_targets=True)\n",
    "    targets = data_formatter.format_predictions(output_map['targets'])\n",
    "    \n",
    "    targets = data_formatter.format_predictions(output_map['targets'])\n",
    "    \n",
    "    # Format predictions\n",
    "    p50_forecast = data_formatter.format_predictions(output_map[\"p50\"])\n",
    "    p90_forecast = data_formatter.format_predictions(output_map[\"p90\"])\n",
    "    \n",
    "    def extract_numerical_data(data):\n",
    "        \"\"\"Strips out forecast time and identifier columns\"\"\"\n",
    "        return data[[\n",
    "            col for col in data.columns\n",
    "            if col not in {\"forecast_time\", \"identifier\"}\n",
    "        ]]\n",
    "    \n",
    "    p50_loss = utils.numpy_normalised_quantile_loss(\n",
    "        extract_numerical_data(targets), extract_numerical_data(p50_forecast),\n",
    "        0.5\n",
    "    )\n",
    "    p90_loss = utils.numpy_normalised_quantile_loss(\n",
    "        extract_numerical_data(targets), extract_numerical_data(p90_forecast),\n",
    "        0.9\n",
    "    )\n",
    "    \n",
    "    print(\"Normalised quantile losses: P50={}, P90={}\".format(p50_loss.mean(), p90_loss.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9SkqesOiGl7"
   },
   "source": [
    "### Interpretability Use Cases\n",
    "The relationships learnt by TFT can also be studied using the trained model, throug\n",
    "- Analyzing the variable selection weights to identify significant features for the prediction problem\n",
    "- Visualizing distributions of self-attention weights to determine the presence of any persistent temporal relationships\n",
    "\n",
    "In the remainder of this section, we demonstrate two interpretability use cases to showcase the above\n",
    "\n",
    "### Generate Weights for Interpretability\n",
    "First, we generate all necessary variable selection and attention weights required for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PYe8iDU3iGl8",
    "outputId": "d27532c6-7885-4fe6-c480-c9d458877325"
   },
   "outputs": [],
   "source": [
    "# Store outputs in maps \n",
    "counts = 0\n",
    "interpretability_weights = {\n",
    "    k: None for k in [\n",
    "        'decoder_self_attn', 'static_flags', 'historical_flags', 'future_flags'\n",
    "    ]\n",
    "}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    \n",
    "    # Create a new model and load weights\n",
    "    model = TemporalFusionTransformer(model_params, use_cudnn=USE_CUDNN)\n",
    "    for identifier, sliced in test.groupby('id'):\n",
    "        print(f\"Getting attention weights for {identifier}\")\n",
    "        weights = model.get_attention(sliced)\n",
    "        \n",
    "        for k in interpretability_weights:\n",
    "            w = weights[k]\n",
    "            \n",
    "            # Average attention across heads if necessary\n",
    "            if(k == 'decoder_self_attn'):\n",
    "                w = w.mean(axis=0)\n",
    "                \n",
    "                # Store a single matrix for weights to reduce memory footprint\n",
    "                batch_size, _, _ = w.shape\n",
    "                counts += batch_size\n",
    "                \n",
    "            if(interpretability_weights[k] is None):\n",
    "                interpretability_weights[k] = w.sum(axis=0)\n",
    "            else:\n",
    "                interpretability_weights[k] += w.sum(axis=0)\n",
    "                \n",
    "interpretability_weight = {\n",
    "    k: interpretability_weights[k] / counts for k in interpretability_weights\n",
    "}\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anyiBf7liGl_"
   },
   "source": [
    "### Use Case 1: Analysing Variable Importance\n",
    "- Analyze the distribution of variable selection weights on the input layer\n",
    "- Use it to quantify the relative importance of a given feature for the prediction problem in general\n",
    "- This is split into variable importance for static covariates, time-varying historical inputs and known future inputs as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AO_Cw0aqiGmA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_range(static_gate, axis=None):\n",
    "    \"\"\"\n",
    "    Return the mean, 10th, 50th and 90th percentile of variable importance weights\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'Mean': static_gate.mean(axis=axis),\n",
    "        '10%': np.quantile(static_gate, 0.1, axis=axis),\n",
    "        '25%': np.quantile(static_gate, 0.25, axis=axis),\n",
    "        '50%': np.quantile(static_gate, 0.5, axis=axis),\n",
    "        '1SD, 67%': np.quantile(static_gate, 0.67, axis=axis),\n",
    "        '90%': np.quantile(static_gate, 0.9, axis=axis),\n",
    "        '2SD 99.7%': np.quantile(static_gate, 0.977, axis=axis),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiL4NnJwiGmE"
   },
   "source": [
    "**Static Variable Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "9Vx2OZb-iGmF",
    "outputId": "12fe9dbe-baf7-40d9-cfd5-ba0705690548"
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    static_attn = x\n",
    "    static_attn = static_attn.reshape([-1, static_attn.shape[-1]])\n",
    "    return static_attn\n",
    "\n",
    "static_attn = flatten(interpretability_weights['static_flags'])\n",
    "m = get_range(static_attn, axis=0)\n",
    "pd.DataFrame({\n",
    "    k: pd.Series(m[k], index=['ID']) for k in m\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvNrmjfxiGmJ"
   },
   "source": [
    "**Temporal Variable Importance - Past Inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "7i6xay2XiGmK",
    "outputId": "48f8e7cd-a5f7-4c25-f227-743505d76a92"
   },
   "outputs": [],
   "source": [
    "x = flatten(interpretability_weights['historical_flags'])\n",
    "m = get_range(x, axis=0)\n",
    "pd.DataFrame({\n",
    "    k: pd.Series(m[k], index=['Hour of Day', 'Day of Week', 'Time Index', 'Target']) for k in m\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H8aerbXiGmN"
   },
   "source": [
    "**Temporal Variable Importance -- Future Inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "sAUvV5hZiGmO",
    "outputId": "d906b7db-7ff9-4a88-d6f9-cbe9f0c6a4db"
   },
   "outputs": [],
   "source": [
    "x = flatten(interpretability_weights['future_flags'])\n",
    "m = get_range(x, axis=0)\n",
    "pd.DataFrame({\n",
    "     k: pd.Series(m[k], index=['Hour of Day', 'Day of Week', 'Time Index']) for k in m\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8neBZmi9iGmS"
   },
   "source": [
    "### Use Case 2: Visualizing Persistent Temporal Patterns\n",
    "- We analyze the distribution of self-attention weights across various horizons to see if any persistent temporal patterns exist within the dataset\n",
    "- Through which, identify any seasonal patterns or lagged relationships in the dataset,\n",
    "- Based on that past time steps are consistently important for predictions at a given horizon\n",
    "\n",
    "We visualize this using the average attention pattern for various prediction horizons \n",
    "\n",
    "**Mean Attention Weights for Various Prediction Horizons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2j401keOiGmS"
   },
   "outputs": [],
   "source": [
    "# Plotting libraries and functions\n",
    "import plotly.offline\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot\n",
    "import plotly.graph_objs as go\n",
    "import cufflinks as cf\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWgiBfLPiGmZ"
   },
   "outputs": [],
   "source": [
    "# Loads plotly charts\n",
    "def iplot(fig, s='plot.html'):\n",
    "    filename = os.path.join(output_folder, s)\n",
    "    plotly.offline.plot(fig, filename=filename, auto_open=False)\n",
    "    return HTML(filename)\n",
    "\n",
    "def plotly_chart(\n",
    "    df, title=None, kind='scatter', \n",
    "    x_label=None, y_label=None, secondary_y=None, \n",
    "    fill=None, shape=None, subplots=False\n",
    "):\n",
    "    fig = df.iplot(\n",
    "        asFigure=True, title=title, kind=kind,\n",
    "        xTitle=x_label, yTitle=y_label, secondary_y=secondary_y,\n",
    "        fill=fill, subplots=subplots, shape=shape\n",
    "    )\n",
    "    \n",
    "    return iplot(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "nu__ia_kiGme",
    "outputId": "626bd3ee-0016-448e-c301-a55f974d944d"
   },
   "outputs": [],
   "source": [
    "self_attn = interpretability_weights['decoder_self_attn']\n",
    "\n",
    "means = pd.DataFrame({\n",
    "    \"horizon={}\".format(k): self_attn[model.num_encoder_steps + k - 1, :] for k in [1, 5, 10, 15, 20]\n",
    "})\n",
    "means.index -= model.num_encoder_steps\n",
    "\n",
    "plotly_chart(\n",
    "    means,\n",
    "    x_label=\"Position Index (n)\",\n",
    "    y_label=\"Mean Attention Weight\",\n",
    "    title=\"Average Attention Pattern at Various Prediction Horizons\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YYxePHHxlwv8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "001_TFT_InterpretableMultiHorizon_TimeSeriesForecasting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
