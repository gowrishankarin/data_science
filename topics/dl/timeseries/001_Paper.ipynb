{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\n",
    "- AI Hub Notebook: [AI Hub - Interpretable Multi-Horizon Time Series Forecasting with TFT](https://aihub.cloud.google.com/p/products%2F9f39ad8d-ad81-4fd9-8238-5186d36db2ec)\n",
    "- Arxiv Paper: [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363.pdf)\n",
    "- UCI ML Repository Data: [PEMS-SF Dataset](https://archive.ics.uci.edu/ml/datasets/PEMS-SF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FAQs**  \n",
    "**What is multi-horizon forecasting(MHF)?**  \n",
    "Multi-horizon forecasting(MHF) often contains a `complex mix of inputs` - including `static covariates`, known `future inputs` and other `exogenous time series` that are only observed in the past - without any prior information on how they interact witht the target.\n",
    "\n",
    "**What is new in this paper?**  \n",
    "There are many DL architectures published for multi-horizon forecasting problems but most of them are `black box` models. This paper brings insights into how the full range of inputs present in practical scenarios\n",
    "\n",
    "**What is a `Temporal Fusion Transformer(TFT)`?**  \n",
    "A TFT is a novel attention based architecture which combines high performance multi-horizon forecasting with interpretable insights into temporal dynamics.\n",
    "\n",
    "**What is powers the TFT architecture?**    \n",
    "To learn temporal relationship at different scales, TFT uses `Recurrent Neural Network(RNN)` for local processing and interpretable self-attention layers for long-term dependencies.\n",
    "It utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios.\n",
    "\n",
    "**What are the key DL concepts used?**\n",
    "- Recurrent Neural Networks (RNN)\n",
    "- Long Short Term Memory Networks (LSTM)\n",
    "- Attention Based Models\n",
    "- Transformer Based Models\n",
    "\n",
    "**What challenges this paper specifically addresses that is not considered before?**\n",
    "- Consideration of Exogenous inputs are known into the future\n",
    "- Not neglecting static covariates\n",
    "- Designing networks with suitable inductive biases \n",
    "- Considering heterogeneity of forecasting inputs\n",
    "\n",
    "**Does this paper comes with source code?**  \n",
    "Yes, the authors implemented on 3 of real-world datasets and demonstrated the significance of this architecture\n",
    "1. Electricity: The UCI Electricity Load Diagrams Dataset containing electricity consumption of 370 customers - Frequency, Hourly\n",
    "2. Traffic: The UCI PEM-SF Traffic Dataset describes the occupancy rate of freeways in Bay Area - Frequency, Hourly\n",
    "3. Retail: Favorita Grocery Sales Dataset from Kaggle - Frequency, daily\n",
    "4. Volatility: The OMI realized library contains daily realized volatility values of 31 stock indices - Frequency, daily(business days)\n",
    "\n",
    "**Why transformers?**  \n",
    "- RNN architecture fail to perform on long sequences efficiently\n",
    "- They take more advantage of parallelism than RNN architectures like LSTM and GRUs\n",
    "- Transformers ensures vanishing gradient problem isnt related with the length of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "MHF, the prediction of variables of interest at multiple future time steps. It is a very important problem in time series ML. Most of the time series problems solves one time step ahead. For example, \n",
    "- Given a series of observations of event collected over a period of time, `what could be the next event`\n",
    "- Given a list of closing day stock price for a given stock, `what is the price at the end of next day?`\n",
    "- From the observations of monthly purchasing power of a household, `what could be the purchase amount for the next month?`\n",
    "- Given unit of electricity consumed at hourly basis, `what is the power consumption by 10AM?`\n",
    "\n",
    "In case of MHF, the above highlighted questions transform to multiple time steps\n",
    "- what could be the next event $\\Longrightarrow$ `what could be the next 4 subsequent events`\n",
    "- what is the price at the end of next day? $\\Longrightarrow$ `what is the end of day price for tomorrow, day-after-tomorrow and the the day after`\n",
    "- what could be the purchase amount for the next month? $\\Longrightarrow$ `what could be the purchase amount for the next 6 months?`\n",
    "- what is the power consumption in the next 1 hour? $\\Longrightarrow$ `what is the power consumption by 10AM, 11AM, Noon and 1PM?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source of MHF\n",
    "MHFs have access to variety of data sources,heterogeneity of the data sources with little information about their interactions makes MHF a challenging problem, \n",
    "- Information about the future (e.g. upcoming holidays)\n",
    "- exogenous time series(e.g. historical customer footprint)\n",
    "- static meta data (e.g. geo-location of the store)\n",
    "\n",
    "#### Usual Challenges \n",
    "\n",
    "- Common challenges or problems with `auto-regressing models` is assume all exogenous inputs are known into future\n",
    "- These variables are simply concatenated with time-dependent features at each step\n",
    "- Most architectures are black box, where forecasts are controlled by complex nonlinear interactions between many parameters\n",
    "- Trustworthiness of the model is questioned due to the opaque nature\n",
    "\n",
    "Further, commonly used explainability methods(LIME and SHAP) for DNNs are not well suited for applying to temporal data.\n",
    "- LIME and SHAP do not consider the time ordering of input features\n",
    "- LIME, surrogate models are independently constructed for each data-point\n",
    "- SHAP features are considered independently for neighboring time steps\n",
    "Such post-hoc approaches might lead to poor explanation quality as dependencies between time steps are typically significant in time series.\n",
    "\n",
    "- Attention based architectures has the inherent interpretability for sequential data (e.g. language or speech). \n",
    "- Usually language or speech datasets are univariabe but temporal datas are multivariate, applying an Attention based model on such datasets is a novelty but heterogeneity of data is still a challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Temporal Fusion Transformers(TFT)\n",
    "The TFT proposed in this paper is an attention based DNN architecture for MHF to achieve high performance while enabling interpretability.\n",
    "Novel ideas incorporated, considering full range of potential inputs and temporal relationships of the variables, like\n",
    "1. Static covariate encoders which `encode context vectors` for use in other parts of the network\n",
    "2. **Gating mechanisms** is incorporated througout for sample dependent variable selection to minimize the contributions of irrelevant inputs\n",
    "3. A sequence to sequence layer to locally process known and observed inputs\n",
    "4. a temporal self-attention decoder to learn any long term dependencies present in the dataset - This facilitates the interpretability by identifying,  \n",
    "    a. Globally important variables for the prediction problem  \n",
    "    b. Persistent temporal patterns  \n",
    "    c. Significant events, like regimes  \n",
    "    \n",
    "While conventional method assumes target alone to be fed into prediction recursion loop and ignores numerous useful time-varying inputs for the 2nd time step onwards. TFT explicitly accounts for the diversity of inputs. This is done by naturally handling static covariates and time-varying inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series Interpretability with Attention - General Approach\n",
    "Attention Mechanisms(AM) are used in \n",
    "- Translation[17]\n",
    "- Image Classification[22]\n",
    "- Tabular Learning[23]  \n",
    "\n",
    "AMs identify saliance of input for each instance using the magnitude of attention weights. With interpretability motivations, time series researches[7, 12, 24] are conducted  using LSTM[25] and Transformer based architectures. However, it is usually done without giving importance to static covariates \n",
    "\n",
    "TFT alleviates the static covariates problem with separate `encoder-decoder attention` at each time step on top of the `self-attention` to determine the contribution of temporal input data.\n",
    "\n",
    "Post-hoc interpretability methods(SHAP, LIME) are applied on pre-trained black-box models and often based on distilling into a `surrogate interpretable model` or `decomposing into feature attributions`. They are not designed to take into account the time ordering of inputs, limiting their use for complex time series data.\n",
    "\n",
    "**Feature Selection Methods**\n",
    "- Inherently, interpretable modeling approaches build components for feature seection directly into the architecture\n",
    "- For time series forecasting, they are based on explicitly quantifying time-dependent variable contributions\n",
    "- Interpretable Multi-Variable LSTMs[27] partitions the hidden state such that each variable contributes uniquely to its own memory segment and weights memory segments to determine variable contributions\n",
    "- By computing `single contribution coefficient` based on attention weights, temporal importance and variable selections schemes are identified\n",
    "\n",
    "TFT is designed to analyze global temporal relationships with input data and allow users to interpret global behviors of the model on the whole dataset. Specifically in the identification of any persistent patterns(e.g. seasonality or lag effects) and `regimes` present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related Work\n",
    "- Traditional Multi Horizon Forecasting Methods[18, 19]\n",
    "- Iterated approaches using auto-regressive models[9, 6, 12]\n",
    "    - They are one step ahead prediction models with `multi-state predictions` obtained by recursively feeding predictions into future inputs\n",
    "    - LSTM Networks like Deep AR[9] uses stacked LSTM to generate parameters of a predefined `linear state-space model` with predictive distributions produced via `Kalman Filter`\n",
    "    - Further convolutional layers for `local processing` and a `sparse attention mechanism` to increase the `receptive field` during forecasting\n",
    "    - These methods assumes target alone to be fed recursively into future inputs\n",
    "- Direct methods based on sequence-to-sequence models[10, 11]\n",
    "    - Direct methods explicity generate forecasts for multiple predefined horizons at each time step relying seq2seq architecture\n",
    "    - LSTM encoders to summarize past inputs and a variety of methods to generate future predictions.\n",
    "    - MQRNN[10] uses LSTM or Convolutional encoders to generate context vectors to feed into an MLP for each horizon\n",
    "    - A multi-model attention mechanism is used with LSTM encoders to context vectors for a bi-directionsal LSTM decoder\n",
    "    - Yet interpretability remains challenging\n",
    "    \n",
    "**Others**  \n",
    "- Post-hoc explanations methods, [LIME, SHAP, RL-LIM],[15, 16, 26]\n",
    "- Inherently interpretable models[27, 24]\n",
    "- Methods combining temporal importance and variable selection[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Multi-Horizon Forecasting\n",
    "\n",
    "### 2.1 Inputs and Targets\n",
    "In a given time series dataset at each time step $\\mathcal{t} \\in [0, T_i]$\n",
    "\n",
    "- $I$ is the unique number of entities. $\\{i_1, i_2, i_3, \\cdots, i_I\\}$\n",
    "- Each entity $i$ is associated with a set of static covariates. i.e. $\\mathbf{s}_i \\in \\mathbb{R}^{m_s}$\n",
    "- Inputs $\\mathcal{X}_{i, t} \\in \\mathbb{R}^{m_x}$\n",
    "- Targets $\\mathcal{y}_{i,t} \\in \\mathbb{R}$\n",
    "\n",
    "Time dependent inputs are divided into 2 categories  \n",
    " \n",
    "$${\\mathcal{X}_{i,t} = [\\mathcal{z}^T_{i,t}, \\mathcal{x}^T_{i,t}]^T}$$  \n",
    "\n",
    "**Observed Inputs**  \n",
    "Measured at each step and are unknown beforehand  \n",
    "$$\\mathcal{z}^T_{i,t} \\in \\mathbb{R}^{m_z}$$  \n",
    "**Known Inputs**  \n",
    "Inputs that are pre-determined (e.g. day of week at time $t$)\n",
    "$$\\mathcal{x}^T_{i,t} \\in \\mathbb{R}^{m_x}$$\n",
    "\n",
    "#### Quantile Forecasting\n",
    "The provision for prediction intervals can be useful for optimizing decisions and risk managemnt by yielding an indication of likely best and worst case values of the target. i.e. Adoption of quantile regression to MHF by setting [10, 50, 90] percentiles at each time step. Quantile forecast takes the form\n",
    "\n",
    "$$\\large{\\hat{\\mathcal{y_i}}(q, t, \\tau) = f(q, \\tau, y_{i, t-k:t}, z_{i, t-k:t}, x_{i,t-k:t + \\tau}, s_i)}  \\tag{1}$$\n",
    "\n",
    "where,\n",
    "- $\\mathcal{\\hat{y}_{i, t+\\tau}}(q, t, \\tau)$ is the predicted $q^{th}$ sample quantile of $\\tau$ step ahead forecast at time $t$\n",
    "- $f_q(\\cdots)$ is the prediction model\n",
    "- $k$ is the finite look back window of all past observations \n",
    "- $t$ is the start time of the forecast where target and known inputs are available only till time $t$.. i.e\n",
    "$$y_{i, t-k:t} = \\{y_{i,t-k}, \\cdots, y_{i,t}\\}$$\n",
    "- All known inputs across the entire range is \n",
    "$$x_{i,t-k:t+\\tau} = \\{x_{i, t-k}, \\cdots, x_{i, t + \\tau}\\}$$\n",
    "\n",
    "Output forecasts for $\\tau_{max}$ time steps - i.e. $\\tau \\in \\{1, \\cdots, \\tau_{max}\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "TFT Architecture inputs,\n",
    "- Static metadata\n",
    "- Time varying past inputs\n",
    "- Time varying a priori known future inputs\n",
    "\n",
    "1. Variable selection  is used for judicious selection of the most salient features based on the input.  \n",
    "2. Gated Residual Network(GRN) blocks enable efficient information flow with skip connections and gating layers.  \n",
    "3. Time dependent processing is based on LSTMs for local processing and multi-head attention for integrating information from any time step\n",
    "\n",
    "Building blocks of a TFT,  \n",
    "1. **Gating Mechanisms**:  \n",
    "    - GMs to skip over any unused components of the architecture, \n",
    "    - Provide adaptive depth and network complexity to accomodate a wide range of datasets and scenarios.\n",
    "\n",
    "\n",
    "2. **Variable Selection Networks**:  \n",
    "    - VSNs to select relevant input variables at each time step\n",
    "\n",
    "\n",
    "3. **Static Covariate Encoders**:  \n",
    "    - SCEs to integrate static features into the network, through encoding of `context vectors to condition temporal dynamics`.\n",
    "\n",
    "\n",
    "4. **Temporal Processing**:  \n",
    "    - TPs to learn both long and short-term temporal relationships from the observed and known time-varying inpts\n",
    "    - A seq2seq layer is employed for local processing, \n",
    "    - Long term dependencies are captured using a novel interpretable multi-head attention block\n",
    "\n",
    "\n",
    "5. **Prediction Intervals**:\n",
    "    - PIs via quantile forecasts to determine the range of likely target values at each prediction horizon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gating Mechanism\n",
    "- The relationship between exogenous inputs and targets are often unknown, makes it difficult to anticipate the relevance of a variable\n",
    "- It is also difficult to determine the extent of required non-linear processing, and a simpler model might work better (e.g. small or noisy data)\n",
    "- To make the model flexible, non-linear processing is applied on need bases.\n",
    "\n",
    "The paper proposes Gated Residual Network(GRN) as a building block of TFT. The GRN takes primary input $a$ and an optional context vector $c$ and yields:\n",
    "\n",
    "$${GRN_{\\omega}(a, c) = LayerNorm(a + GLU_{\\omega}(\\eta_1)} \\tag{2}$$\n",
    "$$\\eta_1 = \\mathbf{W}_{1,\\omega} \\eta_2 + \\mathcal{b}_{1,\\omega} \\tag{3}$$\n",
    "$$\\eta_2 = ELU(\\mathbf{W}_{s,\\omega} a + \\mathbf{W}_{3,\\omega} c + \\mathcal{b}_{2,\\omega} )  \\tag{4}$$\n",
    "\n",
    "where,\n",
    "- ELU is the Exponential Linear Unit activation function[28]\n",
    "- $\\eta_1 \\in \\mathbb{R}^{d_{model}}$ - Intermediate Layer\n",
    "- $\\eta_2 \\in \\mathbb{R}^{d_{model}}$ - Intermediate Layer\n",
    "- $LayerForm$ is standard layer normalization[29]\n",
    "- $\\omega$ is an index to denote weight sharing\n",
    "\n",
    "ELU activation results in linear behavior by rendering 2 properties\n",
    "- identity function $\\mathbf{W}_{s,\\omega} a + \\mathbf{W}_{3,\\omega} c + \\mathcal{b}_{2,\\omega} \\gg 0$\n",
    "- constant generator $\\mathbf{W}_{2,\\omega} a + \\mathbf{W}_{3,\\omega} c + \\mathcal{b}_{2,\\omega} \\ll 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gated Linear Units(GLU)**  \n",
    "GLUs provide the flexibility to suppress any parts of architecture that are not required for a given dataset. \n",
    "\n",
    "Let $\\mathfrak{\\gamma} \\in \\mathbb{R}^{d_{model}}$ be the input, then\n",
    "\n",
    "$$GLU_w(\\mathcal{\\gamma}) = \\sigma(\\mathbf{W}_{4, \\omega} \\gamma + b_{4, \\omega} \\odot (\\mathbf{W}_{5,\\omega} + b_{5, \\omega}) \\tag{5}$$\n",
    "\n",
    "where,\n",
    "- $\\sigma(\\cdot)$ is the sigmoid activation functions\n",
    "- $\\mathbf{W} \\in \\mathbf{R}^{d_{model} x d_{model}}$ is the weight\n",
    "- $b_{(\\cdot)} \\in \\mathbf{R}^{d_{model}}$ is the bias\n",
    "- $\\odot$ is element wise `Hadamard Product` \n",
    "- $d_{model}$ is the hidden state size\n",
    "\n",
    "1. GLU allows TFT to control the extent to which the GRN contributes to the original input $a$. \n",
    "2. It potentially skips over the layer entirely, if necessary GLU ouputs could be all close to 0 in order to suppress the nonlinear contribution\n",
    "3. For instances without context vector, GRN simply treats the context to zero. i.e. $\\mathcal{C} = 0$\n",
    "4. During training, dropout is applied before the gating layer and layer normalization - i.e to $\\eta_1$ in Eq.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Variable Selection Networks\n",
    "Relevance of a specific variable from the dataset and its contributions are unknown while training a deep neural network. TFT is designed to provide `instance-wise variable selection through`\n",
    "- The use of `variable selection networks` applied to both static covariates and time dependent covariates to pick the most salient variables from dataset\n",
    "- Variable selection also allows TFT to remove any unnecessary noisy inputs that may impact the performance negatively\n",
    "\n",
    "This is accomplished using entity embeddings[31] for categorical variabls as feature representations and linear transformations for continuous variables.\n",
    "- Entity embeddings transform a variable into $d_{model}$ dimensional vector that matches the dimensions in the subsequent layers for skip connections\n",
    "- All static, past and future inputs make use of separate variable selection networks\n",
    "- Variable selection network for past inputs are presented without losing generality\n",
    "\n",
    "Let,\n",
    "- $\\xi^{(j)}_t \\in \\mathbb{R}^{d_model}$ denote the `transformed input` of the j-th variable at time t\n",
    "- $\\Xi_t  = [\\xi ^{(1)^T}_t, \\cdots, \\xi^{(m_{\\mathcal{x}})}_t]^T$ being the `flattened vector of all past inputs` at time $t$\n",
    "\n",
    "Variable selection weights are generated by feeding $\\Xi_t$ and an external context vector $\\large\\mathcal{c}_s$ through a GRN and then a Softmax Layer\n",
    "\n",
    "$${\\mathcal{V_{Xt}} = Softmax(GRN_{\\mathcal{V_{Xt}}}(\\Xi_t, \\large\\mathcal{c}_s))}\\tag{6}$$\n",
    "\n",
    "where,\n",
    "- $\\mathcal{V_{Xt}} \\in \\mathbb{R}^{m_x}$ is a vector of variable selections weights \n",
    "- $\\mathcal{c}_s$ is obtained from a statice covariate encoder\n",
    "- For static variables, the context vector $\\large\\mathcal{c}_s$ is omitted - given it already has access to static information\n",
    "\n",
    "At each time step an additional layer of non-linear processing is employed by feeding $\\xi^{(j)}_t$ through its own GRN\n",
    "\n",
    "$${\\tilde{\\xi}^{(j)}_t =  GRN_{\\tilde{\\xi}(j)}(\\xi^{}_t)}\\tag{7}$$\n",
    "\n",
    "Where,\n",
    "- $\\tilde{\\xi}^{(j)}_t$ is the `processed feature vector` for variable $j$\n",
    "- Note, each variable has its own GRN with weights shared across all time steps $t$\n",
    "- Processed features are then weighted by their variable selection weights and combined\n",
    "\n",
    "$${\\tilde{\\xi_t}} = \\sum^{\\mathcal{m_x}}_{j=1} \\mathcal{V^{(j)}_{Xt}} \\tilde{\\xi_t}^{(j)}  \\tag{8}$$\n",
    "\n",
    "Where,\n",
    "- $\\mathcal{V^{(j)}_{Xt}}$ is the $j^{th}$ element of vector $\\mathcal{V_{Xt}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Static Covariate Encoders\n",
    "TFT uses separate GRN encoders to integrate information from static metadata to produce 4 different context vectors\n",
    "$$\\large[\\mathcal{c}_s, \\large\\mathcal{c}_e, \\large\\mathcal{c}_c, \\large\\mathcal{c}_h]$$\n",
    "These context vectors are wired into various locations in the Temporal Fusion Decoder(TFD).\n",
    "- $\\mathcal{c}_s$ is temporal variable selection\n",
    "- $(\\mathcal{c}_c, \\mathcal{c}_h)$ Local processing of Temporal Features\n",
    "- $\\mathcal{c}_e$ Enriching of temporal features with static information\n",
    "\n",
    "Let, $\\zeta$ be the output of the static variable selection network, context for temporal varialble selection would be encoded according to \n",
    "$$\\mathcal{c}_s = GRN_{\\mathcal{c}_s}(\\zeta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Interpretable Multi-Head Attention\n",
    "TFT employs a `self-attention mechanism` to learn long-term relationships across different time steps, which we modify from `multi-head attention` in transformer based architectures[17, 12] to enhance explainability.\n",
    "\n",
    "**The Q, K and V**\n",
    "- Attention mechanisms `scales values` ${V} \\in \\mathbb{R}^{Nxd_v}$ relationship between keys(K) and queries(Q)\n",
    "- $K \\in \\mathbb{R}^{Nxd_{attn}}$ is the Key\n",
    "- $Q \\in \\mathbb{R}^{Nxd_{attn}}$ is the Query\n",
    "\n",
    "$$Attention({Q, K, V}) = A({Q,K})V\\tag{9}$$\n",
    "\n",
    "Where,\n",
    "- $A()$ is the normalization function - A common choice is scaled dot-product attention\n",
    "\n",
    "$$A({Q,K}) = Softmax(\\frac{QK^T}{\\sqrt{d_{attn}} })\\tag{10}$$\n",
    "\n",
    "Multi Head Attention is proposed in employing different heads for different representation subspaces to increase the learning capacity\n",
    "$$MultiHead{(Q,K,V)}) = [H_1, \\cdots, H_{m_H}]W_H\\tag{11}$$\n",
    "$$H_h = Attention(QW^{(h)}_Q, KW^{(h)}_K, VW^{(h)}_V) \\tag{12}$$\n",
    "\n",
    "Where,\n",
    "- $W^{(h)}_K \\in \\mathbb{R}^{d_{model} x d_{attn}}$ is head specific weights for keys\n",
    "- $W^{(h)}_Q \\in \\mathbb{R}^{d_{model} x d_{attn}}$ is head specific weights for queries\n",
    "- $W^{(h)}_V \\in \\mathbb{R}^{d_{model} x d_{V}}$ is head specific weights for values\n",
    "\n",
    "$W_H \\in \\mathbb{R}^{(m_h.d_V)xd_{model}}$ linearly combines outputs contatenated from all heads $H_h$\n",
    "\n",
    "1. Since different values are used in each head, attention weights alone is not indicative of a feature's importance\n",
    "2. Multi-head attention to share values in each head, and employ `additive aggregation` of all heads\n",
    "$$InterpretableMultiHead(Q, K, V) = \\tilde H \\tilde{W}_H \\tag{13}$$\n",
    "$\\tilde H = \\tilde A(Q, K)V W_V \\tag{14}$  \n",
    "\n",
    "$\\tilde H =  \\huge\\{ \\normalsize 1/H  \\sum^{m_H}_{h=1} A(QW^{(h)}_Q, KW^{(h)}_K)  \\huge\\}\\normalsize VW_V \\tag{15}$  \n",
    "$\\tilde H =  1/H  \\sum^{m_H}_{h=1} A(QW^{(h)}_Q, KW^{(h)}_K, VW_V)\\tag{16}$\n",
    "\n",
    "Where,\n",
    "- $W_v \\in \\mathbb{R}^{d_{model} x d_V}$ are value weights shared across all heads\n",
    "- $W_H \\in \\mathbb{R}^{d_{attn} x d_{model}}$ is used for final linear mapping\n",
    "\n",
    "1. Through this, each head can learn different temporal patterns, while attending to a common set of input features.\n",
    "2. These features can be interpretted as a simple ensemble over attention weights into combined matrix $\\tilde A(Q, K)$ Eq.14.\n",
    "3. Compared to $A(Q, K)$ Eq.10 in $\\tilde A(Q, K)$ Eq.14 yields an increased representation capacity in an efficient way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Temporal Fusion Decoder\n",
    "The TFD uses a series of layers descriped below to learn temporal relationships present in the dataset\n",
    "- Locality Enhancement with Sequence-to-Sequence Layer\n",
    "- Static Enrichment Layer\n",
    "- Temporal Self-Attention Layer\n",
    "- Position-wise Feed-forward Layer\n",
    "- Quantile Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Locality Enhancement with Seq2Seq Layer\n",
    "In time series data, points of significance are often identified in relation to their surrounding values, such as\n",
    "- Anomalies\n",
    "- Change-points\n",
    "- Cyclical patterns  \n",
    "\n",
    "Through the construction of features that utilize pattern information on top of `point-wise values` potentially gives better performance in attention based architecture. For e.g.[12] adopts a single convolutional layer for locality enhancement by extracting local patterns using the same filter across all time. This approach may not be suitable when observed inputs exists, due to the differing number of past and future inputs.\n",
    "\n",
    "This paper proposes, \n",
    "- The application of a sequence-to-sequence model to naturally handle these differences - feeding $\\tilde\\xi_{t-k:t}$ into the encoder and $\\tilde\\xi_{t+1:t+\\tau_{max}}$ into the decoder\n",
    "- This then generates a set of uniform temporal features which serve as inputs into the temporal fusion decoder itself - $\\phi(t,n) \\in {\\phi(t, -k), \\cdots, \\phi(t, \\tau_{max})}$, n is a position index\n",
    "- Commonly used seq2seq baselines are compared with LSTM decoder-encoder\n",
    "- LSTM serves as a replacement for standard positional encoding, providing an appropriate `inductive bias` for the time ordering of the inputs\n",
    "- To Allow static metadata to influence local processing, $C_c, C_h$ context vectors from the static covariate encoders to initialize the `cell state and hidden state` respectively for the first LSTM in the layer.\n",
    "- A gated skip connection is employed over this layer\n",
    "\n",
    "$$\\tilde\\phi(t, n) = LayerNorm(\\tilde\\xi_{t+n} + GLU_{\\tilde\\phi}(\\phi(t, n)))\\tag{17}$$\n",
    "\n",
    "Where, $\\mathcal{n} \\in [-k, \\tau_{max}]$ is a position index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Static Enrichment Layer\n",
    "A static enrichment layer that enhances temporal features with static metadata. For a position index $n$, static enrichment takes the form\n",
    "$$\\theta(t,n) = GRN_{\\theta}(\\tilde\\phi(t,n), \\mathcal{c}_e) \\tag{18}$$\n",
    "Where,\n",
    "- Weights of $GRN_{\\phi}$ are shared across the entire layer\n",
    "- $\\mathcal{c}_e$ is a context vector from the static covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Temporal Self-Attention Layer\n",
    "- All static-enriched temporal features are first grouped into a single matrix  $ \\Theta(t) = [\\theta(t, -k), \\cdots, \\theta(t, \\tau_{max})]^T$\n",
    "- Then interpretable multi-head attention is applied at each forecast time - $N = \\tau_{max} + k + 1$\n",
    "$$B(t) = InterpretableMultiHead(\\Theta(t), \\Theta(t), \\Theta(t))\\tag{19}$$\n",
    "- Yields $ \\beta(t) = [\\beta(t, -k), \\cdots, \\beta(t, \\tau_{max})]$\n",
    "- $d_V = d_{attn} = d_{model}/m_H$ are chosen where $m_h$ is the number of heads\n",
    "\n",
    "1. `Decoder masking`[17, 12] is applied to the multi-head attention layer to ensure that each temporal dimension can only attend to features preceeding it.\n",
    "2. Decoder Masking ensures `causal information flow` is preserved \n",
    "3. The self-attention layer allows TFT to pick up long-range dependencies that may be challenging for RNN based architectures to learn\n",
    "4. Following the self-attention layer, a gating layer is also applied to facilitate training\n",
    "$$\\delta(t,n) = LayerNorm(\\theta(t,n) + GLU_{\\delta}(\\beta(t,n))\\tag{20}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 Position-wise Feed-forward Layer\n",
    "An additional non-linear processing to the outputs of self attention layer is applied using GRNs\n",
    "$$\\psi(t,n) = GRN_{\\psi}(\\delta(t, n))\\tag{21}$$\n",
    "\n",
    "Where,\n",
    "- Weights of $GRN_\\psi$ are shared across the entire layer\n",
    "- A `Gated Residual Connection` is applidd that skips over the entire transformer block\n",
    "- This skip provides a direct path to sequence-to-sequence layer - yields, a simpler model when complexity is not required\n",
    "$$\\tilde\\psi(t,n) = LayerNorm(\\tilde\\phi(t, n) + GLU_{\\tilde\\psi}(\\psi(t,n))\\tag{22}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Quantile Outputs\n",
    "1. TFT generates prediction intervals on `top of point forecasts`[10] by achieving $[10^{th}, 50^{th}, 90^{th}]$ percentile predictions at each time step\n",
    "2. Quantile forecasts are generated using linear transformation of the output from the temporal fusion decoder\n",
    "$$\\hat{y}(q, t, \\tau) = \\mathbf{W}_q\\tilde\\psi(t, \\tau) + b_q \\tag{23}$$\n",
    "\n",
    "Where,\n",
    "- $W_q \\in \\mathbb{R}^{1xd}, b_q \\in \\mathbb{R}$ are linear coefficients for the specified quantile $q$\n",
    "- Forecasts are only generated for horizons in the future $\\tau \\in \\{1, \\cdots, \\tau_{max}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions\n",
    "TFT is trained by jointly minimizing the quantile loss, summed across all quantile outputs\n",
    "\n",
    "$$\\mathcal{L}(\\Omega, W) = \\sum_{y_t\\in\\Omega}\\sum_{q\\in\\mathcal{Q}}\\sum_{\\tau=1}^{\\tau_{max}}\\frac{QL(y_t, \\hat y(q, t - \\tau, \\tau), q)}{M\\tau_{max}}\\tag{24}$$\n",
    "$$QL(y, \\hat y, q) = q(y-\\hat y)_+ + (1-q)(\\hat y - y)_+ \\tag{25}$$\n",
    "\n",
    "Where, \n",
    "- $\\Omega$ is the domain of training data containing M samples\n",
    "- $W$ represents the weights of TFT\n",
    "- $\\mathcal{Q} = \\{0.1, 0.5, 0.9\\}$ is the set of output quantiles. ie $(.)_+ = max(0, .)$\n",
    "- For `out of sample testing`, normalized quantile losses across the entire forecasting horizon is evaluated - focusing P50 and P90 risk for consistency with previous work[9, 6, 12]\n",
    "\n",
    "$$q-Risk = \\frac{2 \\sum_{y_t\\in\\tilde\\Omega}\\sum_{\\tau=1}^{\\tau_{max}}QL(y_t, \\hat y(q, t-\\tau, \\tau), q)}{\\sum_{y_t\\in\\tilde\\Omega}\\sum_{\\tau=1}^{\\tau_{max}}|y_t|}\\tag{26}$$\n",
    "\n",
    "Where, $\\tilde\\Omega$ is the domain of test samples\n",
    "\n",
    "### 4.1 Calculate Normalized Quantile Loss(q-Risk)\n",
    "\n",
    "Let us calculated `q-Risk` in two steps for a single observation\n",
    "1. Calculate Quantile Loss Weighted Errors\n",
    "2. Calculate Normalized Quantile Loss(q-Risk)\n",
    "\n",
    "**quantile_loss_weighted_errors**: $QL(y, \\hat y, q)$, Eqn 25\n",
    "$$QL(y, \\hat y, q) = q(y-\\hat y)_+ + (1-q)(\\hat y - y)_+$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T08:42:39.889271Z",
     "iopub.status.busy": "2020-10-18T08:42:39.889036Z",
     "iopub.status.idle": "2020-10-18T08:42:39.893611Z",
     "shell.execute_reply": "2020-10-18T08:42:39.892861Z",
     "shell.execute_reply.started": "2020-10-18T08:42:39.889245Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_loss_weighted_errors(y, y_hat, quantile):\n",
    "    prediction_underflow = y - y_hat\n",
    "    weighted_errors = quantile * np.maximum(prediction_underflow, 0.0) + (1 - quantile) * np.maximum(-prediction_underflow, 0.0)\n",
    "    return weighted_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T07:53:23.252974Z",
     "iopub.status.busy": "2020-10-18T07:53:23.252733Z",
     "iopub.status.idle": "2020-10-18T07:53:23.574089Z",
     "shell.execute_reply": "2020-10-18T07:53:23.573123Z",
     "shell.execute_reply.started": "2020-10-18T07:53:23.252949Z"
    }
   },
   "source": [
    "**q-risk, quantile loss**: Calculate quantile loss and q-Risk for single observation set  \n",
    "from Eqn 24\n",
    "    $$quantile\\_loss = \\frac{QL(y, \\hat y, q)}{M}$$\n",
    "from Eqn 26\n",
    "    $$q\\_Risk = \\frac{2 \\times QL(y_t, \\hat y(q, t-\\tau, \\tau), q)}{|y_t|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T09:13:18.265811Z",
     "iopub.status.busy": "2020-10-18T09:13:18.265574Z",
     "iopub.status.idle": "2020-10-18T09:13:18.270274Z",
     "shell.execute_reply": "2020-10-18T09:13:18.269432Z",
     "shell.execute_reply.started": "2020-10-18T09:13:18.265784Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_loss(y, y_hat, quantile):\n",
    "    \"\"\"\n",
    "    Calculate the weighted error for the quantile loss q-risk calculation\n",
    "    Args:\n",
    "        y: target outcome\n",
    "        y_hat: predicted outcome\n",
    "        quantile: Quantile to use for loss calculations. range [0,1]\n",
    "    Outputs:\n",
    "        Float for normalized quantile loss\n",
    "    \"\"\"    \n",
    "    weighted_errors = quantile_loss_weighted_errors(y, y_hat, quantile)\n",
    "    quantile_loss = weighted_errors.mean()\n",
    "    normalizer = np.abs(y).mean()\n",
    "    return 2 * quantile_loss / normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T09:13:18.641653Z",
     "iopub.status.busy": "2020-10-18T09:13:18.641412Z",
     "iopub.status.idle": "2020-10-18T09:13:18.648738Z",
     "shell.execute_reply": "2020-10-18T09:13:18.647630Z",
     "shell.execute_reply.started": "2020-10-18T09:13:18.641627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05333333333333334, 0.26666666666666666, 0.48]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.array([1, -2, 3, -4, 5])\n",
    "y_hat = np.array([1, -3, 2, -5, 4])\n",
    "\n",
    "quantiles = np.array([0.1, 0.5, 0.9])\n",
    "[quantile_loss(y, y_hat, q) for q in quantiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
