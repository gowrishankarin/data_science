{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\n",
    "- AI Hub Notebook: [AI Hub - Interpretable Multi-Horizon Time Series Forecasting with TFT](https://aihub.cloud.google.com/p/products%2F9f39ad8d-ad81-4fd9-8238-5186d36db2ec)\n",
    "- Arxiv Paper: [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363.pdf)\n",
    "- UCI ML Repository Data: [PEMS-SF Dataset](https://archive.ics.uci.edu/ml/datasets/PEMS-SF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FAQs**  \n",
    "**What is multi-horizon forecasting?**  \n",
    "Multi-horizon forecasting(MHF) often contains a `complex mix of inputs` - including `static covariates`, known `future inputs` and other `exogenous time series` that are only observed in the past - without any prior information on how they interact witht the target.\n",
    "\n",
    "**What is new in this paper?**  \n",
    "There are many DL architectures published for multi-horizon forecasting problems but most of them are `black box` models. This paper brings insights into how the full range of inputs present in practical scenarios\n",
    "\n",
    "**What is a `Temporal Fusion Transformer(TFT)`?**  \n",
    "A TFT is a novel attention based architecture which combines high performance multi-horizon forecasting with interpretable insights into temporal dynamics.\n",
    "\n",
    "**What is powers the TFT architecture?**    \n",
    "To learn temporal relationship at different scales, TFT uses `Recurrent Neural Network(RNN)` for local processing and interpretable self-attention layers for long-term dependencies.\n",
    "It utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios.\n",
    "\n",
    "**What are the key DL concepts used?**\n",
    "- Recurrent Neural Networks (RNN)\n",
    "- Long Short Term Memory Networks (LSTM)\n",
    "- Attention Based Models\n",
    "- Transformer Based Models\n",
    "\n",
    "**What challenges this paper specifically addresses that is not considered before?**\n",
    "- Consideration of Exogenous inputs are known into the future\n",
    "- Not neglecting static covariates\n",
    "- Designing networks with suitable inductive biases \n",
    "- Considering heterogeneity of forecasting inputs\n",
    "\n",
    "**Does this paper comes with source code?**  \n",
    "Yes, the authors implemented on 3 of real-world datasets and demonstrated the significance of this architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "MHF, the prediction of variables of interest at multiple future time steps. It is a very important problem in time series ML. Most of the time series problems solves one time step ahead. For example, \n",
    "- Given a series of observations of event collected over a period of time, `what could be the next event`\n",
    "- Given a list of closing day stock price for a given stock, `what is the price at the end of next day?`\n",
    "- From the observations of monthly purchasing power of a household, `what could be the purchase amount for the next month?`\n",
    "- Given unit of electricity consumed at hourly basis, `what is the power consumption by 10AM?`\n",
    "\n",
    "In case of MHF, the above highlighted questions transform to multiple time steps\n",
    "- what could be the next event $\\Longrightarrow$ `what could be the next 4 subsequent events`\n",
    "- what is the price at the end of next day? $\\Longrightarrow$ `what is the end of day price for tomorrow, day-after-tomorrow and the the day after`\n",
    "- what could be the purchase amount for the next month? $\\Longrightarrow$ `what could be the purchase amount for the next 6 months?`\n",
    "- what is the power consumption in the next 1 hour? $\\Longrightarrow$ `what is the power consumption by 10AM, 11AM, Noon and 1PM?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source of MHF\n",
    "MHFs have access to variety of data sources,heterogeneity of the data sources with little information about their interactions makes MHF a challenging problem, \n",
    "- Information about the future (e.g. upcoming holidays)\n",
    "- exogenous time series(e.g. historical customer footprint)\n",
    "- static meta data (e.g. geo-location of the store)\n",
    "\n",
    "#### Usual Challenges \n",
    "\n",
    "- Common challenges or problems with autoregressing models is assume all exogenous inputs are known into future\n",
    "- These variables are simply concatenated with time-dependent features at each step\n",
    "- Most architectures are `black box`, where forecasts are controlled by complex nonlinear interactions between many parameters\n",
    "- Trustworthiness of the model is questioned due to the opaque nature\n",
    "\n",
    "Further, commonly used explainability methods(LIME and SHAP) for DNNs are not well suited for applying to temporal data.\n",
    "- LIME and SHAP do not consider the time ordering of input features\n",
    "- LIME, surrogate models are independently constructed for each data-point\n",
    "- SHAP features are considered independently for neighboring time steps\n",
    "Such post-hoc approaches might lead to poor explanation quality as dependencies between time steps are typically significant in time series.\n",
    "\n",
    "- Attention based architectures has the inherent interpretability for sequential data (e.g. language or speech). \n",
    "- Usually language or speech datasets are univariabe but temporal datas are multivariate, applying an Attention based model on such datasets is a novelty but heterogeneity of data is still a challenge\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Fusion Transformers(TFT)\n",
    "The TFT proposed in this paper is an attention based DNN architecture for MHF to achieve high performance while enabling interpretability.\n",
    "Novel ideas incorporated considering full range of potential inputs and temporal relationships are\n",
    "1. Static covariate encoders which encode context vectors for use in other parts of the network\n",
    "2. Gating mechanisms througout and sample dependent variable selection to minimize the contributions of irrelevant inputs\n",
    "3. A sequence to sequence layer to locally process known and observed inputs\n",
    "4. a temporal self-attention decoder to learn any long term dependencies present in the dataset - This facilitates the interpretability by identifying,  \n",
    "    a. Globally important variables for the prediction problem  \n",
    "    b. Persistent temporal patterns  \n",
    "    c. Significant events  \n",
    "    \n",
    "While conventional method assumes target alone to be fed into prediction recursion loop and ignores numerous useful time-varying inputs for the 2nd time step onwards, TFT explicitly accounts for the diversity of inputs. This is done by naturally handling static covariates and time-varying inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series Interpretability with Attention\n",
    "Attention mechanisms are used in \n",
    "- Translation[17]\n",
    "- Image Classification[22]\n",
    "- Tabular Learning[23]  \n",
    "\n",
    "To identify saliance of input for each instance using the magnitude of attention weights. With interpretability motivations, time series researches[7, 12, 24] were conducted  using LSTM[25] and Transformer based architectures. However it is done, without giving importance of static covariates \n",
    "\n",
    "TFT alleviates th static covariates problem with separate encoder-decoder attention at each step on top of the self-attention to determine the contribution of temporal inputs\n",
    "\n",
    "Post-hoc interpretability methods are applied on pre-trained black-box models and often based on distilling into a surrogate interpretable model or decomposing into feature attributions. They are not designed to take into account the time ordering of inputs, limiting their use for complex time series data.\n",
    "\n",
    "**Feature Selection Methods**\n",
    "- Inherently interpretable modeling approaches build components for feature seection directly into the architecture\n",
    "- For time series forecasting, they are based on explicitly quantifying time-dependent variable contributions\n",
    "- Interpretable Multi-Variable LSTMs[27] partitions the hidden state such that each variable contributes uniquely to its own memory segment and weights memory segments to determine variable contributions\n",
    "- By computing single contribution coefficient based on attention weights, temporal importance and variable selections schemes are identified\n",
    "\n",
    "TFTs is designed to analyze global temporal relationships with input data and allow users to interpret global behviors of the model on the whole dataset. Specifically in the identification of any persistent patterns(e.g. seasonality or lag effects) and regimes present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Work\n",
    "- Traditional Multi Horizon Forecasting Methods[18, 19]\n",
    "- Iterated approaches using autoregressive models[9, 6, 12]\n",
    "    - They are one step ahead prediction models with multi-stpe predictions obtained by recursively feeding predictions into future inputs\n",
    "    - LSTM Networks like Deep AR[9] uses stacked LSTM to generate parameters of a predefined linear state-space model with predictive distributions produced via Kalman Filter\n",
    "    - Further convolutional layers for local processing and a sparse attention mechanism to increase the receptive field during forecasting\n",
    "    - These methods assumes target alone to be fed recursively into future inputs\n",
    "- Direct methods based on sequence-to-sequence models[10, 11]\n",
    "    - Direct methods explicity generate forecasts for multiple predefined horizons at each time step relying seq2seq architecture\n",
    "    - LSTM encoders to summarize past inputs and a variety of methods to generate future predictions.\n",
    "    - MQRNN[10] uses LSTM or Convolutional encoders to generate context vectors to feed into an MLP for each horizon\n",
    "    - A multi-model attention mechanism is used with LSTM encoders to context vectors for a bi-directionsal LSTM decoder\n",
    "    - Yet interpretability remains challenging\n",
    "    \n",
    "**Others**  \n",
    "- Post-hoc explanations methods, [LIME, SHAP, RL-LIM],[15, 16, 26]\n",
    "- Inherently interpretable models[27, 24]\n",
    "- Methods combining temporal importance and variable selection[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Horizon Forecasting\n",
    "\n",
    "### Inputs and Targets\n",
    "In a given time series dataset at each time step $\\mathcal{t} \\in [0, T_i]$\n",
    "- $I$ is the unique number of entities.\n",
    "- Each entity $i$ is associated with a set of static covariates. i.e. $\\mathbf{s}_i \\in \\mathbb{R}^{m_s}$\n",
    "- Inputs $\\mathcal{X}_{i, t} \\in \\mathbb{R}^{m_x}$\n",
    "- Targets $\\mathcal{y}_{i,t} \\in \\mathbb{R}$\n",
    "\n",
    "Time dependent inputs are divided into 2 categories  \n",
    " \n",
    "$$\\mathcal{X}_{i,t} = [\\mathcal{z}^T_{i,t}, \\mathcal{x}^T_{i,t}]^T$$  \n",
    "\n",
    "**Observed Inputs**  \n",
    "Measured at each step and are unknown beforehand  \n",
    "$$\\mathcal{z}^T_{i,t} \\in \\mathbb{R}^{m_s}$$\n",
    "**Known Inputs** \n",
    "Inputs that are pre-determined (e.g. day of week at time $t$)\n",
    "$$\\mathcal{x}^T_{i,t} \\in \\mathbb{R}^{m_x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
