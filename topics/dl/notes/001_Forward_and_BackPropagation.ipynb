{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Backpropagation Algorithm - Pull your sleeves, Let's do some partial derivatives\n",
    "\n",
    "An associated source code will be published shortly.\n",
    "\n",
    "In this post, we shall explore a shallow Dense Neural Network(DNN) with \n",
    "- an Input Layer\n",
    "- a Single Hidden Layer and\n",
    "- an Output Layer\n",
    "\n",
    "The typical architecture of a feed forward network has the above units. \n",
    "Where \n",
    "- $x$ is a vector that represents the input values, \n",
    "- $\\hat y$ is a vector that represents the predictions and \n",
    "- $h$ is vector that represents the hidden layer, $h$ is a hyperparameter selected based on the context of the problem\n",
    "\n",
    "Further to the above 3 parameters, connection weights $[W_1, W_2]$ and connection biases $[b_1, b_2]$ are the other matrices and vectors used in a DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of a Shallow DNN\n",
    "**6 Inputs, 9 Hidden Nodes, 4 Outputs**\n",
    "\n",
    "<img src='shallow-dnn.png' alt=\"alternate text\"/>\n",
    "\n",
    "Following are the processes we shall cover in the quest of understanding a simple Neural Network architecture,  \n",
    "- Forward Propagation\n",
    "- Cross Entropy Loss (covered in a separate post)\n",
    "- Activation Functions\n",
    "    - Softmax\n",
    "    - ReLU\n",
    "- Backpropagation\n",
    "- Gradient Descent\n",
    "- Extraction of features of the hidden layer\n",
    "\n",
    "From the above Diagram...\n",
    "- There are 6 Inputs of dimension $(I \\times 1)$ ie (6 X 1)\n",
    "- There are 9 nodes in the Hidden Layer of dimension $(N \\times 1)$ ie (9 X 1)\n",
    "- There are 4 nodes in the Output Layer of dimension $(O \\times 1)$ ie (4 X 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "From the inputs and weights, biases of the hidden layer, \n",
    "$$z_1 = W_1x + b_1 \\tag{1}$$\n",
    "$$h = ReLU(z_1) \\tag{2}$$\n",
    "From hidden layer ($h$) and weights, biases to the output layer\n",
    "$$z_2 = W_2h + b_2 \\tag{3}$$\n",
    "$$\\hat y = softmax(z_2) \\tag{4}$$\n",
    "\n",
    "### Dimension Analysis\n",
    "How to select the weights, let us do dimensionality reduction\n",
    "Let us examine the dimensions of all elements in the equation 1\n",
    "$$z_1 = W_1h + b_1$$\n",
    "$$dimensions$$\n",
    "$$N \\times 1 = [?, ?] \\times [I, 1] + [?, ?]$$\n",
    "$$naturally$$\n",
    "$$N \\times 1 = [N, I] \\times [I, 1] + [N, 1]$$\n",
    "$$hence$$\n",
    "$$9 \\times 1 = [9, 6] \\times [6, 1] + [9, 1]$$\n",
    "Hence $N \\times I $ is the dimension of the weight matrix $W_1$\n",
    "\n",
    "\n",
    "Let us examine the dimensions of all elements in the equation 2\n",
    "$$z_2 = W_2h + b_2$$\n",
    "$$dimensions$$\n",
    "$$O \\times 1 = [?, ?] \\times [N, 1] + [?, ?]$$\n",
    "$$then$$\n",
    "$$O \\times 1 = [O, N] \\times [N, 1] + [O, 1]$$\n",
    "$$hence$$\n",
    "$$4 \\times 1 = [4, 9] \\times [9, 1] + [4, 1]$$\n",
    "Hence $O \\times N $ is the dimension of the weight matrix $W_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "Our goal is to minimize the loss $J$\n",
    "$$J = - \\sum\\limits_{k=1}^V y_k \\log\\hat y_k \\tag{5}$$ \n",
    "[Cost(Loss) Function for Binary Classification - A Deep Dive](https://www.linkedin.com/pulse/costloss-function-binary-classification-gowri-shankar/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation - Pull your sleeves and do some partial derivatives\n",
    "We calculated the $\\hat y$ during forward propagation and now our goal is to optimize the weights by minimizing the loss. This is done by calculating the partial derivatives wrt $[W_2, b_2]$ and then $[W_1, b_1]$\n",
    "$$\\frac{\\partial J}{\\partial W_1} = ReLU\\left( W^T_2(\\hat y - y)\\right)x^T \\tag{6}$$\n",
    "$$\\frac{\\partial J}{\\partial W_2} = (\\hat y - y)h^T \\tag{7}$$\n",
    "$$\\frac{\\partial J}{\\partial b_1} = ReLU\\left(W^T_2(\\hat y - y)\\right) \\tag{8}$$\n",
    "$$\\frac{\\partial J}{\\partial b_2} = \\hat y - y \\tag{9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient descent is the process during which the weights and biases are updated by subtracting $\\alpha$ times the learning rate of calculated gradients from the original matrices and biases\n",
    "\n",
    "$$W_1 := W1 - \\alpha\\frac{\\partial J}{\\partial W_1} \\tag{10}$$\n",
    "$$W_2 := W1 - \\alpha\\frac{\\partial J}{\\partial W_2} \\tag{11}$$\n",
    "$$b_1 := b1 - \\alpha\\frac{\\partial J}{\\partial b_1} \\tag{12}$$\n",
    "$$b_2 := b2 - \\alpha\\frac{\\partial J}{\\partial b_2} \\tag{13}$$\n",
    "\n",
    "[Demystifying Gradient Descent - A Deep Dive](https://www.linkedin.com/pulse/demystifying-gradient-descent-classification-problem-gowri-shankar/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
