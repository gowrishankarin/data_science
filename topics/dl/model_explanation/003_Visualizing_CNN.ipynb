{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing CNNs - The GradCAM Way\n",
    "Gradient Class Activation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Highlighting important pixels of predictios \n",
    "- Visualizing partial derivatives of predicted scores wrt pixel intensities\n",
    "- Deconvolution by making modifications to raw gradients \n",
    "- Methods that sythesize images to maximally activate a network unit\n",
    "- Invert a latent representation\n",
    "\n",
    "However all above are not class descriminative. Visualizations wrt different classes are nearly identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to trust a model?  \n",
    "- Interpretability\n",
    "- Comprehendability\n",
    "\n",
    "Gradient based Importances\n",
    "- Gradient based neuron importances mapped to Class specific domanin knowledge\n",
    "- Later is human centric\n",
    "\n",
    "Weakly Supervised Localization  \n",
    "- Modifies the fully connected layers with convolutional layers and global average pooling\n",
    "- Resulting in achieving class specific feature maps\n",
    "- Other methods, Global Max Pooling and Log Sum Exp Pooling\n",
    "\n",
    "Drawback of GradCAM  \n",
    "- GradCAMs requires feature maps to directly preced softmax layers - So a particular kind of CNN arch alone works\n",
    "- Prior to prediction architectures CONV Feature Maps -> Global Average Pooling -> Softmax Layers\n",
    "- \n",
    "\n",
    "Approach\n",
    "- Single shot localization through single forward and a partial backward pass per image\n",
    "- Convolutional layers naturally retail spatial information that is lost in fully connected layers\n",
    "- Last Conv layers to have the best compromize between high level semantics and detailed spatial info\n",
    "- Neurons here look for semantic class specific info in the image\n",
    "- GradCAM uses gradient information flow into the last conv layer to assign importance values to each neurons for a class of interest\n",
    "- This can be applied for any layer of a DNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Equation - 1\n",
    "- Compute gradient score of the last conv layer for a class c, y_c before softmax\n",
    "- Score is calculated with respect to feature map activations Ak of the conv layer\n",
    "- dyc/dAk \n",
    "- The gradient flowing back are global average pooled over width and height to obtain neuron importance αc_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Equation - 2  \n",
    "- The computation amounts to successive matrix products of the weight matrices and gradient wrt activation functions till final conv layer\n",
    "- Hence αc_k the weight represents a partial linearization of the dnn downstream from A\n",
    "- Then captures the importance of feature map k for the class c\n",
    "- Performed a weighted combination of forward activation maps and follow it by a ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Result\n",
    "- This results in a coarse heatmap of the same size of the Conv feature maps(14x14 for VGG, 10x10 fo Xception)\n",
    "- ReLU is applied to the linear combination of maps to get the positive influence on the class of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided GradCAM\n",
    "- GradCAM is a class discriminative and localizes relevant image regions, it lacks the ability to highligh fine grained details\n",
    "- Like, pixel-space gradient visualization methods(Guided backpropagation, Deconvolution)\n",
    "- Guided BP visualizes gradients wrt to the image where negative gradients are suppressed when BP throuh ReLU\n",
    "- Intuitively, this aims to to capture pixels detected by neurons, not the ones that supress neurons\n",
    "- Fuse Guided BP and GradCAM vis via element wise multiplication\n",
    "- ie Lc_GradCAM is first upsampled to the input image resolution using bilinear interpolation\n",
    "\n",
    "### Counterfactual Explanations\n",
    "- With slight modification to GradCAM wone can obtain explanations that highlight support for regions that would make the network change its prediction\n",
    "- As a consequence, removing concepts occurring in those regions would make the model more confident about its prediction\n",
    "- Equation of negative gradients\n",
    "- Take weighted sum of the forward activation maps A with αc_k and then ReLU it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
