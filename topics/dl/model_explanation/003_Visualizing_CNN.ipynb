{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing CNNs - The GradCAM Way\n",
    "Gradient Class Activation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Highlighting important pixels of predictios \n",
    "- Visualizing partial derivatives of predicted scores wrt pixel intensities\n",
    "- Deconvolution by making modifications to raw gradients \n",
    "- Methods that sythesize images to maximally activate a network unit\n",
    "- Invert a latent representation\n",
    "\n",
    "However all above are not class descriminative. Visualizations wrt different classes are nearly identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to trust a model?  \n",
    "- Interpretability\n",
    "- Comprehendability\n",
    "\n",
    "Gradient based Importances\n",
    "- Gradient based neuron importances mapped to Class specific domanin knowledge\n",
    "- Later is human centric\n",
    "\n",
    "Weakly Supervised Localization  \n",
    "- Modifies the fully connected layers with convolutional layers and global average pooling\n",
    "- Resulting in achieving class specific feature maps\n",
    "- Other methods, Global Max Pooling and Log Sum Exp Pooling\n",
    "\n",
    "Drawback of GradCAM  \n",
    "- GradCAMs requires feature maps to directly preced softmax layers - So a particular kind of CNN arch alone works\n",
    "- Prior to prediction architectures CONV Feature Maps -> Global Average Pooling -> Softmax Layers\n",
    "- \n",
    "\n",
    "Approach\n",
    "- Single shot localization through single forward and a partial backward pass per image\n",
    "- Convolutional layers naturally retail spatial information that is lost in fully connected layers\n",
    "- Last Conv layers to have the best compromize between high level semantics and detailed spatial info\n",
    "- Neurons here look for semantic class specific info in the image\n",
    "- GradCAM uses gradient information flow into the last conv layer to assign importance values to each neurons for a class of interest\n",
    "- This can be applied for any layer of a DNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Equation - 1\n",
    "- Compute gradient score of the last conv layer for a class c, y_c before softmax\n",
    "- Score is calculated with respect to feature map activations Ak of the conv layer\n",
    "- dyc/dAk \n",
    "- The gradient flowing back are global average pooled over width and height to obtain neuron importance αc_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Equation - 2  \n",
    "- The computation amounts to successive matrix products of the weight matrices and gradient wrt activation functions till final conv layer\n",
    "- Hence αc_k the weight represents a partial linearization of the dnn downstream from A\n",
    "- Then captures the importance of feature map k for the class c\n",
    "- Performed a weighted combination of forward activation maps and follow it by a ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Result\n",
    "- This results in a coarse heatmap of the same size of the Conv feature maps(14x14 for VGG, 10x10 fo Xception)\n",
    "- ReLU is applied to the linear combination of maps to get the positive influence on the class of interest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
