{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Set the random seed for consistent output\n",
    "np.random.seed(18)\n",
    "\n",
    "# Read in the data\n",
    "data = pd.read_csv(\"data/dummy_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 37, Test 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data.outcome\n",
    "X = data.drop('outcome', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "print(f\"Train {y_train.size}, Test {y_test.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions is of type: <class 'numpy.ndarray'>\n",
      "predictions has shape: (13,)\n",
      "predicted class for 10th element in test set: 0\n"
     ]
    }
   ],
   "source": [
    "prediction_probs = lr.predict_proba(X_test)\n",
    "\n",
    "# View the prediction type, shape, and print out a sample prediction\n",
    "print(f\"predictions is of type: {type(predictions)}\")\n",
    "print(f\"predictions has shape: {predictions.shape}\")\n",
    "print(f\"predicted class for 10th element in test set: {predictions[9]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element number: 0\n",
      "Predicted class: 1\n",
      "Probability of predicting class 0: 0.4234829673784545\n",
      "Probability of predicting class 1: 0.5765170326215455\n",
      "\n",
      "Element number: 1\n",
      "Predicted class: 1\n",
      "Probability of predicting class 0: 0.4914968703166641\n",
      "Probability of predicting class 1: 0.5085031296833359\n",
      "\n",
      "Element number: 2\n",
      "Predicted class: 1\n",
      "Probability of predicting class 0: 0.483088763245348\n",
      "Probability of predicting class 1: 0.516911236754652\n",
      "\n",
      "Element number: 3\n",
      "Predicted class: 0\n",
      "Probability of predicting class 0: 0.869536534985778\n",
      "Probability of predicting class 1: 0.13046346501422199\n",
      "\n",
      "Element number: 4\n",
      "Predicted class: 0\n",
      "Probability of predicting class 0: 0.8470774295731546\n",
      "Probability of predicting class 1: 0.15292257042684546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Element number: {i}\")\n",
    "    print(f\"Predicted class: {predictions[i]}\")\n",
    "    print(f\"Probability of predicting class 0: {prediction_probs[i][0]}\")\n",
    "    print(f\"Probability of predicting class 1: {prediction_probs[i][1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57651703, 0.50850313, 0.51691124, 0.13046347, 0.15292257,\n",
       "       0.26162479, 0.50831618, 0.3190805 , 0.37250246, 0.47736442,\n",
       "       0.15743244, 0.51193665, 0.26832495])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_probs[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846153846154"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweaked hyperparameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': False, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'warn', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "Mean Accuracy: 0.5384615384615384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'solver': 'liblinear',\n",
    "    'fit_intercept': False,\n",
    "    'penalty': 'l1',\n",
    "    'max_iter': 500\n",
    "}\n",
    "\n",
    "lr_tweaked = LogisticRegression(**params)\n",
    "lr_tweaked.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Tweaked hyperparameters: {lr_tweaked.get_params()}\\n\")\n",
    "print(f\"Mean Accuracy: {lr_tweaked.score(X_test, y_test)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['liblinear'], [True, False], ['l1', 'l2'], [None, 'balanced']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'solver': ['liblinear'],\n",
    "    'fit_intercept': [True, False],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'class_weight': [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "hp_values = list(hyperparams.values())\n",
    "hp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('liblinear', True, 'l1', None)\n",
      "('liblinear', True, 'l1', 'balanced')\n",
      "('liblinear', True, 'l2', None)\n",
      "('liblinear', True, 'l2', 'balanced')\n",
      "('liblinear', False, 'l1', None)\n",
      "('liblinear', False, 'l1', 'balanced')\n",
      "('liblinear', False, 'l2', None)\n",
      "('liblinear', False, 'l2', 'balanced')\n"
     ]
    }
   ],
   "source": [
    "for hp in itertools.product(*hp_values):\n",
    "    print(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters Userd: ('liblinear', True, 'l1', None)\n",
      "Mean accuracy of the model: 0.5384615384615384\n",
      "\n",
      "Parameters Userd: ('liblinear', True, 'l1', 'balanced')\n",
      "Mean accuracy of the model: 0.46153846153846156\n",
      "\n",
      "Parameters Userd: ('liblinear', True, 'l2', None)\n",
      "Mean accuracy of the model: 0.38461538461538464\n",
      "\n",
      "Parameters Userd: ('liblinear', True, 'l2', 'balanced')\n",
      "Mean accuracy of the model: 0.46153846153846156\n",
      "\n",
      "Parameters Userd: ('liblinear', False, 'l1', None)\n",
      "Mean accuracy of the model: 0.5384615384615384\n",
      "\n",
      "Parameters Userd: ('liblinear', False, 'l1', 'balanced')\n",
      "Mean accuracy of the model: 0.46153846153846156\n",
      "\n",
      "Parameters Userd: ('liblinear', False, 'l2', None)\n",
      "Mean accuracy of the model: 0.3076923076923077\n",
      "\n",
      "Parameters Userd: ('liblinear', False, 'l2', 'balanced')\n",
      "Mean accuracy of the model: 0.46153846153846156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hp in itertools.product(*hp_values):\n",
    "    \n",
    "    estimator = LogisticRegression(\n",
    "        solver=hp[0],\n",
    "        fit_intercept=hp[1],\n",
    "        penalty=hp[2],\n",
    "        class_weight=hp[3]\n",
    "    )\n",
    "    \n",
    "    estimator.fit(X_train, y_train)\n",
    "    print(f\"Parameters Userd: {hp}\")\n",
    "    print(f\"Mean accuracy of the model: {estimator.score(X_test, y_test)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
