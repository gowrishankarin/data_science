{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:45:53.515272Z",
     "iopub.status.busy": "2021-01-21T10:45:53.515029Z",
     "iopub.status.idle": "2021-01-21T10:45:58.649188Z",
     "shell.execute_reply": "2021-01-21T10:45:58.648396Z",
     "shell.execute_reply.started": "2021-01-21T10:45:53.515246Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:46:01.530383Z",
     "iopub.status.busy": "2021-01-21T10:46:01.530129Z",
     "iopub.status.idle": "2021-01-21T10:46:01.534754Z",
     "shell.execute_reply": "2021-01-21T10:46:01.534010Z",
     "shell.execute_reply.started": "2021-01-21T10:46:01.530356Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_image(image, label):\n",
    "  '''Normalizes the image. Returns image as input and label.'''\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = image / 255.0\n",
    "\n",
    "  return image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:46:14.730513Z",
     "iopub.status.busy": "2021-01-21T10:46:14.730254Z",
     "iopub.status.idle": "2021-01-21T10:47:40.042942Z",
     "shell.execute_reply": "2021-01-21T10:47:40.042123Z",
     "shell.execute_reply.started": "2021-01-21T10:46:14.730486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset fashion_mnist/3.0.1 (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to /Users/shankar/tensorflow_datasets/fashion_mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603ff9401f8840bba40c86fe26fc64a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac5773878494768b3e3e5e3eacf21ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7dea6ee580495180679e6ea43038b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/shankar/tensorflow_datasets/fashion_mnist/3.0.1.incompleteOJD8V1/fashion_mnist-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc57dfd7c7234c2986eb35bba2b55cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/shankar/tensorflow_datasets/fashion_mnist/3.0.1.incompleteOJD8V1/fashion_mnist-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bba0053b6a04899b17dfdefddca4df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset fashion_mnist downloaded and prepared to /Users/shankar/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "\n",
    "train_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"train\")\n",
    "train_dataset = train_dataset.map(map_image)\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "test_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"test\")\n",
    "test_dataset = test_dataset.map(map_image)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:47:40.045151Z",
     "iopub.status.busy": "2021-01-21T10:47:40.044857Z",
     "iopub.status.idle": "2021-01-21T10:47:40.050999Z",
     "shell.execute_reply": "2021-01-21T10:47:40.050330Z",
     "shell.execute_reply.started": "2021-01-21T10:47:40.045121Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder(inputs):\n",
    "    '''Defines the encoder with two Conv2D and max pooling layers.'''\n",
    "    conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_1)\n",
    "\n",
    "    conv_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_2)\n",
    "\n",
    "    return max_pool_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:47:40.053654Z",
     "iopub.status.busy": "2021-01-21T10:47:40.053236Z",
     "iopub.status.idle": "2021-01-21T10:47:40.065061Z",
     "shell.execute_reply": "2021-01-21T10:47:40.062718Z",
     "shell.execute_reply.started": "2021-01-21T10:47:40.053600Z"
    }
   },
   "outputs": [],
   "source": [
    "def bottle_neck(inputs):\n",
    "    '''Defines the bottleneck.'''\n",
    "    bottle_neck = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "    encoder_visualization = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(bottle_neck)\n",
    "\n",
    "    return bottle_neck, encoder_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:47:40.067642Z",
     "iopub.status.busy": "2021-01-21T10:47:40.067274Z",
     "iopub.status.idle": "2021-01-21T10:47:40.074579Z",
     "shell.execute_reply": "2021-01-21T10:47:40.073391Z",
     "shell.execute_reply.started": "2021-01-21T10:47:40.067597Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoder(inputs):\n",
    "    '''Defines the decoder path to upsample back to the original image size.'''\n",
    "    conv_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "    up_sample_1 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_1)\n",
    "\n",
    "    conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(up_sample_1)\n",
    "    up_sample_2 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_2)\n",
    "\n",
    "    conv_3 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(up_sample_2)\n",
    "\n",
    "    return conv_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:52:02.824500Z",
     "iopub.status.busy": "2021-01-21T10:52:02.824247Z",
     "iopub.status.idle": "2021-01-21T10:52:02.829908Z",
     "shell.execute_reply": "2021-01-21T10:52:02.829148Z",
     "shell.execute_reply.started": "2021-01-21T10:52:02.824471Z"
    }
   },
   "outputs": [],
   "source": [
    "def convolutional_auto_encoder():\n",
    "    '''\n",
    "    Builds the entire autoencoder model\n",
    "    '''\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1,))\n",
    "    encoder_output = encoder(inputs)\n",
    "    bottleneck_output, encoder_visualization = bottle_neck(encoder_output)\n",
    "    decoder_output = decoder(bottleneck_output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=decoder_output)\n",
    "    encoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_visualization)\n",
    "    \n",
    "    return model, encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T10:52:03.671798Z",
     "iopub.status.busy": "2021-01-21T10:52:03.671560Z",
     "iopub.status.idle": "2021-01-21T10:52:03.871086Z",
     "shell.execute_reply": "2021-01-21T10:52:03.870206Z",
     "shell.execute_reply.started": "2021-01-21T10:52:03.671772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         295040    \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 739,073\n",
      "Trainable params: 739,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convolutional_model, convolutional_encoder_model = convolutional_auto_encoder()\n",
    "convolutional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 60000 // BATCH_SIZE\n",
    "valid_steps = 60000 // BATCH_SIZE\n",
    "\n",
    "convolutional_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
    "conv_model_history = convolutional_model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=test_dataset, validation_steps=valid_steps, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-01-21T10:55:28.989968Z",
     "iopub.status.idle": "2021-01-21T10:55:29.004379Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "    '''Display sample outputs in one row.'''\n",
    "    for idx, test_image in enumerate(disp_images):\n",
    "        plt.subplot(3, 10, offset + idx + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        test_image = np.reshape(test_image, shape)\n",
    "        plt.imshow(test_image, cmap='gray')\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
    "    '''Displays the input, encoded, and decoded output values.'''\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    display_one_row(disp_input_images, 0, shape=(28,28,))\n",
    "    display_one_row(disp_encoded, 10, shape=enc_shape)\n",
    "    display_one_row(disp_predicted, 20, shape=(28,28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 1 batch of the dataset\n",
    "test_dataset = test_dataset.take(1)\n",
    "\n",
    "# take the input images and put them in a list\n",
    "output_samples = []\n",
    "for input_image, image in tfds.as_numpy(test_dataset):\n",
    "      output_samples = input_image\n",
    "\n",
    "# pick 10 indices\n",
    "idxs = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# prepare test samples as a batch of 10 images\n",
    "conv_output_samples = np.array(output_samples[idxs])\n",
    "conv_output_samples = np.reshape(conv_output_samples, (10, 28, 28, 1))\n",
    "\n",
    "# get the encoder ouput\n",
    "encoded = convolutional_encoder_model.predict(conv_output_samples)\n",
    "\n",
    "# get a prediction for some values in the dataset\n",
    "predicted = convolutional_model.predict(conv_output_samples)\n",
    "\n",
    "# display the samples, encodings and decoded values!\n",
    "display_results(conv_output_samples, encoded, predicted, enc_shape=(7,7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
