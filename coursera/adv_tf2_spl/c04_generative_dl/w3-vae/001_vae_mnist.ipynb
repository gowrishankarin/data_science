{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T05:41:55.510883Z",
     "iopub.status.busy": "2021-01-24T05:41:55.510617Z",
     "iopub.status.idle": "2021-01-24T05:42:01.923842Z",
     "shell.execute_reply": "2021-01-24T05:42:01.923045Z",
     "shell.execute_reply.started": "2021-01-24T05:41:55.510849Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T05:42:11.765234Z",
     "iopub.status.busy": "2021-01-24T05:42:11.764988Z",
     "iopub.status.idle": "2021-01-24T05:42:11.775417Z",
     "shell.execute_reply": "2021-01-24T05:42:11.774305Z",
     "shell.execute_reply.started": "2021-01-24T05:42:11.765209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define global constants to be used in this notebook\n",
    "BATCH_SIZE=128\n",
    "LATENT_DIM=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T05:42:31.233409Z",
     "iopub.status.busy": "2021-01-24T05:42:31.233171Z",
     "iopub.status.idle": "2021-01-24T05:42:31.240601Z",
     "shell.execute_reply": "2021-01-24T05:42:31.239606Z",
     "shell.execute_reply.started": "2021-01-24T05:42:31.233383Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_image(image, label):\n",
    "    '''returns a normalized and reshaped tensor from a given image'''\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    image = image / 255.0\n",
    "    image = tf.reshape(image, shape=(28, 28, 1,))\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_dataset(map_fn, is_validation=False):\n",
    "    '''Loads and prepares the mnist dataset from TFDS.'''\n",
    "    if is_validation:\n",
    "        split_name = \"test\"\n",
    "    else:\n",
    "        split_name = \"train\"\n",
    "\n",
    "    dataset = tfds.load('mnist', as_supervised=True, split=split_name)\n",
    "    dataset = dataset.map(map_fn)\n",
    "\n",
    "    if is_validation:\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "    else:\n",
    "        dataset = dataset.shuffle(1024).batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T05:42:48.268096Z",
     "iopub.status.busy": "2021-01-24T05:42:48.267864Z",
     "iopub.status.idle": "2021-01-24T05:42:48.490237Z",
     "shell.execute_reply": "2021-01-24T05:42:48.489694Z",
     "shell.execute_reply.started": "2021-01-24T05:42:48.268071Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(map_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "You will now be building your VAE model. The main parts are shown in the figure below:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1YAZAeMGEJ1KgieYk1ju-S9DoshpMREeC\" width=\"60%\" height=\"60%\"/>\n",
    "\n",
    "Like the autoencoder last week, the VAE also has an encoder-decoder architecture with the main difference being the grey box in the middle which stands for the latent representation. In this layer, the model mixes a random sample and combines it with the outputs of the encoder. This mechanism makes it useful for generating new content. Let's build these parts one-by-one in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T05:44:04.005655Z",
     "iopub.status.busy": "2021-01-24T05:44:04.005395Z",
     "iopub.status.idle": "2021-01-24T05:44:04.013104Z",
     "shell.execute_reply": "2021-01-24T05:44:04.011658Z",
     "shell.execute_reply.started": "2021-01-24T05:44:04.005627Z"
    }
   },
   "source": [
    "### Sampling Class\n",
    "\n",
    "First, you will build the `Sampling` class. This will be a custom Keras layer that will provide the Gaussian noise input along with the mean (mu) and standard deviation (sigma) of the encoder's output. In practice, the output of this layer is given by the equation:\n",
    "\n",
    "$$z = \\mu + e^{0.5\\sigma} * \\epsilon  $$\n",
    "\n",
    "where $\\mu$ = mean, $\\sigma$ = standard deviation, and $\\epsilon$ = random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T05:47:10.208792Z",
     "iopub.status.busy": "2021-01-24T05:47:10.208566Z",
     "iopub.status.idle": "2021-01-24T05:47:10.214160Z",
     "shell.execute_reply": "2021-01-24T05:47:10.213453Z",
     "shell.execute_reply.started": "2021-01-24T05:47:10.208768Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Generates a random sample and combines with the encoder output\n",
    "        \n",
    "        Args:\n",
    "        inputs - output tensor from the encoder\n",
    "        \n",
    "        Returns\n",
    "        inputs tensor combined with a random sample\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack the output of the encoder\n",
    "        mu, sigma = inputs\n",
    "        \n",
    "        #Get the size and dimensions of the batch\n",
    "        batch = tf.shape(mu)[0]\n",
    "        dim = tf.shape(mu)[1]\n",
    "        \n",
    "        # Generate a random tensor\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        \n",
    "        return mu + tf.exp(0.5 * sigma) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Next, you will build the encoder part of the network. You will follow the architecture shown in class which looks like this. Note that aside from mu and sigma, you will also output the shape of features before flattening it. This will be useful when reconstructing the image later in the decoder.\n",
    "\n",
    "*Note:* You might encounter issues with using batch normalization with smaller batches, and sometimes the advice is given to avoid using batch normalization when training VAEs in particular. Feel free to experiment with adding or removing it from this notebook to explore the effects.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1eoxFK_UVSHd3a_5EHcCU8F8QDZlPiXfW\" width=\"60%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T05:53:26.671503Z",
     "iopub.status.busy": "2021-01-24T05:53:26.671263Z",
     "iopub.status.idle": "2021-01-24T05:53:26.679457Z",
     "shell.execute_reply": "2021-01-24T05:53:26.678723Z",
     "shell.execute_reply.started": "2021-01-24T05:53:26.671478Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder_layers(inputs, latent_dim):\n",
    "    \n",
    "    # Add the conv2D layers followed by BN\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n",
    "\n",
    "    # Assign to a different variable so you can extract the shape later\n",
    "    batch_2 = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Flatten the features and feed into the Dense Network\n",
    "    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_2)\n",
    "    \n",
    "    # We arebitrarily used 20 units here but feel free to change and see what reuslts you get\n",
    "    x = tf.keras.layers.Dense(20, activation='relu', name=\"encode_dense\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add ouput Dense networks for mu and sigma, units equal to the declared latent_dim\n",
    "    mu = tf.keras.layers.Dense(latent_dim, name='latent_dim')(x)\n",
    "    sigma = tf.keras.layers.Dense(latent_dim, name='latent_sigma')(x)\n",
    "    \n",
    "    return mu, sigma, batch_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:03:31.013686Z",
     "iopub.status.busy": "2021-01-24T06:03:31.013448Z",
     "iopub.status.idle": "2021-01-24T06:03:31.018765Z",
     "shell.execute_reply": "2021-01-24T06:03:31.017910Z",
     "shell.execute_reply.started": "2021-01-24T06:03:31.013660Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder_model(latent_dim, input_shape):\n",
    "    \n",
    "    # Declare the inputs tensor with the given shape\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Get the ouput of the encoder_layers() function\n",
    "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
    "    \n",
    "    # Feed mu and sigma to thhe Sampling layer\n",
    "    z = Sampling()((mu, sigma))\n",
    "    \n",
    "    # Build the whole encoder model\n",
    "    model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
    "    \n",
    "    return model, conv_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Next, you will build the decoder part of the network which expands the latent representations back to the original image dimensions. As you'll see later in the training loop, you can feed random inputs to this model and it will generate content that resemble the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:09:16.322412Z",
     "iopub.status.busy": "2021-01-24T06:09:16.322175Z",
     "iopub.status.idle": "2021-01-24T06:09:16.330535Z",
     "shell.execute_reply": "2021-01-24T06:09:16.329758Z",
     "shell.execute_reply.started": "2021-01-24T06:09:16.322387Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, conv_shape):\n",
    "    \"\"\"Defines the decoder layers.\n",
    "    Args:\n",
    "    inputs -- output of the encoder \n",
    "    conv_shape -- shape of the features before flattening\n",
    "\n",
    "    Returns:\n",
    "    tensor containing the decoded output\n",
    "    \"\"\"\n",
    "\n",
    "    # feed to a Dense network with units computed from the conv_shape dimensions\n",
    "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
    "    x = tf.keras.layers.Dense(units, activation = 'relu', name=\"decode_dense1\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # reshape output using the conv_shape dimensions\n",
    "    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n",
    "\n",
    "    # upsample the features back to the original dimensions\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:11:03.106351Z",
     "iopub.status.busy": "2021-01-24T06:11:03.106106Z",
     "iopub.status.idle": "2021-01-24T06:11:03.110891Z",
     "shell.execute_reply": "2021-01-24T06:11:03.110057Z",
     "shell.execute_reply.started": "2021-01-24T06:11:03.106326Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoder_model(latent_dim, conv_shape):\n",
    "    \n",
    "    # Set the inputs to the shape of the latent space\n",
    "    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "    # Get the output of the decoder layers\n",
    "    outputs = decoder_layers(inputs, conv_shape)\n",
    "    \n",
    "    # Declare the inputs and outputs of the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergence\n",
    "\n",
    "To improve the generative capability of the model, you have to take into account the random normal distribution introduced in the latent space\n",
    "- to do that, KLD is computed and added to the reconstruction loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:27:02.015518Z",
     "iopub.status.busy": "2021-01-24T06:27:02.015265Z",
     "iopub.status.idle": "2021-01-24T06:27:02.020569Z",
     "shell.execute_reply": "2021-01-24T06:27:02.019473Z",
     "shell.execute_reply.started": "2021-01-24T06:27:02.015492Z"
    }
   },
   "outputs": [],
   "source": [
    "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
    "    \"\"\"\n",
    "    Computes the KLD \n",
    "    \n",
    "    Args:\n",
    "    inputs - batch from the dataset\n",
    "    outputs - output of the Sampling layer\n",
    "    mu - mean\n",
    "    sigma - std\n",
    "    \"\"\"\n",
    "    \n",
    "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
    "    kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
    "    \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Model\n",
    "You can now define the entire VAE model. Note the use of `model.add_loss()` to add the KL reconstruction loss. Computing this loss doesnt use \n",
    "`y_true` and `y_pred` so it cant be used in model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:27:02.626897Z",
     "iopub.status.busy": "2021-01-24T06:27:02.626651Z",
     "iopub.status.idle": "2021-01-24T06:27:02.631820Z",
     "shell.execute_reply": "2021-01-24T06:27:02.631090Z",
     "shell.execute_reply.started": "2021-01-24T06:27:02.626872Z"
    }
   },
   "outputs": [],
   "source": [
    "def vae_model(encoder, decoder, input_shape):\n",
    "    \n",
    "    # Set the input\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Get mu, sigma and z from the encoder input\n",
    "    mu, sigma, z = encoder(inputs)\n",
    "    \n",
    "    # Get reconstructed output from the decoder\n",
    "    reconstructed = decoder(z)\n",
    "    \n",
    "    # Define the inputs and outputs of the VAE\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
    "    \n",
    "    # Add the KL loss\n",
    "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
    "    model.add_loss(loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:27:03.068061Z",
     "iopub.status.busy": "2021-01-24T06:27:03.067689Z",
     "iopub.status.idle": "2021-01-24T06:27:03.073289Z",
     "shell.execute_reply": "2021-01-24T06:27:03.072535Z",
     "shell.execute_reply.started": "2021-01-24T06:27:03.067951Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_models(input_shape, latent_dim):\n",
    "    \"\"\"\n",
    "    Returns the encoder, decoder and vae models\n",
    "    \"\"\"\n",
    "    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
    "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
    "    \n",
    "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
    "    \n",
    "    return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:27:03.436213Z",
     "iopub.status.busy": "2021-01-24T06:27:03.435941Z",
     "iopub.status.idle": "2021-01-24T06:27:04.026364Z",
     "shell.execute_reply": "2021-01-24T06:27:04.025685Z",
     "shell.execute_reply.started": "2021-01-24T06:27:03.436186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the encoder, decoder and 'master' model (called vae)\n",
    "encoder, decoder, vae = get_models(input_shape=(28,28,1,), latent_dim=LATENT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:27:22.878231Z",
     "iopub.status.busy": "2021-01-24T06:27:22.877985Z",
     "iopub.status.idle": "2021-01-24T06:27:22.888595Z",
     "shell.execute_reply": "2021-01-24T06:27:22.887698Z",
     "shell.execute_reply.started": "2021-01-24T06:27:22.878205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define our loss functions and optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T06:28:15.041448Z",
     "iopub.status.busy": "2021-01-24T06:28:15.041194Z",
     "iopub.status.idle": "2021-01-24T06:28:15.048277Z",
     "shell.execute_reply": "2021-01-24T06:28:15.047475Z",
     "shell.execute_reply.started": "2021-01-24T06:28:15.041421Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, step, test_input):\n",
    "    \"\"\"Helper function to plot our 16 images\n",
    "\n",
    "    Args:\n",
    "\n",
    "    model -- the decoder model\n",
    "    epoch -- current epoch number during training\n",
    "    step -- current step number during training\n",
    "    test_input -- random tensor with shape (16, LATENT_DIM)\n",
    "    \"\"\"\n",
    "\n",
    "    # generate images from the test input\n",
    "    predictions = model.predict(test_input)\n",
    "\n",
    "    # plot the results\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
    "    plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training loop is shown below. This will display generated images each epoch and will take around 30 minutes to complete\n",
    "- Notice too that we add the KLD loss to the binary crossentropy loss before we get the gradients and update the weights\n",
    "\n",
    "- As you might expect, the initial 16 images will look random but it will improve overtime as the network learns \n",
    "- You will see images that resemble the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T10:47:40.191321Z",
     "iopub.status.busy": "2021-01-24T10:47:40.191063Z",
     "iopub.status.idle": "2021-01-24T10:47:40.195582Z",
     "shell.execute_reply": "2021-01-24T10:47:40.194597Z",
     "shell.execute_reply.started": "2021-01-24T10:47:40.191294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate random vector as test input to the decoder\n",
    "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
    "\n",
    "# number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Initialize the helper function to display outputs from an untrained model\n",
    "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}')\n",
    "    \n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Feed a batch to the VAE model\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
    "            flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
    "            \n",
    "            loss = bce_loss(flattened_inputs, flattened_outputs) * 784\n",
    "            \n",
    "            # Add KLD regularization loss\n",
    "            loss += sum(vae.losses)\n",
    "            \n",
    "        # Get the gradients and update the weights\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        \n",
    "        # Compute the loss metrix\n",
    "        loss_metric(loss)\n",
    "        \n",
    "        # display outputs every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            display.clear_output(wait=False)    \n",
    "            generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
    "            print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
