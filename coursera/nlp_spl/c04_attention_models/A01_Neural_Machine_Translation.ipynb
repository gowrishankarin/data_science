{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "This article takes you through the core concepts of the following...\n",
    "- English to German translation using Neural Machine Translation(NMT)\n",
    "- It uses LSTM networks with Attention\n",
    "- Beyond translation MT deciphers word sense disambiguation (e.x. `bank` refers to `financial bank` or `riverside bank`\n",
    "- Implemented using RNN with LSTMs can work for short to medium sentences but can result in vanishing gradient for long sequences\n",
    "- To address this, an attention mechanism is used to allow the decoder to access all relevant parts of the input sentence regardless of its lenght\n",
    "\n",
    "1. Preprocess the training and eval data\n",
    "2. Implement an encoder-decoder system with attention\n",
    "3. Understand how attention works\n",
    "4. Build the NMT model from scratch using Trax\n",
    "5. Generate translations using `Greedy and Minimum Bayes Risk`(MBR) decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preparation\n",
    "\n",
    "### 1.1 Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:11.176237Z",
     "iopub.status.busy": "2020-09-30T12:24:11.175979Z",
     "iopub.status.idle": "2020-09-30T12:24:19.757557Z",
     "shell.execute_reply": "2020-09-30T12:24:19.756131Z",
     "shell.execute_reply.started": "2020-09-30T12:24:11.176202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n",
      "trax                          1.3.4\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "DATA_DIR = './data/01'\n",
    "\n",
    "!pip list | grep trax # trax == 1.3.4 is required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:19.760278Z",
     "iopub.status.busy": "2020-09-30T12:24:19.759863Z",
     "iopub.status.idle": "2020-09-30T12:24:19.954832Z",
     "shell.execute_reply": "2020-09-30T12:24:19.954175Z",
     "shell.execute_reply.started": "2020-09-30T12:24:19.760230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get generator function for the training set\n",
    "# This will download the train dataset if no data_dir is specified\n",
    "train_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=True\n",
    ")\n",
    "\n",
    "# Get generator function for the eval set\n",
    "eval_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:19.957312Z",
     "iopub.status.busy": "2020-09-30T12:24:19.956986Z",
     "iopub.status.idle": "2020-09-30T12:24:20.754662Z",
     "shell.execute_reply": "2020-09-30T12:24:20.754022Z",
     "shell.execute_reply.started": "2020-09-30T12:24:19.957271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Tel: +421 2 57 103 777\\n', b'Tel: +421 2 57 103 777\\n')\n",
      "\n",
      "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"
     ]
    }
   ],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization and Formatting\n",
    "- Tokenizing the sentences using subword representations\n",
    "- Each sentences is represented as an array of integers \n",
    "- To avoid out-of-vocab words, subword representations are used\n",
    "- For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed.\n",
    "- This allows it to be more flexible, wont have to save uncommon words explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:20.756483Z",
     "iopub.status.busy": "2020-09-30T12:24:20.756286Z",
     "iopub.status.idle": "2020-09-30T12:24:20.759367Z",
     "shell.execute_reply": "2020-09-30T12:24:20.758738Z",
     "shell.execute_reply.started": "2020-09-30T12:24:20.756461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variables that state the filename and directory of the vocabulary file\n",
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = './data/01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:20.760514Z",
     "iopub.status.busy": "2020-09-30T12:24:20.760336Z",
     "iopub.status.idle": "2020-09-30T12:24:20.770252Z",
     "shell.execute_reply": "2020-09-30T12:24:20.768976Z",
     "shell.execute_reply.started": "2020-09-30T12:24:20.760494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the dataset.\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:20.772410Z",
     "iopub.status.busy": "2020-09-30T12:24:20.772002Z",
     "iopub.status.idle": "2020-09-30T12:24:20.779378Z",
     "shell.execute_reply": "2020-09-30T12:24:20.778142Z",
     "shell.execute_reply.started": "2020-09-30T12:24:20.772375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence\n",
    "# Integer assigned as end of sentence (EOS)\n",
    "# This will help us to infer the model has completed the translation\n",
    "EOS = 1\n",
    "\n",
    "# Generator helper function to append EOS to each sentence\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        \n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "        \n",
    "# Append EOS to the train data\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:20.780940Z",
     "iopub.status.busy": "2020-09-30T12:24:20.780628Z",
     "iopub.status.idle": "2020-09-30T12:24:20.918620Z",
     "shell.execute_reply": "2020-09-30T12:24:20.918011Z",
     "shell.execute_reply.started": "2020-09-30T12:24:20.780909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle tokenized example input:\u001b[0m [ 2538  2248    30 12114 23184 16889     5     2 20852  6456 20592  5812\n",
      "  3932    96  5178  3851    30  7891  3550 30650  4729   992     1]\n",
      "\u001b[31mSingle tokenized example target:\u001b[0m [ 1872    11  3544    39  7019 17877 30432    23  6845    10 14222    47\n",
      "  4004    18 21674     5 27467  9513   920   188 10630    18  3550 30650\n",
      "  4729   992     1]\n"
     ]
    }
   ],
   "source": [
    "# Filter long sentences\n",
    "# Filter too long sentences to not run out of memory\n",
    "# length_keys=[0, 1] means we filter both English and German sentences \n",
    "# Both must be not longer than 256 tokens for training / 512 for eval\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=256,\n",
    "    length_keys=[0, 1]\n",
    ")(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512,\n",
    "    length_keys=[0,1]\n",
    ")(tokenized_eval_stream)\n",
    "\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenize and Detokenize Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:20.921642Z",
     "iopub.status.busy": "2020-09-30T12:24:20.921441Z",
     "iopub.status.idle": "2020-09-30T12:24:20.927030Z",
     "shell.execute_reply": "2020-09-30T12:24:20.926380Z",
     "shell.execute_reply.started": "2020-09-30T12:24:20.921621Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    EOS = 1\n",
    "    inputs = next(trax.data.tokenize(\n",
    "        iter([input_str]),\n",
    "        vocab_file=vocab_file,\n",
    "        vocab_dir=vocab_dir\n",
    "    ))\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    integers = list(np.squeeze(integers))\n",
    "    EOS = 1\n",
    "    if(EOS in integers):\n",
    "        integers = integers[:integers.index(EOS)]\n",
    "        \n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:20.928480Z",
     "iopub.status.busy": "2020-09-30T12:24:20.928305Z",
     "iopub.status.idle": "2020-09-30T12:24:21.432244Z",
     "shell.execute_reply": "2020-09-30T12:24:21.431549Z",
     "shell.execute_reply.started": "2020-09-30T12:24:20.928459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle detokenized example input:\u001b[0m During treatment with olanzapine, adolescents gained significantly more weight compared with adults.\n",
      "\n",
      "\u001b[31mSingle detokenized example target:\u001b[0m Während der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.\n",
      "\n",
      "\n",
      "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n",
      "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
     ]
    }
   ],
   "source": [
    "# Detokenize an input-target pair of tokenized sentences\n",
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print()\n",
    "\n",
    "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
    "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T16:54:19.902351Z",
     "iopub.status.busy": "2020-09-27T16:54:19.902143Z",
     "iopub.status.idle": "2020-09-27T16:54:19.907272Z",
     "shell.execute_reply": "2020-09-27T16:54:19.906046Z",
     "shell.execute_reply.started": "2020-09-27T16:54:19.902329Z"
    }
   },
   "source": [
    "### 1.4 Bucketing\n",
    "[Comprehensive Hands-on Guide to Sequence Model batching strategy: Bucketing technique](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:21.433861Z",
     "iopub.status.busy": "2020-09-30T12:24:21.433487Z",
     "iopub.status.idle": "2020-09-30T12:24:21.439756Z",
     "shell.execute_reply": "2020-09-30T12:24:21.439060Z",
     "shell.execute_reply.started": "2020-09-30T12:24:21.433829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bucketing to create streams of batches\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes\n",
    "# batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 256 sentences of length < 8, 128, if length is\n",
    "# between 8 and 16. and so on -- and only 2 if length is over 512\n",
    "boundaries = [8, 16, 32, 64, 128,256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 15, 8, 4, 2]\n",
    "\n",
    "# Create the generators\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, \n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries,\n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "# Add masking for the padding (0s)\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:21.441173Z",
     "iopub.status.busy": "2020-09-30T12:24:21.440969Z",
     "iopub.status.idle": "2020-09-30T12:24:21.507190Z",
     "shell.execute_reply": "2020-09-30T12:24:21.506380Z",
     "shell.execute_reply.started": "2020-09-30T12:24:21.441150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch data type:  <class 'numpy.ndarray'>\n",
      "target_batch data type:  <class 'numpy.ndarray'>\n",
      "input_batch shape:  (32, 64)\n",
      "target_batch shape:  (32, 64)\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# let's see the data type of a batch\n",
    "print(\"input_batch data type: \", type(input_batch))\n",
    "print(\"target_batch data type: \", type(target_batch))\n",
    "\n",
    "# let's see the shape of this particular batch (batch length, sentence length)\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tokens acquired are used to produce embedding vectors for each word in the sentence\n",
    "- Hence, the embedding for a sentence is a matrix\n",
    "- The number of sentence in each batch us usually a power of 2 for optimal computer memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:21.508565Z",
     "iopub.status.busy": "2020-09-30T12:24:21.508308Z",
     "iopub.status.idle": "2020-09-30T12:24:21.771551Z",
     "shell.execute_reply": "2020-09-30T12:24:21.770537Z",
     "shell.execute_reply.started": "2020-09-30T12:24:21.508541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
      "\u001b[0m The adjusted mean difference was -4.3 points (CI 95% -6.4; -2.1 points, p-value < 0.0001).\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
      " \u001b[0m [   29  9701  1516  2640    53  1581   219     3   199  1164    50  7082\n",
      "     5  4207 11767    15   330     3   219  7108    15   150     3   135\n",
      "  1164     2   719    15   980   909 33287   913   266     3  8074  3912\n",
      " 33022 30650  4729   992     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
      "\u001b[0m Die angepasste mittlere Differenz betrug -4,3 Punkte (95 %-Konfidenzintervall: -6,4 bis -2,1 Punkte, p-Wert < 0,0001).\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
      "\u001b[0m [   57 30482  8385   191 14998     5 12919 20657  1581   219   227   199\n",
      "  2927    50  4207 11770    15 11580  7770 13427  9436 19070     5  2801\n",
      "    15   330   227   219   248  1581   150   227   135  2927     2   719\n",
      "    15  1619   909 33287   913   266   227  8074  3912 33022 30650  4729\n",
      "   992     1     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. NMT with Attention\n",
    "\n",
    "### 2.1 Attention Overview\n",
    "- An attention model will be built using an encoder-decoder architecture\n",
    "- The RNN will take in a tokenized version of a sentence in its encoder.\n",
    "- Pass the tokenized data into the decoder for translation\n",
    "- Using a sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will degrade for longer ones\n",
    "- All the context of the input sentence is compressed into one vector and passed into a decoder block\n",
    "- Context of the first parts of the input will have very little effect on the final vector passed to the decoder\n",
    "\n",
    "$$ENCODER \\rightarrow \\small{hello}\\hspace{2mm} \\normalsize{how}\\hspace{2mm} \\large{are}\\hspace{2mm} \\Large{you}\\hspace{2mm} \\huge{today}\\hspace{2mm} \\Huge{!} \\normalsize \\rightarrow DECODER$$\n",
    "\n",
    "- Adding an attention layer to this model avoids this problme by giving the decoder access to all parts of the input sentence\n",
    "- In a 4 word input sentence,\n",
    "    - Remember that a hidden state is produced at each timestep of the encoder\n",
    "    - These hidden states are all passed to the attention layer and each are given a score given the current activation(ie hidden state) of the decoder\n",
    "    - ie After predicting the first word, the attention layer will receive all the encoder hidden states as well as decoder hiddent state when producing the word wie\n",
    "    - Given these information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word\n",
    "- The result of the model training might have learned that it should align to the second encoder hidden state a\n",
    "- Subsequently assigns a high probability to the word geht.\n",
    "- If we use greedy decoding, we will output the said word as the next symbol, \n",
    "- then restart the process to produce the next word until we reach an end sentence prediction\n",
    "\n",
    "This is Scaled Dot Product Attention of the form\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "- Computing scores using Queries(Q) and keys(K) followed by a multiplication values(V) to get a context vector at a particular timestep of the decoder\n",
    "- The context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word\n",
    "- The division by squre root of the keys dimensionality $(\\sqrt{d_k})$ is for improving model performance\n",
    "- The encoder activations (hidden states) will be the keys and values, while decoder activations will be queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Input Encoder\n",
    "- The input encoder runs on the input tokens, creates its embeddings and feeds it to an LSTM network.\n",
    "- This outputs the activations that will be the keys and values for attention\n",
    "- It is a serial network with `tl.Embedding` and `tl.LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:21.775945Z",
     "iopub.status.busy": "2020-09-30T12:24:21.775737Z",
     "iopub.status.idle": "2020-09-30T12:24:21.779672Z",
     "shell.execute_reply": "2020-09-30T12:24:21.779015Z",
     "shell.execute_reply.started": "2020-09-30T12:24:21.775923Z"
    }
   },
   "outputs": [],
   "source": [
    "def input_encoder_Fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    input_encoder = tl.Serial(\n",
    "        tl.Embedding(input_vocab_size, d_model),\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
    "    )\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:24:32.705316Z",
     "iopub.status.busy": "2020-09-30T12:24:32.705048Z",
     "iopub.status.idle": "2020-09-30T12:24:32.711747Z",
     "shell.execute_reply": "2020-09-30T12:24:32.711061Z",
     "shell.execute_reply.started": "2020-09-30T12:24:32.705288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "import w1_unittest\n",
    "\n",
    "w1_unittest.test_input_encoder_fn(input_encoder_Fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Pre-attention Decoder\n",
    "- The pre-attention decoder runs on the targets and creates activations that are used as queries in attention\n",
    "- This is a Serial netwokr which is composed of the `tl.ShiftRight`, `tl.Embedding`, `tl.LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:25:04.104289Z",
     "iopub.status.busy": "2020-09-30T12:25:04.103936Z",
     "iopub.status.idle": "2020-09-30T12:25:04.109386Z",
     "shell.execute_reply": "2020-09-30T12:25:04.108576Z",
     "shell.execute_reply.started": "2020-09-30T12:25:04.104255Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        # Shift right to insert start-of-sentence token and implement \n",
    "        # teacher forcing during training\n",
    "        tl.ShiftRight(mode),\n",
    "        tl.Embedding(target_vocab_size, d_model),\n",
    "        tl.LSTM(d_model)\n",
    "    )\n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:25:04.361833Z",
     "iopub.status.busy": "2020-09-30T12:25:04.361550Z",
     "iopub.status.idle": "2020-09-30T12:25:04.365839Z",
     "shell.execute_reply": "2020-09-30T12:25:04.365083Z",
     "shell.execute_reply.started": "2020-09-30T12:25:04.361791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Preparing the Attention Input\n",
    "- This function will prepare the input for the attention layer\n",
    "- Take encoder and pre-attention decoder activations and assign it to the queries, ie hidden states of encoder and decoder\n",
    "- These activations are assigned to Queries, Keys and Values\n",
    "- Another output will be the mask to distinguish real tokens from padding tokens\n",
    "- This mask is used internally by Trax while computing the softmax so padding tokens  will not have an effect on the computed probabilities\n",
    "- Observe which tokens in the input corresponding to padding\n",
    "- Multi-headed attention - Computing attention multiple times to improve model's predictions\n",
    "- It is required to consider this additional axis in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:53:28.648511Z",
     "iopub.status.busy": "2020-09-30T12:53:28.648271Z",
     "iopub.status.idle": "2020-09-30T12:53:28.654242Z",
     "shell.execute_reply": "2020-09-30T12:53:28.653479Z",
     "shell.execute_reply.started": "2020-09-30T12:53:28.648484Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    \"\"\"\n",
    "    Prepare Queries, Keys, Values and Mask for attention\n",
    "    Args:\n",
    "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
    "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
    "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
    "    \n",
    "    Returns:\n",
    "        queries, keys, values and mask for attention.\n",
    "    \"\"\"\n",
    "    # Set the keys and values to the encoder activations\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "    \n",
    "    # Set the queries to the decoder activations\n",
    "    queries = decoder_activations\n",
    "    \n",
    "    # Generate the mask to distinguish real tokens from padding\n",
    "    # hint: inputs is 1 for real tokens and 0 where they are pdding\n",
    "    mask = (inputs >= 1)\n",
    "    \n",
    "    # add axes to the mask for attention heads and decoder length\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    \n",
    "    # Broadcast so mask shape is [batch_size, attention heads, decoder-len, encoder-len]\n",
    "    # Note: for thsi assignment, attention head is set to 1\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T12:53:29.077954Z",
     "iopub.status.busy": "2020-09-30T12:53:29.077715Z",
     "iopub.status.idle": "2020-09-30T12:53:29.123094Z",
     "shell.execute_reply": "2020-09-30T12:53:29.122072Z",
     "shell.execute_reply.started": "2020-09-30T12:53:29.077933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_prepare_attention_input(prepare_attention_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implementation Overview\n",
    "- Step 0: Prepare input encoder and pre-attention decoder branches\n",
    "- Step 1: Create a Serial Network, It will stack the layers in the next steps one after the other.\n",
    "- Step 2: Make a copy of the input and target tokens\n",
    "    - The input and target tokens will be fed into different layers of the model\n",
    "    - `tl.Select` layer to create copies oft these tokens\n",
    "    - Arrange them as `[input_tokens, target_tokens, input_tokens, target_tokens]`\n",
    "- Step 3: Create a parallel branch to feed the input tokens to the `input encoder` and target tokens to the `pre-attention decoder`\n",
    "    - `tl.Parallel` to create these sublayers in parallel\n",
    "- Step 4: Call the `prepare_attention_input` function to convert the encoder and pre-attention decoder actiations to a format that the attention layer will accept\n",
    "    - `tl.Fn` to be used.\n",
    "- Step 5: Feed Q, K V and Mask to the tl.AttentionQKV layer\n",
    "    - This computes the scaled dot product attention and outputs the attention weights and mask\n",
    "    - Though it is a one liner, It composed of a DNN made up of several branches\n",
    "    - Having deep layers pose the risk of vanishing gradients during training - to be mitigated\n",
    "    - `tl.Residual` layer is added to improve ability to learn - Added to output of `AttentionQKV` with the queries input\n",
    "    - Nest the `AttentionQKV` inside `Residual`\n",
    "- Step 6: Mask is not needed, hence dropped.\n",
    "    - At this point in network - the signal stack currently has `[attention activations, mask, target tokens]` and you can use `tl.Select` to output just 3\n",
    "- Step 7: Feed the attention weighted output to the LSTM decoder. \n",
    "    - Stack multiple `tl.LSTM` layers to improve the output so remember to append to LSTMs equal to n_decoder_layers of the model\n",
    "- Step 8: Determine the probabilities of each subword in the vocabulary and set this up easily with a `tl.Dense` layer  by making its size equal to the size of our vocabulary\n",
    "- Step 9: Normalize the output to log probabilities by passing the activations in Step 8 to a `tl.LogSoftmax` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
