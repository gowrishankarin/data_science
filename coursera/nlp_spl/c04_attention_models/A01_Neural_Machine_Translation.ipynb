{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "This article takes you through the core concepts of the following...\n",
    "- English to German translation using Neural Machine Translation(NMT)\n",
    "- It uses LSTM networks with Attention\n",
    "- Beyond translation MT deciphers word sense disambiguation (e.x. `bank` refers to `financial bank` or `riverside bank`\n",
    "- Implemented using RNN with LSTMs can work for short to medium sentences but can result in vanishing gradient for long sequences\n",
    "- To address this, an attention mechanism is used to allow the decoder to access all relevant parts of the input sentence regardless of its lenght\n",
    "\n",
    "1. Preprocess the training and eval data\n",
    "2. Implement an encoder-decoder system with attention\n",
    "3. Understand how attention works\n",
    "4. Build the NMT model from scratch using Trax\n",
    "5. Generate translations using `Greedy and Minimum Bayes Risk`(MBR) decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preparation\n",
    "\n",
    "### 1.1 Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T15:28:09.154360Z",
     "iopub.status.busy": "2020-09-27T15:28:09.154084Z",
     "iopub.status.idle": "2020-09-27T15:28:15.565815Z",
     "shell.execute_reply": "2020-09-27T15:28:15.565098Z",
     "shell.execute_reply.started": "2020-09-27T15:28:09.154324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n",
      "trax                          1.3.4\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "DATA_DIR = './data/01'\n",
    "\n",
    "!pip list | grep trax # trax == 1.3.4 is required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T15:28:15.567723Z",
     "iopub.status.busy": "2020-09-27T15:28:15.567408Z",
     "iopub.status.idle": "2020-09-27T15:28:15.743843Z",
     "shell.execute_reply": "2020-09-27T15:28:15.743276Z",
     "shell.execute_reply.started": "2020-09-27T15:28:15.567682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get generator function for the training set\n",
    "# This will download the train dataset if no data_dir is specified\n",
    "train_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=True\n",
    ")\n",
    "\n",
    "# Get generator function for the eval set\n",
    "eval_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T15:28:15.746200Z",
     "iopub.status.busy": "2020-09-27T15:28:15.745876Z",
     "iopub.status.idle": "2020-09-27T15:28:16.549943Z",
     "shell.execute_reply": "2020-09-27T15:28:16.549279Z",
     "shell.execute_reply.started": "2020-09-27T15:28:15.746164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Decreased Appetite\\n', b'Verminderter Appetit\\n')\n",
      "\n",
      "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"
     ]
    }
   ],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization and Formatting\n",
    "- Tokenizing the sentences using subword representations\n",
    "- Each sentences is represented as an array of integers \n",
    "- To avoid out-of-vocab words, subword representations are used\n",
    "- For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed.\n",
    "- This allows it to be more flexible, wont have to save uncommon words explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T15:28:21.148747Z",
     "iopub.status.busy": "2020-09-27T15:28:21.148517Z",
     "iopub.status.idle": "2020-09-27T15:28:21.151961Z",
     "shell.execute_reply": "2020-09-27T15:28:21.151151Z",
     "shell.execute_reply.started": "2020-09-27T15:28:21.148723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variables that state the filename and directory of the vocabulary file\n",
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = './data/01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T15:28:39.937567Z",
     "iopub.status.busy": "2020-09-27T15:28:39.937227Z",
     "iopub.status.idle": "2020-09-27T15:28:39.945265Z",
     "shell.execute_reply": "2020-09-27T15:28:39.943796Z",
     "shell.execute_reply.started": "2020-09-27T15:28:39.937530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the dataset.\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T15:32:58.000906Z",
     "iopub.status.busy": "2020-09-27T15:32:58.000660Z",
     "iopub.status.idle": "2020-09-27T15:32:58.006276Z",
     "shell.execute_reply": "2020-09-27T15:32:58.005489Z",
     "shell.execute_reply.started": "2020-09-27T15:32:58.000881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence\n",
    "# Integer assigned as end of sentence (EOS)\n",
    "# This will help us to infer the model has completed the translation\n",
    "EOS = 1\n",
    "\n",
    "# Generator helper function to append EOS to each sentence\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        \n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "        \n",
    "# Append EOS to the train data\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T16:38:24.487138Z",
     "iopub.status.busy": "2020-09-27T16:38:24.486897Z",
     "iopub.status.idle": "2020-09-27T16:38:24.648739Z",
     "shell.execute_reply": "2020-09-27T16:38:24.647991Z",
     "shell.execute_reply.started": "2020-09-27T16:38:24.487112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle tokenized example input:\u001b[0m [  549   617   117   479     9  1737     4   888  3550 30650  4729   992\n",
      "     1]\n",
      "\u001b[31mSingle tokenized example target:\u001b[0m [  328   468  5579    61 12657  3550 30650  4729   992     1]\n"
     ]
    }
   ],
   "source": [
    "# Filter long sentences\n",
    "# Filter too long sentences to not run out of memory\n",
    "# length_keys=[0, 1] means we filter both English and German sentences \n",
    "# Both must be not longer than 256 tokens for training / 512 for eval\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=256,\n",
    "    length_keys=[0, 1]\n",
    ")(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512,\n",
    "    length_keys=[0,1]\n",
    ")(tokenized_eval_stream)\n",
    "\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenize and Detokenize Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T16:49:13.715018Z",
     "iopub.status.busy": "2020-09-27T16:49:13.714775Z",
     "iopub.status.idle": "2020-09-27T16:49:13.721817Z",
     "shell.execute_reply": "2020-09-27T16:49:13.720947Z",
     "shell.execute_reply.started": "2020-09-27T16:49:13.714993Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    EOS = 1\n",
    "    inputs = next(trax.data.tokenize(\n",
    "        iter([input_str]),\n",
    "        vocab_file=vocab_file,\n",
    "        vocab_dir=vocab_dir\n",
    "    ))\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    integers = list(np.squeeze(integers))\n",
    "    EOS = 1\n",
    "    if(EOS in integers):\n",
    "        integers = integers[:integers.index(EOS)]\n",
    "        \n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T16:49:46.509982Z",
     "iopub.status.busy": "2020-09-27T16:49:46.509746Z",
     "iopub.status.idle": "2020-09-27T16:49:47.055802Z",
     "shell.execute_reply": "2020-09-27T16:49:47.055015Z",
     "shell.execute_reply.started": "2020-09-27T16:49:46.509957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle detokenized example input:\u001b[0m These measures should help to protect the environment.\n",
      "\n",
      "\u001b[31mSingle detokenized example target:\u001b[0m Diese Maßnahmen dienen dem Umweltschutz.\n",
      "\n",
      "\n",
      "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n",
      "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
     ]
    }
   ],
   "source": [
    "# Detokenize an input-target pair of tokenized sentences\n",
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print()\n",
    "\n",
    "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
    "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T16:54:19.902351Z",
     "iopub.status.busy": "2020-09-27T16:54:19.902143Z",
     "iopub.status.idle": "2020-09-27T16:54:19.907272Z",
     "shell.execute_reply": "2020-09-27T16:54:19.906046Z",
     "shell.execute_reply.started": "2020-09-27T16:54:19.902329Z"
    }
   },
   "source": [
    "### 1.4 Bucketing\n",
    "[Comprehensive Hands-on Guide to Sequence Model batching strategy: Bucketing technique](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T17:01:11.215053Z",
     "iopub.status.busy": "2020-09-27T17:01:11.214593Z",
     "iopub.status.idle": "2020-09-27T17:01:11.224849Z",
     "shell.execute_reply": "2020-09-27T17:01:11.224107Z",
     "shell.execute_reply.started": "2020-09-27T17:01:11.214994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bucketing to create streams of batches\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes\n",
    "# batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 256 sentences of length < 8, 128, if length is\n",
    "# between 8 and 16. and so on -- and only 2 if length is over 512\n",
    "boundaries = [8, 16, 32, 64, 128,256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 15, 8, 4, 2]\n",
    "\n",
    "# Create the generators\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, \n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries,\n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "# Add masking for the padding (0s)\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T17:21:46.067721Z",
     "iopub.status.busy": "2020-09-27T17:21:46.067381Z",
     "iopub.status.idle": "2020-09-27T17:21:46.138193Z",
     "shell.execute_reply": "2020-09-27T17:21:46.136923Z",
     "shell.execute_reply.started": "2020-09-27T17:21:46.067691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch data type:  <class 'numpy.ndarray'>\n",
      "target_batch data type:  <class 'numpy.ndarray'>\n",
      "input_batch shape:  (32, 64)\n",
      "target_batch shape:  (32, 64)\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# let's see the data type of a batch\n",
    "print(\"input_batch data type: \", type(input_batch))\n",
    "print(\"target_batch data type: \", type(target_batch))\n",
    "\n",
    "# let's see the shape of this particular batch (batch length, sentence length)\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tokens acquired are used to produce embedding vectors for each word in the sentence\n",
    "- Hence, the embedding for a sentence is a matrix\n",
    "- The number of sentence in each batch us usually a power of 2 for optimal computer memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T17:24:19.269448Z",
     "iopub.status.busy": "2020-09-27T17:24:19.269052Z",
     "iopub.status.idle": "2020-09-27T17:24:19.555965Z",
     "shell.execute_reply": "2020-09-27T17:24:19.555324Z",
     "shell.execute_reply.started": "2020-09-27T17:24:19.269407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
      "\u001b[0m HIV infection is a disease spread by contact with blood or sexual contact with an infected individual.\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
      " \u001b[0m [ 4188 17251    16    13  3126  4078    45  2412    30  6196    66  7660\n",
      "  2412    30    27 11123  1283  3550 30650  4729   992     1     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
      "\u001b[0m Die HIV-Infektion ist eine Erkrankung, die durch Kontakt mit infiziertem Blut oder durch sexuellen Kontakt mit HIV-Infizierten übertragen wird.\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
      "\u001b[0m [   57  4188    15 16015     5    24    41  9183  6818   147     2    10\n",
      "   121  5378    39 12258 11813   164 18689    97   121 16695  5378    39\n",
      "  4188    15 10614 23539   177  6011    78  3550 30650  4729   992     1\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
