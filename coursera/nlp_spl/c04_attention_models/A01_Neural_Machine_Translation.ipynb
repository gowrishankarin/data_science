{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "This article takes you through the core concepts of the following...\n",
    "- English to German translation using Neural Machine Translation(NMT)\n",
    "- It uses LSTM networks with Attention\n",
    "- Beyond translation MT deciphers word sense disambiguation (e.x. `bank` refers to `financial bank` or `riverside bank`\n",
    "- Implemented using RNN with LSTMs can work for short to medium sentences but can result in vanishing gradient for long sequences\n",
    "- To address this, an attention mechanism is used to allow the decoder to access all relevant parts of the input sentence regardless of its lenght\n",
    "\n",
    "1. Preprocess the training and eval data\n",
    "2. Implement an encoder-decoder system with attention\n",
    "3. Understand how attention works\n",
    "4. Build the NMT model from scratch using Trax\n",
    "5. Generate translations using `Greedy and Minimum Bayes Risk`(MBR) decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preparation\n",
    "\n",
    "### 1.1 Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:08.418067Z",
     "iopub.status.busy": "2020-09-28T19:38:08.417819Z",
     "iopub.status.idle": "2020-09-28T19:38:16.953276Z",
     "shell.execute_reply": "2020-09-28T19:38:16.952475Z",
     "shell.execute_reply.started": "2020-09-28T19:38:08.418036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n",
      "trax                          1.3.4\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "DATA_DIR = './data/01'\n",
    "\n",
    "!pip list | grep trax # trax == 1.3.4 is required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:16.955530Z",
     "iopub.status.busy": "2020-09-28T19:38:16.955229Z",
     "iopub.status.idle": "2020-09-28T19:38:17.162708Z",
     "shell.execute_reply": "2020-09-28T19:38:17.161939Z",
     "shell.execute_reply.started": "2020-09-28T19:38:16.955497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get generator function for the training set\n",
    "# This will download the train dataset if no data_dir is specified\n",
    "train_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=True\n",
    ")\n",
    "\n",
    "# Get generator function for the eval set\n",
    "eval_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:17.165775Z",
     "iopub.status.busy": "2020-09-28T19:38:17.165522Z",
     "iopub.status.idle": "2020-09-28T19:38:17.978995Z",
     "shell.execute_reply": "2020-09-28T19:38:17.978030Z",
     "shell.execute_reply.started": "2020-09-28T19:38:17.165739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Sverige BRISTOL-MYERS SQUIBB AB Tel: + 46 8 704 71 00\\n', b'Sverige BRISTOL-MYERS SQUIBB AB Tel: + 46 8 704 71 00\\n')\n",
      "\n",
      "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"
     ]
    }
   ],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization and Formatting\n",
    "- Tokenizing the sentences using subword representations\n",
    "- Each sentences is represented as an array of integers \n",
    "- To avoid out-of-vocab words, subword representations are used\n",
    "- For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed.\n",
    "- This allows it to be more flexible, wont have to save uncommon words explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:17.981153Z",
     "iopub.status.busy": "2020-09-28T19:38:17.980921Z",
     "iopub.status.idle": "2020-09-28T19:38:17.984495Z",
     "shell.execute_reply": "2020-09-28T19:38:17.983588Z",
     "shell.execute_reply.started": "2020-09-28T19:38:17.981129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variables that state the filename and directory of the vocabulary file\n",
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = './data/01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:17.986014Z",
     "iopub.status.busy": "2020-09-28T19:38:17.985791Z",
     "iopub.status.idle": "2020-09-28T19:38:17.991442Z",
     "shell.execute_reply": "2020-09-28T19:38:17.990746Z",
     "shell.execute_reply.started": "2020-09-28T19:38:17.985980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the dataset.\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:17.992845Z",
     "iopub.status.busy": "2020-09-28T19:38:17.992490Z",
     "iopub.status.idle": "2020-09-28T19:38:18.000277Z",
     "shell.execute_reply": "2020-09-28T19:38:17.998896Z",
     "shell.execute_reply.started": "2020-09-28T19:38:17.992816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence\n",
    "# Integer assigned as end of sentence (EOS)\n",
    "# This will help us to infer the model has completed the translation\n",
    "EOS = 1\n",
    "\n",
    "# Generator helper function to append EOS to each sentence\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        \n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "        \n",
    "# Append EOS to the train data\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:18.002107Z",
     "iopub.status.busy": "2020-09-28T19:38:18.001897Z",
     "iopub.status.idle": "2020-09-28T19:38:18.142116Z",
     "shell.execute_reply": "2020-09-28T19:38:18.141458Z",
     "shell.execute_reply.started": "2020-09-28T19:38:18.002083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle tokenized example input:\u001b[0m [14026  2801  3551 32955   135   150 14443 22008 21980   332 30650  4729\n",
      "   992     1]\n",
      "\u001b[31mSingle tokenized example target:\u001b[0m [14026  2801  3551 32955   135   150 14443 22008 21980   332 30650  4729\n",
      "   992     1]\n"
     ]
    }
   ],
   "source": [
    "# Filter long sentences\n",
    "# Filter too long sentences to not run out of memory\n",
    "# length_keys=[0, 1] means we filter both English and German sentences \n",
    "# Both must be not longer than 256 tokens for training / 512 for eval\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=256,\n",
    "    length_keys=[0, 1]\n",
    ")(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512,\n",
    "    length_keys=[0,1]\n",
    ")(tokenized_eval_stream)\n",
    "\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenize and Detokenize Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:18.146259Z",
     "iopub.status.busy": "2020-09-28T19:38:18.146036Z",
     "iopub.status.idle": "2020-09-28T19:38:18.152475Z",
     "shell.execute_reply": "2020-09-28T19:38:18.151752Z",
     "shell.execute_reply.started": "2020-09-28T19:38:18.146234Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    EOS = 1\n",
    "    inputs = next(trax.data.tokenize(\n",
    "        iter([input_str]),\n",
    "        vocab_file=vocab_file,\n",
    "        vocab_dir=vocab_dir\n",
    "    ))\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    integers = list(np.squeeze(integers))\n",
    "    EOS = 1\n",
    "    if(EOS in integers):\n",
    "        integers = integers[:integers.index(EOS)]\n",
    "        \n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:18.154366Z",
     "iopub.status.busy": "2020-09-28T19:38:18.154136Z",
     "iopub.status.idle": "2020-09-28T19:38:18.666579Z",
     "shell.execute_reply": "2020-09-28T19:38:18.665630Z",
     "shell.execute_reply.started": "2020-09-28T19:38:18.154342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle detokenized example input:\u001b[0m Tel: +421 2 57 103 777\n",
      "\n",
      "\u001b[31mSingle detokenized example target:\u001b[0m Tel: +421 2 57 103 777\n",
      "\n",
      "\n",
      "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n",
      "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
     ]
    }
   ],
   "source": [
    "# Detokenize an input-target pair of tokenized sentences\n",
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print()\n",
    "\n",
    "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
    "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T16:54:19.902351Z",
     "iopub.status.busy": "2020-09-27T16:54:19.902143Z",
     "iopub.status.idle": "2020-09-27T16:54:19.907272Z",
     "shell.execute_reply": "2020-09-27T16:54:19.906046Z",
     "shell.execute_reply.started": "2020-09-27T16:54:19.902329Z"
    }
   },
   "source": [
    "### 1.4 Bucketing\n",
    "[Comprehensive Hands-on Guide to Sequence Model batching strategy: Bucketing technique](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:18.667897Z",
     "iopub.status.busy": "2020-09-28T19:38:18.667549Z",
     "iopub.status.idle": "2020-09-28T19:38:18.673275Z",
     "shell.execute_reply": "2020-09-28T19:38:18.672592Z",
     "shell.execute_reply.started": "2020-09-28T19:38:18.667870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bucketing to create streams of batches\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes\n",
    "# batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 256 sentences of length < 8, 128, if length is\n",
    "# between 8 and 16. and so on -- and only 2 if length is over 512\n",
    "boundaries = [8, 16, 32, 64, 128,256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 15, 8, 4, 2]\n",
    "\n",
    "# Create the generators\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, \n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries,\n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "# Add masking for the padding (0s)\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:18.674438Z",
     "iopub.status.busy": "2020-09-28T19:38:18.674269Z",
     "iopub.status.idle": "2020-09-28T19:38:18.739312Z",
     "shell.execute_reply": "2020-09-28T19:38:18.738635Z",
     "shell.execute_reply.started": "2020-09-28T19:38:18.674418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch data type:  <class 'numpy.ndarray'>\n",
      "target_batch data type:  <class 'numpy.ndarray'>\n",
      "input_batch shape:  (32, 64)\n",
      "target_batch shape:  (32, 64)\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# let's see the data type of a batch\n",
    "print(\"input_batch data type: \", type(input_batch))\n",
    "print(\"target_batch data type: \", type(target_batch))\n",
    "\n",
    "# let's see the shape of this particular batch (batch length, sentence length)\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tokens acquired are used to produce embedding vectors for each word in the sentence\n",
    "- Hence, the embedding for a sentence is a matrix\n",
    "- The number of sentence in each batch us usually a power of 2 for optimal computer memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:18.740717Z",
     "iopub.status.busy": "2020-09-28T19:38:18.740438Z",
     "iopub.status.idle": "2020-09-28T19:38:19.001635Z",
     "shell.execute_reply": "2020-09-28T19:38:19.000976Z",
     "shell.execute_reply.started": "2020-09-28T19:38:18.740689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
      "\u001b[0m Following subcutaneous injection, onset of action is within 30 minutes, the phase of maximum action is between 1 and 4 hours after injection and the duration of action is 7 to 9 hours.\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
      " \u001b[0m [32162  5457  8347 20808  1183 28751     2  3192   487     7   588    16\n",
      "   374   448  1379     2     4  3377     7  4433   588    16   255   135\n",
      "     8   219  1476   288 28751     8     4 15384     7   588    16   332\n",
      "     9   444  1476  3550 30650  4729   992     1     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
      "\u001b[0m Bei subkutaner Gabe beginnt die Wirkung innerhalb von 30 Minuten, liegt die Phase maximaler Wirkung 1 bis 4 Stunden nach Injektion und die Wirkdauer zwischen 7 und 9 Stunden.\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
      "\u001b[0m [  752  5457  5261 22575    76 10639    35  6565    10  5891   824    21\n",
      "   448  2234     2   486    10  4899 23333    76  5891   135   248   219\n",
      "  2586   126  3633 19952   379    12    10 24953 29856   235   332    12\n",
      "   444  2586  3550 30650  4729   992     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. NMT with Attention\n",
    "\n",
    "### 2.1 Attention Overview\n",
    "- An attention model will be built using an encoder-decoder architecture\n",
    "- The RNN will take in a tokenized version of a sentence in its encoder.\n",
    "- Pass the tokenized data into the decoder for translation\n",
    "- Using a sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will degrade for longer ones\n",
    "- All the context of the input sentence is compressed into one vector and passed into a decoder block\n",
    "- Context of the first parts of the input will have very little effect on the final vector passed to the decoder\n",
    "\n",
    "$$ENCODER \\rightarrow \\small{hello}\\hspace{2mm} \\normalsize{how}\\hspace{2mm} \\large{are}\\hspace{2mm} \\Large{you}\\hspace{2mm} \\huge{today}\\hspace{2mm} \\Huge{!} \\normalsize \\rightarrow DECODER$$\n",
    "\n",
    "- Adding an attention layer to this model avoids this problme by giving the decoder access to all parts of the input sentence\n",
    "- In a 4 word input sentence,\n",
    "    - Remember that a hidden state is produced at each timestep of the encoder\n",
    "    - These hidden states are all passed to the attention layer and each are given a score given the current activation(ie hidden state) of the decoder\n",
    "    - ie After predicting the first word, the attention layer will receive all the encoder hidden states as well as decoder hiddent state when producing the word wie\n",
    "    - Given these information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word\n",
    "- The result of the model training might have learned that it should align to the second encoder hidden state a\n",
    "- Subsequently assigns a high probability to the word geht.\n",
    "- If we use greedy decoding, we will output the said word as the next symbol, \n",
    "- then restart the process to produce the next word until we reach an end sentence prediction\n",
    "\n",
    "This is Scaled Dot Product Attention of the form\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "- Computing scores using Queries(Q) and keys(K) followed by a multiplication values(V) to get a context vector at a particular timestep of the decoder\n",
    "- The context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word\n",
    "- The division by squre root of the keys dimensionality $(\\sqrt{d_k})$ is for improving model performance\n",
    "- The encoder activations (hidden states) will be the keys and values, while decoder activations will be queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Input Encoder\n",
    "- The input encoder runs on the input tokens, creates its embeddings and feeds it to an LSTM network.\n",
    "- This outputs the activations that will be the keys and values for attention\n",
    "- It is a serial network with `tl.Embedding` and `tl.LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:19.002972Z",
     "iopub.status.busy": "2020-09-28T19:38:19.002640Z",
     "iopub.status.idle": "2020-09-28T19:38:19.006708Z",
     "shell.execute_reply": "2020-09-28T19:38:19.006075Z",
     "shell.execute_reply.started": "2020-09-28T19:38:19.002950Z"
    }
   },
   "outputs": [],
   "source": [
    "def input_encoder_Fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    input_encoder = tl.Serial(\n",
    "        tl.Embedding(input_vocab_size, d_model),\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
    "    )\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:19.008059Z",
     "iopub.status.busy": "2020-09-28T19:38:19.007868Z",
     "iopub.status.idle": "2020-09-28T19:38:19.022759Z",
     "shell.execute_reply": "2020-09-28T19:38:19.022008Z",
     "shell.execute_reply.started": "2020-09-28T19:38:19.008036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "import w1_unittest\n",
    "\n",
    "w1_unittest.test_input_encoder_fn(input_encoder_Fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Pre-attention Decoder\n",
    "- The pre-attention decoder runs on the targets and creates activations that are used as queries in attention\n",
    "- This is a Serial netwokr which is composed of the `tl.ShiftRight`, `tl.Embedding`, `tl.LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:19.025041Z",
     "iopub.status.busy": "2020-09-28T19:38:19.024393Z",
     "iopub.status.idle": "2020-09-28T19:38:19.031023Z",
     "shell.execute_reply": "2020-09-28T19:38:19.029785Z",
     "shell.execute_reply.started": "2020-09-28T19:38:19.024993Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        # Shift right to insert start-of-sentence token and implement \n",
    "        # teacher forcing during training\n",
    "        tl.ShiftRight(mode),\n",
    "        tl.Embedding(target_vocab_size, d_model),\n",
    "        tl.LSTM(d_model)\n",
    "    )\n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T19:38:19.034857Z",
     "iopub.status.busy": "2020-09-28T19:38:19.034543Z",
     "iopub.status.idle": "2020-09-28T19:38:19.039466Z",
     "shell.execute_reply": "2020-09-28T19:38:19.038557Z",
     "shell.execute_reply.started": "2020-09-28T19:38:19.034824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
