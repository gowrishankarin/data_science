{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "This article takes you through the core concepts of the following...\n",
    "- English to German translation using Neural Machine Translation(NMT)\n",
    "- It uses LSTM networks with Attention\n",
    "- Beyond translation MT deciphers word sense disambiguation (e.x. `bank` refers to `financial bank` or `riverside bank`\n",
    "- Implemented using RNN with LSTMs can work for short to medium sentences but can result in vanishing gradient for long sequences\n",
    "- To address this, an attention mechanism is used to allow the decoder to access all relevant parts of the input sentence regardless of its lenght\n",
    "\n",
    "1. Preprocess the training and eval data\n",
    "2. Implement an encoder-decoder system with attention\n",
    "3. Understand how attention works\n",
    "4. Build the NMT model from scratch using Trax\n",
    "5. Generate translations using `Greedy and Minimum Bayes Risk`(MBR) decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preparation\n",
    "\n",
    "### 1.1 Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:40:51.355651Z",
     "iopub.status.busy": "2020-10-04T18:40:51.355334Z",
     "iopub.status.idle": "2020-10-04T18:40:56.811199Z",
     "shell.execute_reply": "2020-10-04T18:40:56.810355Z",
     "shell.execute_reply.started": "2020-10-04T18:40:51.355618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "DATA_DIR = './data/01'\n",
    "\n",
    "# !pip list | grep trax # trax == 1.3.4 is required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get generator function for the training set\n",
    "# This will download the train dataset if no data_dir is specified\n",
    "train_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=True\n",
    ")\n",
    "\n",
    "# Get generator function for the eval set\n",
    "eval_stream_fn = trax.data.TFDS(\n",
    "    'opus/medical',\n",
    "    data_dir=DATA_DIR,\n",
    "    keys=('en', 'de'),\n",
    "    eval_holdout_size=0.01, #1% for eval\n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization and Formatting\n",
    "- Tokenizing the sentences using subword representations\n",
    "- Each sentences is represented as an array of integers \n",
    "- To avoid out-of-vocab words, subword representations are used\n",
    "- For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed.\n",
    "- This allows it to be more flexible, wont have to save uncommon words explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables that state the filename and directory of the vocabulary file\n",
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = './data/01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset.\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence\n",
    "# Integer assigned as end of sentence (EOS)\n",
    "# This will help us to infer the model has completed the translation\n",
    "EOS = 1\n",
    "\n",
    "# Generator helper function to append EOS to each sentence\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        \n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "        \n",
    "# Append EOS to the train data\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter long sentences\n",
    "# Filter too long sentences to not run out of memory\n",
    "# length_keys=[0, 1] means we filter both English and German sentences \n",
    "# Both must be not longer than 256 tokens for training / 512 for eval\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=256,\n",
    "    length_keys=[0, 1]\n",
    ")(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512,\n",
    "    length_keys=[0,1]\n",
    ")(tokenized_eval_stream)\n",
    "\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenize and Detokenize Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    EOS = 1\n",
    "    inputs = next(trax.data.tokenize(\n",
    "        iter([input_str]),\n",
    "        vocab_file=vocab_file,\n",
    "        vocab_dir=vocab_dir\n",
    "    ))\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    integers = list(np.squeeze(integers))\n",
    "    EOS = 1\n",
    "    if(EOS in integers):\n",
    "        integers = integers[:integers.index(EOS)]\n",
    "        \n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize an input-target pair of tokenized sentences\n",
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print()\n",
    "\n",
    "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
    "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T16:54:19.902351Z",
     "iopub.status.busy": "2020-09-27T16:54:19.902143Z",
     "iopub.status.idle": "2020-09-27T16:54:19.907272Z",
     "shell.execute_reply": "2020-09-27T16:54:19.906046Z",
     "shell.execute_reply.started": "2020-09-27T16:54:19.902329Z"
    }
   },
   "source": [
    "### 1.4 Bucketing\n",
    "[Comprehensive Hands-on Guide to Sequence Model batching strategy: Bucketing technique](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing to create streams of batches\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes\n",
    "# batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 256 sentences of length < 8, 128, if length is\n",
    "# between 8 and 16. and so on -- and only 2 if length is over 512\n",
    "boundaries = [8, 16, 32, 64, 128,256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 15, 8, 4, 2]\n",
    "\n",
    "# Create the generators\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, \n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries,\n",
    "    batch_sizes,\n",
    "    length_keys=[0,1]\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "# Add masking for the padding (0s)\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# let's see the data type of a batch\n",
    "print(\"input_batch data type: \", type(input_batch))\n",
    "print(\"target_batch data type: \", type(target_batch))\n",
    "\n",
    "# let's see the shape of this particular batch (batch length, sentence length)\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tokens acquired are used to produce embedding vectors for each word in the sentence\n",
    "- Hence, the embedding for a sentence is a matrix\n",
    "- The number of sentence in each batch us usually a power of 2 for optimal computer memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. NMT with Attention\n",
    "\n",
    "### 2.1 Attention Overview\n",
    "- An attention model will be built using an encoder-decoder architecture\n",
    "- The RNN will take in a tokenized version of a sentence in its encoder.\n",
    "- Pass the tokenized data into the decoder for translation\n",
    "- Using a sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will degrade for longer ones\n",
    "- All the context of the input sentence is compressed into one vector and passed into a decoder block\n",
    "- Context of the first parts of the input will have very little effect on the final vector passed to the decoder\n",
    "\n",
    "$$ENCODER \\rightarrow \\small{hello}\\hspace{2mm} \\normalsize{how}\\hspace{2mm} \\large{are}\\hspace{2mm} \\Large{you}\\hspace{2mm} \\huge{today}\\hspace{2mm} \\Huge{!} \\normalsize \\rightarrow DECODER$$\n",
    "\n",
    "- Adding an attention layer to this model avoids this problme by giving the decoder access to all parts of the input sentence\n",
    "- In a 4 word input sentence,\n",
    "    - Remember that a hidden state is produced at each timestep of the encoder\n",
    "    - These hidden states are all passed to the attention layer and each are given a score given the current activation(ie hidden state) of the decoder\n",
    "    - ie After predicting the first word, the attention layer will receive all the encoder hidden states as well as decoder hiddent state when producing the word wie\n",
    "    - Given these information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word\n",
    "- The result of the model training might have learned that it should align to the second encoder hidden state a\n",
    "- Subsequently assigns a high probability to the word geht.\n",
    "- If we use greedy decoding, we will output the said word as the next symbol, \n",
    "- then restart the process to produce the next word until we reach an end sentence prediction\n",
    "\n",
    "This is Scaled Dot Product Attention of the form\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "- Computing scores using Queries(Q) and keys(K) followed by a multiplication values(V) to get a context vector at a particular timestep of the decoder\n",
    "- The context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word\n",
    "- The division by squre root of the keys dimensionality $(\\sqrt{d_k})$ is for improving model performance\n",
    "- The encoder activations (hidden states) will be the keys and values, while decoder activations will be queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Input Encoder\n",
    "- The input encoder runs on the input tokens, creates its embeddings and feeds it to an LSTM network.\n",
    "- This outputs the activations that will be the keys and values for attention\n",
    "- It is a serial network with `tl.Embedding` and `tl.LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:40:56.812911Z",
     "iopub.status.busy": "2020-10-04T18:40:56.812679Z",
     "iopub.status.idle": "2020-10-04T18:40:56.817300Z",
     "shell.execute_reply": "2020-10-04T18:40:56.816505Z",
     "shell.execute_reply.started": "2020-10-04T18:40:56.812882Z"
    }
   },
   "outputs": [],
   "source": [
    "def input_encoder_Fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    input_encoder = tl.Serial(\n",
    "        tl.Embedding(input_vocab_size, d_model),\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
    "    )\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:40:56.819291Z",
     "iopub.status.busy": "2020-10-04T18:40:56.819072Z",
     "iopub.status.idle": "2020-10-04T18:40:56.831312Z",
     "shell.execute_reply": "2020-10-04T18:40:56.830474Z",
     "shell.execute_reply.started": "2020-10-04T18:40:56.819266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "import w1_unittest\n",
    "\n",
    "w1_unittest.test_input_encoder_fn(input_encoder_Fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Pre-attention Decoder\n",
    "- The pre-attention decoder runs on the targets and creates activations that are used as queries in attention\n",
    "- This is a Serial netwokr which is composed of the `tl.ShiftRight`, `tl.Embedding`, `tl.LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        # Shift right to insert start-of-sentence token and implement \n",
    "        # teacher forcing during training\n",
    "        tl.ShiftRight(1),\n",
    "        tl.Embedding(target_vocab_size, d_model),\n",
    "        tl.LSTM(d_model)\n",
    "    )\n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Preparing the Attention Input\n",
    "- This function will prepare the input for the attention layer\n",
    "- Take encoder and pre-attention decoder activations and assign it to the queries, ie hidden states of encoder and decoder\n",
    "- These activations are assigned to Queries, Keys and Values\n",
    "- Another output will be the mask to distinguish real tokens from padding tokens\n",
    "- This mask is used internally by Trax while computing the softmax so padding tokens  will not have an effect on the computed probabilities\n",
    "- Observe which tokens in the input corresponding to padding\n",
    "- Multi-headed attention - Computing attention multiple times to improve model's predictions\n",
    "- It is required to consider this additional axis in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    \"\"\"\n",
    "    Prepare Queries, Keys, Values and Mask for attention\n",
    "    Args:\n",
    "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
    "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
    "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
    "    \n",
    "    Returns:\n",
    "        queries, keys, values and mask for attention.\n",
    "    \"\"\"\n",
    "    # Set the keys and values to the encoder activations\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "    \n",
    "    # Set the queries to the decoder activations\n",
    "    queries = decoder_activations\n",
    "    \n",
    "    # Generate the mask to distinguish real tokens from padding\n",
    "    # hint: inputs is 1 for real tokens and 0 where they are pdding\n",
    "    mask = (inputs >= 1)\n",
    "    \n",
    "    # add axes to the mask for attention heads and decoder length\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    \n",
    "    # Broadcast so mask shape is [batch_size, attention heads, decoder-len, encoder-len]\n",
    "    # Note: for thsi assignment, attention head is set to 1\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_unittest.test_prepare_attention_input(prepare_attention_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implementation Overview\n",
    "- Step 0: Prepare input encoder and pre-attention decoder branches\n",
    "- Step 1: Create a Serial Network, It will stack the layers in the next steps one after the other.\n",
    "- Step 2: Make a copy of the input and target tokens\n",
    "    - The input and target tokens will be fed into different layers of the model\n",
    "    - `tl.Select` layer to create copies oft these tokens\n",
    "    - Arrange them as `[input_tokens, target_tokens, input_tokens, target_tokens]`\n",
    "- Step 3: Create a parallel branch to feed the input tokens to the `input encoder` and target tokens to the `pre-attention decoder`\n",
    "    - `tl.Parallel` to create these sublayers in parallel\n",
    "- Step 4: Call the `prepare_attention_input` function to convert the encoder and pre-attention decoder actiations to a format that the attention layer will accept\n",
    "    - `tl.Fn` to be used.\n",
    "- Step 5: Feed Q, K V and Mask to the tl.AttentionQKV layer\n",
    "    - This computes the scaled dot product attention and outputs the attention weights and mask\n",
    "    - Though it is a one liner, It composed of a DNN made up of several branches\n",
    "    - Having deep layers pose the risk of vanishing gradients during training - to be mitigated\n",
    "    - `tl.Residual` layer is added to improve ability to learn - Added to output of `AttentionQKV` with the queries input\n",
    "    - Nest the `AttentionQKV` inside `Residual`\n",
    "- Step 6: Mask is not needed, hence dropped.\n",
    "    - At this point in network - the signal stack currently has `[attention activations, mask, target tokens]` and you can use `tl.Select` to output just 3\n",
    "- Step 7: Feed the attention weighted output to the LSTM decoder. \n",
    "    - Stack multiple `tl.LSTM` layers to improve the output so remember to append to LSTMs equal to n_decoder_layers of the model\n",
    "- Step 8: Determine the probabilities of each subword in the vocabulary and set this up easily with a `tl.Dense` layer  by making its size equal to the size of our vocabulary\n",
    "- Step 9: Normalize the output to log probabilities by passing the activations in Step 8 to a `tl.LogSoftmax` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMTAttn(\n",
    "    input_vocab_size=33300, \n",
    "    target_vocab_size=33300, \n",
    "    d_model=1024, \n",
    "    n_encoder_layers=2, \n",
    "    n_decoder_layers=4,\n",
    "    n_attention_heads=4,\n",
    "    attention_dropout=0.0, \n",
    "    mode='train'\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns an LSTM sequence-to-sequence model with attention\n",
    "    \n",
    "    The input to the model is a pair (input tokens, target tokens)\n",
    "    e.g. an English sentence (tokenized) and its translation into German\n",
    "    \n",
    "    Args:\n",
    "    input_vocab_size: int: vocab size of the input\n",
    "    target_vocab_size: int: vocab size of the target\n",
    "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
    "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
    "    n_attention_heads: int: number of attention heads\n",
    "    attention_dropout: float, dropout for the attention layer\n",
    "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 0: call the helper function to create layers for the input encoder and pre attention decoder\n",
    "    input_encoder = input_encoder_Fn(input_vocab_size, d_model, n_encoder_layers)\n",
    "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
    "    \n",
    "    # Step 1: Create a serial network\n",
    "    model = tl.Serial(\n",
    "        # Copy input tokens and target tokens as they will be needed later\n",
    "        tl.Select([0, 1, 0, 1]),\n",
    "        # Run input encoder on the input and pre-attention decoder the target\n",
    "        tl.Parallel(input_encoder, pre_attention_decoder),\n",
    "        # Prepare queries, keys, values and mask for attention\n",
    "        tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
    "        # Run the AttentionQKV layer\n",
    "        # nest it inside a Residual layer to add to the pre-attention decoder activations\n",
    "        tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "        # Drop attention mask\n",
    "        tl.Select([0,2], n_in=None),\n",
    "        # Run the rest of the RNN decoder\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)],\n",
    "        # Prepare output by making it to the right size\n",
    "        tl.Dense(target_vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "        \n",
    "    )\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMTAttn()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_unittest.test_NMTAttn(NMTAttn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training\n",
    "- Train the model using supervised learning.\n",
    "- Classes `TrainTask`, `EvalTask` and `Loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = training.TrainTask(\n",
    "    # use the train batch stream as labeled data\n",
    "    labeled_data = train_batch_stream,\n",
    "    # Use the cross entropy loss\n",
    "    loss_layer = tl.CrossEntropyLoss(),\n",
    "    # use the Adam optimizer with learning rate of 0.01\n",
    "    optimizer = trax.optimizers.Adam(0.01),\n",
    "    # learning rate schedule have 1000 warmup steps with a max value of 0.01\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(1000, max_value=0.01),\n",
    "    # have a checkpoint every 10 steps\n",
    "    n_steps_per_checkpoint = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_unittest.test_train_task(train_task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Eval Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_task = training.EvalTask(\n",
    "    labeled_data=eval_batch_stream,\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loop\n",
    "The Loop class defines the model we trains as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './data/01/output_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!rm -f ~/output_dir/model.pkl.gz\n",
    "\n",
    "training_loop = training.Loop(\n",
    "    NMTAttn(mode='train'),\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Testing\n",
    "- Method 1: Identified the next symbol ie output token\n",
    "- Method 2: Combines the entire translation string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_file = os.path.join(output_dir, 'model.pkl.gz')\n",
    "# Instantiate the model we built in eval mode\n",
    "model = NMTAttn(mode='eval')\n",
    "\n",
    "# intialize weights from a pre-trained model\n",
    "model.init_from_file(model_file, weights_only=True)\n",
    "model = tl.Accelerate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Decoding\n",
    "- There are several ways to get the next token when translating a sentence\n",
    "- Get the most probable token at each step(ie Greedy Decoding) or get a sample from a distribution\n",
    "- We can generalize the implementation of these two approaches by using `tl.LogSoftmax_Sample()`\n",
    "\n",
    "```python\n",
    "def logsoftmax_sample(log_probs, temperature=1.0):  # pylint: disable=invalid-name\n",
    "  \"\"\"Returns a sample from a log-softmax output, with temperature.\n",
    "\n",
    "  Args:\n",
    "    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n",
    "    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n",
    "  \"\"\"\n",
    "  # This is equivalent to sampling from a softmax with temperature.\n",
    "  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "  g = -np.log(-np.log(u))\n",
    "  return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "```\n",
    "\n",
    "Take aways...  \n",
    "1. It gets random samples with the same shape as your input(i.e log_probs)\n",
    "2. The amount of \"noise\" added to input by these random samples is scaled by a `temperature` setting, set to 0\n",
    "3. It makes the return statement equal to getting the argmax of `log_probs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
    "    \"\"\"\n",
    "    Returns the index of the next token\n",
    "    \n",
    "    Args:\n",
    "    NMTAttn: An LSTM Sequence to sequence model with attention\n",
    "    input_tokens: (np.ndarray 1 x n_tokens): tokenized representation of the input sequence\n",
    "    cur_output_tokens(list): Tokenized representation of previously translated words\n",
    "    temperature: Parameter for sampling ranging from 0.0 to 1.0\n",
    "        0.0: Same as argmax, always pic the most probable token\n",
    "        1.0: Sampling from the distribution (can sometimes say random things)\n",
    "    \n",
    "    Returns:\n",
    "    int: index of the next token in the translated sentence\n",
    "    float: log probability of the next symbod\n",
    "    \"\"\"\n",
    "    # Set the length of the current output tokens\n",
    "    token_length = len(cur_output_tokens)\n",
    "    \n",
    "    # Calculate next power of 2 for padding length\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length+1)))\n",
    "    \n",
    "    # Pad curr_output_tokens up to the padded_length\n",
    "    padded = cur_output_tokens + (padded_length - token_length) * [0]\n",
    "     \n",
    "    # Model expects the output to have an axis for the batch size in front so\n",
    "    # convert `padded` list to a numpy array  with shape (x, <pad_lenght>) where the\n",
    "    # x position is the batch axis. np.expand_dims() with axis=0\n",
    "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
    "    \n",
    "    # Get the model prediction. remember to use the NMAttn argument defined above\n",
    "    # hit: the model accepts  tuple as input\n",
    "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
    "    \n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
    "    \n",
    "    return symbol, float(log_probs[symbol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_unittest.test_next_symbol(next_symbol, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_decode(input_sentence, NMTAttn=None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"\n",
    "    Returns the translated sentence\n",
    "    \n",
    "    Args:\n",
    "    input_sentence(str): Sentence to translate\n",
    "    NMTAttn(tl.Serial): An LSTM sequence-to-sequence model with attention \n",
    "    temperature(float): parameter for sampling ranging from 0.0 to 1.0\n",
    "        0.0: Same as argmax, always pick the most proabable token\n",
    "        1.0: Sampling from the distribution (can sometimes sya random things)\n",
    "    vocab_file (str): filename of the vocabulary\n",
    "    vocab_dir (str): path to the vocabulary file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (list, str, float)\n",
    "    list of int: tokenized version of the translated sentence\n",
    "    float: log probability of the translated sentence\n",
    "    str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    input_tokens = tokenize(input_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "    # initialize the list of output tokens\n",
    "    cur_output_tokens = []\n",
    "    \n",
    "    cur_output = 0\n",
    "    # Set the encoding of end of sentence as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # Check that the current output is not the end of sentence token\n",
    "    while(cur_output != EOS):\n",
    "        # Update the current output token by getting the index of the next workd\n",
    "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
    "        \n",
    "        # Append the current output token to the list of output tokens\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        \n",
    "    # detokenize the output tokens\n",
    "    sentence = trax.data.detokenize(cur_output_tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "    \n",
    "    return cur_output_tokens, log_prob, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_decode(\"I love languages.\", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decode Test\n",
    "- We have se a default value of 0 to the temperature setting in our implementation of `sampling_decode()` above\n",
    "- `logsoftmax_sample()` method ultimately result in greedy decoding\n",
    "- The algorithm generates the translation by getting the most probable word at each step\n",
    "- It gets the argmax of the output array of your model adn then returns the index\n",
    "- In the test below, output remains the same eachtime you run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"\n",
    "    Prints the input and output of our NMTAttn model using Greedy decode\n",
    "    Args:\n",
    "    sentence (str): A custom string\n",
    "    NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention\n",
    "    vocab_file (str): filename of the vocabulary\n",
    "    vocab_dir (str): path to the vocabulary file\n",
    "    \n",
    "    Returns:\n",
    "    str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    _, _, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "    \n",
    "    print(f\"English: {sentence}\")\n",
    "    print(f\"German: {translated_sentence}\")\n",
    "    \n",
    "    return translated_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_sentence = \"I love languages\"\n",
    "greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Minimum Bayes-Risk Decoding\n",
    "Getting most probable token at each step may not necessarily produce the best results. Another approach is to do minimum Bayes Risk Decoding or MBR. Steps are\n",
    "\n",
    "1. Take several random samples\n",
    "2. Score each sample against all other samples\n",
    "3. Selecte the one with the highest score\n",
    "\n",
    "#### 4.2.1 Generating Samples\n",
    "Build a function to generate several samples. Use the `sampling_decode()` function you developed earlier to do this easily. We want to record the token list and log probability for each sample as these will be needed in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
    "    samples, log_probs = [], []\n",
    "    \n",
    "    # run a for loop to generate n samples\n",
    "    for _ in range(n_samples):\n",
    "        # Get a sample using the sampling_decode() function\n",
    "        sample, logp, _ = sampling_decode(\n",
    "            sentence,\n",
    "            NMTAttn, \n",
    "            temperature,\n",
    "            vocab_file=vocab_file,\n",
    "            vocab_dir=vocab_dir\n",
    "        )\n",
    "        # Append the token_list to the samples list\n",
    "        samples.append(sample)\n",
    "        # Append the log probability to the log_probs list\n",
    "        log_probs.append(logp)\n",
    "        \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 4 samples with the default temperature (0.6)\n",
    "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2.2 Comparing Overlaps\n",
    "- Build a function to compare a sample against another\n",
    "- Jaccard similarity, a simple method - Gets the intersection over union of two sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:45:39.996420Z",
     "iopub.status.busy": "2020-10-04T18:45:39.996181Z",
     "iopub.status.idle": "2020-10-04T18:45:40.001535Z",
     "shell.execute_reply": "2020-10-04T18:45:40.000805Z",
     "shell.execute_reply.started": "2020-10-04T18:45:39.996394Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "    \"\"\"\n",
    "    Returns the Jaccard Similarity between two token lists\n",
    "    Args:\n",
    "        candidate (list of int): tokenized version of the candidate translation\n",
    "        reference (list of int): tokenized version of the reference candidate\n",
    "        \n",
    "    Returns:\n",
    "        float: Overlap between the two token lists\n",
    "    \"\"\"\n",
    "    # Convert the lists to a set to get the unique tokens\n",
    "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)\n",
    "    \n",
    "    # Get the set of tokens common to both candidate and reference\n",
    "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
    "    \n",
    "    # Get teh set of all tokens found in either candidate or reference\n",
    "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
    "    \n",
    "    # Divide the number of joint elements by the number of all elements\n",
    "    overlap = len(joint_elems) / len(all_elems)\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:45:41.682825Z",
     "iopub.status.busy": "2020-10-04T18:45:41.682567Z",
     "iopub.status.idle": "2020-10-04T18:45:41.687585Z",
     "shell.execute_reply": "2020-10-04T18:45:41.686879Z",
     "shell.execute_reply.started": "2020-10-04T18:45:41.682797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try using the function. remember the result here and compare with the next function below.\n",
    "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more commonly used metrics in machine translation is ROUGE score. For unigrams, this called ROUGE-1 and as shown in class, you can ouput the socres for both precision and recall when comparing two samples. To get the final score, an F1-score is computed\n",
    "\n",
    "$$score = 2 * \\frac{(precision * recall)}{(precision + recall)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:48:28.583389Z",
     "iopub.status.busy": "2020-10-04T18:48:28.583148Z",
     "iopub.status.idle": "2020-10-04T18:48:28.590928Z",
     "shell.execute_reply": "2020-10-04T18:48:28.589853Z",
     "shell.execute_reply.started": "2020-10-04T18:48:28.583363Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def rouge1_similarity(system, reference):\n",
    "    \n",
    "    # Make a frequency table of the system tokens \n",
    "    sys_counter = Counter(system)\n",
    "\n",
    "    # Make a frequency table of the reference tokens\n",
    "    ref_counter = Counter(reference)\n",
    "    # initialize overlap to 0\n",
    "    overlap = 0\n",
    "    \n",
    "    # Run a for loop over the sys_counter object\n",
    "    for token in sys_counter:\n",
    "        # Lookup the value of the token in the sys_counter dictionary\n",
    "        token_count_sys = sys_counter.get(token)\n",
    "        \n",
    "        # Lookup the value of the token in the ref_counter dictionary\n",
    "        token_count_ref = ref_counter.get(token) if token in ref_counter else 0\n",
    "        \n",
    "        # Update the overlap by getting the smaller number between the two token counts above\n",
    "        overlap += (token_count_sys if token_count_sys <= token_count_ref else token_count_ref)\n",
    "        \n",
    "    # Get the precision\n",
    "    precision = overlap / len(system)\n",
    "    recall = overlap / len(reference)\n",
    "    \n",
    "    if(precision + recall != 0):\n",
    "        rouge1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        rouge1_score = 0\n",
    "    \n",
    "    return rouge1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:48:28.914742Z",
     "iopub.status.busy": "2020-10-04T18:48:28.914513Z",
     "iopub.status.idle": "2020-10-04T18:48:28.919420Z",
     "shell.execute_reply": "2020-10-04T18:48:28.918701Z",
     "shell.execute_reply.started": "2020-10-04T18:48:28.914717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice that this produces a different value from the jaccard similarity earlier\n",
    "rouge1_similarity([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:48:29.201302Z",
     "iopub.status.busy": "2020-10-04T18:48:29.201058Z",
     "iopub.status.idle": "2020-10-04T18:48:29.204839Z",
     "shell.execute_reply": "2020-10-04T18:48:29.204100Z",
     "shell.execute_reply.started": "2020-10-04T18:48:29.201276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_rouge1_similarity(rouge1_similarity)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Overall score\n",
    "\n",
    "We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
    "\n",
    "1. Get similarity score between sample 1 and sample 2\n",
    "2. Get similarity score between sample 1 and sample 3\n",
    "3. Get similarity score between sample 1 and sample 4\n",
    "4. Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
    "5. Iterate and repeat until samples 1 to 4 have overall scores.\n",
    "\n",
    "We will be storing the results in a dictionary for easy lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:53:50.624763Z",
     "iopub.status.busy": "2020-10-04T18:53:50.624523Z",
     "iopub.status.idle": "2020-10-04T18:53:50.630470Z",
     "shell.execute_reply": "2020-10-04T18:53:50.629704Z",
     "shell.execute_reply.started": "2020-10-04T18:53:50.624738Z"
    }
   },
   "outputs": [],
   "source": [
    "def average_overlap(similarity_fn, samples, *ignore_params):\n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):\n",
    "        \n",
    "        # Init to 0.0\n",
    "        overlap = 0\n",
    "        \n",
    "        # Run a for loop for each sample\n",
    "        for index_sample, sample in enumerate(samples):\n",
    "            # Skip if the candidate index is the same as the sample index\n",
    "            if(index_candidate == index_sample):\n",
    "                continue\n",
    "                \n",
    "            # Get the overlap between candidate and sample using the\n",
    "            # similarity function\n",
    "            sample_overlap = similarity_fn(sample, candidate)\n",
    "            \n",
    "            overlap += sample_overlap\n",
    "            \n",
    "        # Get the score for the candidate by computing the average\n",
    "        score = overlap / (len(samples) - 1)\n",
    "        scores[index_candidate] = score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:53:51.072459Z",
     "iopub.status.busy": "2020-10-04T18:53:51.072220Z",
     "iopub.status.idle": "2020-10-04T18:53:51.077997Z",
     "shell.execute_reply": "2020-10-04T18:53:51.077092Z",
     "shell.execute_reply.started": "2020-10-04T18:53:51.072433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.45, 1: 0.625, 2: 0.575}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:53:51.665886Z",
     "iopub.status.busy": "2020-10-04T18:53:51.665645Z",
     "iopub.status.idle": "2020-10-04T18:53:51.669889Z",
     "shell.execute_reply": "2020-10-04T18:53:51.669059Z",
     "shell.execute_reply.started": "2020-10-04T18:53:51.665860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_average_overlap(average_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. We have implemented it below and you can use it in your experiements to see which one will give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:46:09.706751Z",
     "iopub.status.busy": "2020-10-04T18:46:09.706514Z",
     "iopub.status.idle": "2020-10-04T18:46:09.712860Z",
     "shell.execute_reply": "2020-10-04T18:46:09.712206Z",
     "shell.execute_reply.started": "2020-10-04T18:46:09.706726Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):\n",
    "        \n",
    "        # Init to 0.0\n",
    "        overlap, weighted_sum = 0.0, 0.0\n",
    "        \n",
    "        # Run a for loop for each sample\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "            # Skip if the candidate index is the same as the sample index\n",
    "            if(index_candidate == index_sample):\n",
    "                continue\n",
    "                \n",
    "            # Convert log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "            \n",
    "            # Update the weighted sum\n",
    "            weighted_sum += sample_p\n",
    "                \n",
    "            # Get the overlap between candidate and sample using the\n",
    "            # similarity function\n",
    "            sample_overlap = similarity_fn(sample, candidate)\n",
    "            \n",
    "            overlap += sample_overlap * sample_p\n",
    "            \n",
    "        # Get the score for the candidate by computing the average\n",
    "        score = overlap / weighted_sum\n",
    "        scores[index_candidate] = score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T18:46:10.601327Z",
     "iopub.status.busy": "2020-10-04T18:46:10.601088Z",
     "iopub.status.idle": "2020-10-04T18:46:10.606430Z",
     "shell.execute_reply": "2020-10-04T18:46:10.605812Z",
     "shell.execute_reply.started": "2020-10-04T18:46:10.601306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.44255574831883415, 1: 0.631244796869735, 2: 0.5575581009406329}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Putting it all together\n",
    "\n",
    "We will now put everything together and develop the `mbr_decode()` function. Please use the helper functions you just developed to complete this. You will want to generate samples, get the score for each sample, get the highest score among all samples, then detokenize this sample to get the translated sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_decode(\n",
    "    sentence, n_samples, score_fn, similarity_fn, \n",
    "    NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None\n",
    "):\n",
    "    \n",
    "    samples, log_probs = generate_samples(\n",
    "        sentence, n_samples, NMTAttn, \n",
    "        temperature, vocab_file, vocab_dir\n",
    "    )\n",
    "    \n",
    "    # Use the scoring function to get a dictionary of scores\n",
    "    # pass in the relevant parameters as shown in the function definition of\n",
    "    # the mean methods you developed earlier\n",
    "    scores = weighted_avg_overlap(similarity_fn, samples, log_probs)\n",
    "    \n",
    "    # find the key with the highest score\n",
    "    max_index = np.argmax(scores)\n",
    "    \n",
    "    # detokenize the token list associated with the max_index\n",
    "    translated_sentence = trax.data.detokenize(samples[max_index])\n",
    "\n",
    "    return (translated_sentence, max_index, scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 1.0\n",
    "\n",
    "# put a custom string here\n",
    "your_sentence = 'She speaks English and German.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr_decode('Congratulations!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr_decode('You have completed the assignment!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_mbr_decode(mbr_decode, model)\n",
    "# END UNIT TEST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
