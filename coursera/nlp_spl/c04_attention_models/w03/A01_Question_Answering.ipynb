{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering - Text to Text Transfer from Transformers(T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:50:53.872003Z",
     "iopub.status.busy": "2020-10-24T05:50:53.871678Z",
     "iopub.status.idle": "2020-10-24T05:51:02.415731Z",
     "shell.execute_reply": "2020-10-24T05:51:02.415201Z",
     "shell.execute_reply.started": "2020-10-24T05:50:53.871961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import string\n",
    "import textwrap\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.supervised import decoding\n",
    "\n",
    "# Wil come handy later\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:02.417170Z",
     "iopub.status.busy": "2020-10-24T05:51:02.416993Z",
     "iopub.status.idle": "2020-10-24T05:51:02.422028Z",
     "shell.execute_reply": "2020-10-24T05:51:02.421388Z",
     "shell.execute_reply.started": "2020-10-24T05:51:02.417150Z"
    }
   },
   "outputs": [],
   "source": [
    "example_jsons = list(map(ast.literal_eval, open('data/data.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:02.425171Z",
     "iopub.status.busy": "2020-10-24T05:51:02.424802Z",
     "iopub.status.idle": "2020-10-24T05:51:02.430225Z",
     "shell.execute_reply": "2020-10-24T05:51:02.429019Z",
     "shell.execute_reply.started": "2020-10-24T05:51:02.425133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example number 1: \n",
      "\n",
      "{'content-length': b'1970', 'content-type': b'text/plain', 'text': b'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': b'2019-04-25T12:57:54Z', 'url': b'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'} \n",
      "\n",
      "example number 2: \n",
      "\n",
      "{'content-length': b'12064', 'content-type': b'text/plain', 'text': b'Discussion in \\'Mac OS X Lion (10.7)\\' started by axboi87, Jan 20, 2012.\\nI\\'ve got a 500gb internal drive and a 240gb SSD.\\nWhen trying to restore using disk utility i\\'m given the error \"Not enough space on disk ____ to restore\"\\nBut I shouldn\\'t have to do that!!!\\nAny ideas or workarounds before resorting to the above?\\nUse Carbon Copy Cloner to copy one drive to the other. I\\'ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\\'t be bootable. CCC usually works in \"file mode\" and it can easily copy a larger drive (that\\'s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\\nI\\'ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\\'t fit is there was slightly more than 4 GB of data.', 'timestamp': b'2019-04-21T10:07:13Z', 'url': b'https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/'} \n",
      "\n",
      "example number 3: \n",
      "\n",
      "{'content-length': b'5235', 'content-type': b'text/plain', 'text': b'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.', 'timestamp': b'2019-04-25T10:40:23Z', 'url': b'https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way'} \n",
      "\n",
      "example number 4: \n",
      "\n",
      "{'content-length': b'4967', 'content-type': b'text/plain', 'text': b\"How many backlinks per day for new site?\\nDiscussion in 'Black Hat SEO' started by Omoplata, Dec 3, 2010.\\n1) for a newly created site, what's the max # backlinks per day I should do to be safe?\\n2) how long do I have to let my site age before I can start making more blinks?\\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?\", 'timestamp': b'2019-04-21T12:46:19Z', 'url': b'https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/'} \n",
      "\n",
      "example number 5: \n",
      "\n",
      "{'content-length': b'4499', 'content-type': b'text/plain', 'text': b'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\\xe2\\x80\\x99s included in the mill levy measure.', 'timestamp': b'2019-04-20T14:33:21Z', 'url': b'http://bond.dpsk12.org/category/news/'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the examples to see how the data looks like\n",
    "for i in range(5):\n",
    "    print(f'example number {i+1}: \\n\\n{example_jsons[i]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:02.432765Z",
     "iopub.status.busy": "2020-10-24T05:51:02.432422Z",
     "iopub.status.idle": "2020-10-24T05:51:02.440941Z",
     "shell.execute_reply": "2020-10-24T05:51:02.439941Z",
     "shell.execute_reply.started": "2020-10-24T05:51:02.432737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example_jsons[0].get('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:02.442672Z",
     "iopub.status.busy": "2020-10-24T05:51:02.442301Z",
     "iopub.status.idle": "2020-10-24T05:51:02.451170Z",
     "shell.execute_reply": "2020-10-24T05:51:02.450031Z",
     "shell.execute_reply.started": "2020-10-24T05:51:02.442633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grab text field from dictionary\n",
    "natural_language_texts = [example_json['text'] for example_json in example_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:02.453326Z",
     "iopub.status.busy": "2020-10-24T05:51:02.452897Z",
     "iopub.status.idle": "2020-10-24T05:51:02.458253Z",
     "shell.execute_reply": "2020-10-24T05:51:02.457526Z",
     "shell.execute_reply.started": "2020-10-24T05:51:02.453296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\\xe2\\x80\\x99s included in the mill levy measure.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First text example\n",
    "natural_language_texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:02.460411Z",
     "iopub.status.busy": "2020-10-24T05:51:02.459841Z",
     "iopub.status.idle": "2020-10-24T05:51:02.469673Z",
     "shell.execute_reply": "2020-10-24T05:51:02.467941Z",
     "shell.execute_reply.started": "2020-10-24T05:51:02.460364Z"
    }
   },
   "outputs": [],
   "source": [
    "PAD, EOS, UNK = 0, 1, 2\n",
    "def detokenize(np_array):\n",
    "    return trax.data.detokenize(\n",
    "        np_array, \n",
    "        vocab_type='sentencepiece',\n",
    "        vocab_file='sentencepiece.model',\n",
    "        vocab_dir='./data/'\n",
    "    )\n",
    "\n",
    "def tokenize(s):\n",
    "    # The trax.data.tokenize function operates on streams\n",
    "    # theat why we have to create 1-element stream with iter\n",
    "    # and later retrieve the result with next\n",
    "    return next(\n",
    "        trax.data.tokenize(\n",
    "            iter([s]),\n",
    "            vocab_type='sentencepiece',\n",
    "            vocab_file='sentencepiece.model',\n",
    "            vocab_dir='./data/'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:02.471758Z",
     "iopub.status.busy": "2020-10-24T05:51:02.471480Z",
     "iopub.status.idle": "2020-10-24T05:51:10.294140Z",
     "shell.execute_reply": "2020-10-24T05:51:10.293392Z",
     "shell.execute_reply.started": "2020-10-24T05:51:02.471720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([12847, 277], b'Beginners'), ([15068], b'BBQ'), ([4501], b'Class'), ([3, 12297], b'Taking'), ([3399], b'Place'), ([16], b'in'), ([5964, 7115, 9, 55], b'Missoula!'), ([531], b'Do'), ([25], b'you'), ([241], b'want'), ([12], b'to'), ([129], b'get'), ([394], b'better'), ([44], b'at'), ([492], b'making'), ([3326], b'delicious'), ([15068, 58], b'BBQ?'), ([148], b'You'), ([56], b'will'), ([43], b'have'), ([8], b'the'), ([1004, 6], b'opportunity,'), ([474], b'put'), ([48], b'this'), ([30], b'on'), ([39], b'your'), ([4793], b'calendar'), ([230, 5], b'now.'), ([2721, 6], b'Thursday,'), ([1600], b'September'), ([1630, 727], b'22nd'), ([1715], b'join'), ([1150], b'World'), ([4501], b'Class'), ([15068], b'BBQ'), ([16127, 6], b'Champion,'), ([9137], b'Tony'), ([2659, 5595], b'Balay'), ([45], b'from'), ([301, 782, 3624], b'Lonestar'), ([14627, 15], b'Smoke'), ([12612, 277, 5], b'Rangers.'), ([216], b'He'), ([56], b'will'), ([36], b'be'), ([2119], b'teaching'), ([3, 9], b'a'), ([19529], b'beginner'), ([593], b'level'), ([853], b'class'), ([21], b'for'), ([921], b'everyone'), ([113], b'who'), ([2746], b'wants'), ([12], b'to'), ([129], b'get'), ([394], b'better'), ([28], b'with'), ([70], b'their'), ([17712], b'culinary'), ([1098, 5], b'skills.'), ([216], b'He'), ([56], b'will'), ([3884], b'teach'), ([25], b'you'), ([762], b'everything'), ([25], b'you'), ([174], b'need'), ([12], b'to'), ([214], b'know'), ([12], b'to'), ([5978], b'compete'), ([16], b'in'), ([3, 9], b'a'), ([3, 23405, 4547], b'KCBS'), ([15068], b'BBQ'), ([2259, 6], b'competition,'), ([379], b'including'), ([2097, 6], b'techniques,'), ([5459, 6], b'recipes,'), ([13618, 7, 6], b'timelines,'), ([3604], b'meat'), ([1801], b'selection'), ([11], b'and'), ([27856, 6], b'trimming,'), ([303], b'plus'), ([24190], b'smoker'), ([11], b'and'), ([1472], b'fire'), ([251, 5], b'information.'), ([37], b'The'), ([583], b'cost'), ([12], b'to'), ([36], b'be'), ([16], b'in'), ([8], b'the'), ([853], b'class'), ([19], b'is'), ([25264], b'$35'), ([399], b'per'), ([568, 6], b'person,'), ([11], b'and'), ([21], b'for'), ([21380, 7], b'spectators'), ([34], b'it'), ([19], b'is'), ([339, 5], b'free.'), ([15746, 26], b'Included'), ([16], b'in'), ([8], b'the'), ([583], b'cost'), ([56], b'will'), ([36], b'be'), ([893], b'either'), ([3, 9], b'a'), ([3, 17, 18, 9486], b't-shirt'), ([42], b'or'), ([3, 9, 1409, 29], b'apron'), ([11], b'and'), ([25], b'you'), ([56], b'will'), ([36], b'be'), ([12246], b'tasting'), ([5977], b'samples'), ([13], b'of'), ([284], b'each'), ([3604], b'meat'), ([24], b'that'), ([19], b'is'), ([2657, 5], b'prepared.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the encoding of each word to see how subwords are tokenized\n",
    "tokenized_text = [(tokenize(word).tolist(), word) for word in natural_language_texts[0].split()]\n",
    "print(tokenized_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:10.295601Z",
     "iopub.status.busy": "2020-10-24T05:51:10.295212Z",
     "iopub.status.idle": "2020-10-24T05:51:10.479173Z",
     "shell.execute_reply": "2020-10-24T05:51:10.478611Z",
     "shell.execute_reply.started": "2020-10-24T05:51:10.295576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: [12847   277]\n",
      "detokenized: Beginners\n"
     ]
    }
   ],
   "source": [
    "# We can see that detokenize successfully undoes the tokenization\n",
    "print(f\"tokenized: {tokenize('Beginners')}\\ndetokenized: {detokenize(tokenize('Beginners'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:10.480496Z",
     "iopub.status.busy": "2020-10-24T05:51:10.480311Z",
     "iopub.status.idle": "2020-10-24T05:51:10.557554Z",
     "shell.execute_reply": "2020-10-24T05:51:10.556947Z",
     "shell.execute_reply.started": "2020-10-24T05:51:10.480475Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = trax.data.vocab_size(\n",
    "    vocab_type='sentencepiece',\n",
    "    vocab_file='sentencepiece.model',\n",
    "    vocab_dir='./data/'\n",
    ")\n",
    "\n",
    "def get_sentinels(vocab_size=vocab_size, display=False):\n",
    "    sentinels = {}\n",
    "    \n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = detokenize([vocab_size - i])\n",
    "        \n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'\n",
    "        \n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "\n",
    "    return sentinels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:10.558780Z",
     "iopub.status.busy": "2020-10-24T05:51:10.558463Z",
     "iopub.status.idle": "2020-10-24T05:51:13.650086Z",
     "shell.execute_reply": "2020-10-24T05:51:13.649417Z",
     "shell.execute_reply.started": "2020-10-24T05:51:10.558758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentinel is <Z> and the decoded token is: Internațional\n",
      "The sentinel is <Y> and the decoded token is: erwachsene\n",
      "The sentinel is <X> and the decoded token is: Cushion\n",
      "The sentinel is <W> and the decoded token is: imunitar\n",
      "The sentinel is <V> and the decoded token is: Intellectual\n",
      "The sentinel is <U> and the decoded token is: traditi\n",
      "The sentinel is <T> and the decoded token is: disguise\n",
      "The sentinel is <S> and the decoded token is: exerce\n",
      "The sentinel is <R> and the decoded token is: nourishe\n",
      "The sentinel is <Q> and the decoded token is: predominant\n",
      "The sentinel is <P> and the decoded token is: amitié\n",
      "The sentinel is <O> and the decoded token is: erkennt\n",
      "The sentinel is <N> and the decoded token is: dimension\n",
      "The sentinel is <M> and the decoded token is: inférieur\n",
      "The sentinel is <L> and the decoded token is: refugi\n",
      "The sentinel is <K> and the decoded token is: cheddar\n",
      "The sentinel is <J> and the decoded token is: unterlieg\n",
      "The sentinel is <I> and the decoded token is: garanteaz\n",
      "The sentinel is <H> and the decoded token is: făcute\n",
      "The sentinel is <G> and the decoded token is: réglage\n",
      "The sentinel is <F> and the decoded token is: pedepse\n",
      "The sentinel is <E> and the decoded token is: Germain\n",
      "The sentinel is <D> and the decoded token is: distinctly\n",
      "The sentinel is <C> and the decoded token is: Schraub\n",
      "The sentinel is <B> and the decoded token is: emanat\n",
      "The sentinel is <A> and the decoded token is: trimestre\n",
      "The sentinel is <z> and the decoded token is: disrespect\n",
      "The sentinel is <y> and the decoded token is: Erasmus\n",
      "The sentinel is <x> and the decoded token is: Australia\n",
      "The sentinel is <w> and the decoded token is: permeabil\n",
      "The sentinel is <v> and the decoded token is: deseori\n",
      "The sentinel is <u> and the decoded token is: manipulated\n",
      "The sentinel is <t> and the decoded token is: suggér\n",
      "The sentinel is <s> and the decoded token is: corespund\n",
      "The sentinel is <r> and the decoded token is: nitro\n",
      "The sentinel is <q> and the decoded token is: oyons\n",
      "The sentinel is <p> and the decoded token is: Account\n",
      "The sentinel is <o> and the decoded token is: échéan\n",
      "The sentinel is <n> and the decoded token is: laundering\n",
      "The sentinel is <m> and the decoded token is: genealogy\n",
      "The sentinel is <l> and the decoded token is: QuickBooks\n",
      "The sentinel is <k> and the decoded token is: constituted\n",
      "The sentinel is <j> and the decoded token is: Fertigung\n",
      "The sentinel is <i> and the decoded token is: goutte\n",
      "The sentinel is <h> and the decoded token is: regulă\n",
      "The sentinel is <g> and the decoded token is: overwhelmingly\n",
      "The sentinel is <f> and the decoded token is: émerg\n",
      "The sentinel is <e> and the decoded token is: broyeur\n",
      "The sentinel is <d> and the decoded token is: povești\n",
      "The sentinel is <c> and the decoded token is: emulator\n",
      "The sentinel is <b> and the decoded token is: halloween\n",
      "The sentinel is <a> and the decoded token is: combustibil\n"
     ]
    }
   ],
   "source": [
    "sentinels = get_sentinels(vocab_size, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:13.651393Z",
     "iopub.status.busy": "2020-10-24T05:51:13.651201Z",
     "iopub.status.idle": "2020-10-24T05:51:13.655927Z",
     "shell.execute_reply": "2020-10-24T05:51:13.654995Z",
     "shell.execute_reply.started": "2020-10-24T05:51:13.651372Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_decode(encoded_str_list, sentinels=sentinels):\n",
    "    # If already a string just do the replacement\n",
    "    if(isinstance(encoded_str_list, (str, bytes))):\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = encoded_str_list.replace(token, char)\n",
    "            \n",
    "        return encoded_str_list\n",
    "    \n",
    "    # We need to decode and then prettyfy it\n",
    "    return pretty_decode(detokenize(encoded_str_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T05:51:13.657837Z",
     "iopub.status.busy": "2020-10-24T05:51:13.657467Z",
     "iopub.status.idle": "2020-10-24T05:51:13.666320Z",
     "shell.execute_reply": "2020-10-24T05:51:13.665255Z",
     "shell.execute_reply.started": "2020-10-24T05:51:13.657794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to dress up as an <V> this <b>.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_decode(\"I want to dress up as an Intellectual this halloween.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Masking\n",
    "The function will allow you to tokenize and mask input words with a noise probability. Mask 15% of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T09:13:48.348614Z",
     "iopub.status.busy": "2020-10-24T09:13:48.348405Z",
     "iopub.status.idle": "2020-10-24T09:13:48.354961Z",
     "shell.execute_reply": "2020-10-24T09:13:48.354342Z",
     "shell.execute_reply.started": "2020-10-24T09:13:48.348592Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_mask(\n",
    "    text, \n",
    "    vocab_size=vocab_size, \n",
    "    noise=0.15, \n",
    "    randomizer=np.random.uniform, \n",
    "    tokenize=tokenize\n",
    "):\n",
    "    \"\"\"\n",
    "    Tokenizes and masks a given input\n",
    "    \n",
    "    Args:\n",
    "    text: Text input\n",
    "    vocab_size: Size of the vocabulary. Defaults to vocab_size\n",
    "    noise(float, optional): Probability of masking a token. Defaults to 0.15\n",
    "    randomizer(function) : Function athat genererates random values \n",
    "    tokenize: Tokenizer function defaults to tokenize\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    tuple: Tuple of lists of integers associated to inputs and targets\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Current sentinel number \n",
    "    cur_sentinel_num = 0\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    prev_no_mask = True\n",
    "    \n",
    "    for token in tokenize(text):\n",
    "        # Check if the noise is greater than a random value \n",
    "        if(randomizer() < noise):\n",
    "            # Check to see if the previous token was not masked\n",
    "            if(prev_no_mask == True):\n",
    "                # number of masked tokens increases by 1\n",
    "                cur_sentinel_num += 1\n",
    "                # Compute end_id by subtracting current sentinel value out of the total vocabulary size\n",
    "                end_id = vocab_size - cur_sentinel_num\n",
    "                targets.append(end_id)\n",
    "                inputs.append(end_id)\n",
    "                \n",
    "            targets.append(token)\n",
    "            prev_no_mask = False\n",
    "        else:\n",
    "            inputs.append(token)\n",
    "            prev_no_mask = True\n",
    "            \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T09:13:48.783736Z",
     "iopub.status.busy": "2020-10-24T09:13:48.783483Z",
     "iopub.status.idle": "2020-10-24T09:13:48.868893Z",
     "shell.execute_reply": "2020-10-24T09:13:48.867949Z",
     "shell.execute_reply.started": "2020-10-24T09:13:48.783701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input string:\n",
      "\n",
      "b'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'\n",
      "\n",
      "tokenized inputs:\n",
      "\n",
      "[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5, 216, 31993, 2119, 3, 9, 19529, 593, 853, 21, 921, 31992, 12, 129, 394, 28, 70, 17712, 1098, 5, 31991, 3884, 25, 762, 25, 174, 12, 214, 12, 31990, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 31989, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 31988, 6, 303, 24190, 11, 1472, 251, 5, 37, 31987, 36, 16, 8, 853, 19, 25264, 399, 568, 31986, 21, 21380, 7, 34, 19, 339, 5, 15746, 31985, 8, 583, 56, 36, 893, 3, 9, 3, 31984, 9486, 42, 3, 9, 1409, 29, 11, 25, 31983, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 31982]\n",
      "\n",
      "targets:\n",
      "\n",
      "[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 31993, 56, 36, 31992, 113, 2746, 31991, 216, 56, 31990, 5978, 16, 31989, 379, 2097, 31988, 11, 27856, 31987, 583, 12, 31986, 6, 11, 31985, 26, 16, 31984, 17, 18, 31983, 56, 36, 31982, 5]\n"
     ]
    }
   ],
   "source": [
    "# Some logic to mock a np.random value generator\n",
    "# Needs to be in the same cell for it to aways generate same output\n",
    "\n",
    "def testing_rnd():\n",
    "    def dummy_generator():\n",
    "        vals = np.linspace(0, 1, 10)\n",
    "        cyclic_vals = itertools.cycle(vals)\n",
    "        for _ in range(100):\n",
    "            yield next(cyclic_vals)\n",
    "            \n",
    "    dumr = itertools.cycle(dummy_generator())\n",
    "    \n",
    "    def dummy_randomizer():\n",
    "        return next(dumr)\n",
    "    \n",
    "    return dummy_randomizer\n",
    "\n",
    "input_str = natural_language_texts[0]\n",
    "print(f\"input string:\\n\\n{input_str}\\n\")\n",
    "inputs, targets = tokenize_and_mask(input_str, randomizer=testing_rnd())\n",
    "print(f\"tokenized inputs:\\n\\n{inputs}\\n\")\n",
    "print(f\"targets:\\n\\n{targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T09:14:19.825314Z",
     "iopub.status.busy": "2020-10-24T09:14:19.825062Z",
     "iopub.status.idle": "2020-10-24T09:14:19.978844Z",
     "shell.execute_reply": "2020-10-24T09:14:19.978195Z",
     "shell.execute_reply.started": "2020-10-24T09:14:19.825288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      " <Z> BBQ Class Taking Place in Missoul <Y> Do you want to get better at making <X>? You will have the opportunity, put <W> your calendar now. Thursday, September 22 <V> World Class BBQ Champion, Tony Balay <U>onestar Smoke Rangers. He <T> teaching a beginner level class for everyone<S> to get better with their culinary skills.<R> teach you everything you need to know to <Q> a KCBS BBQ competition,<P>, recipes, timelines, meat selection <O>, plus smoker and fire information. The<N> be in the class is $35 per person <M> for spectators it is free. Include <L> the cost will be either a  <K>shirt or apron and you <J> tasting samples of each meat that is prepared <I>\n",
      "\n",
      "Targets: \n",
      "\n",
      " <Z> Beginners <Y>a! <X> delicious BBQ <W> this on <V>nd join <U> from L <T> will be<S> who wants<R> He will <Q> compete in<P> including techniques <O> and trimming<N> cost to <M>, and <L>d in <K>t- <J> will be <I>.\n"
     ]
    }
   ],
   "source": [
    "print('Inputs: \\n\\n', pretty_decode(inputs))\n",
    "print('\\nTargets: \\n\\n', pretty_decode(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T09:18:53.675182Z",
     "iopub.status.busy": "2020-10-24T09:18:53.674961Z",
     "iopub.status.idle": "2020-10-24T09:18:54.040049Z",
     "shell.execute_reply": "2020-10-24T09:18:54.038795Z",
     "shell.execute_reply.started": "2020-10-24T09:18:53.675159Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs_targets_pairs = [tokenize_and_mask(text) for text in natural_language_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T14:27:13.175330Z",
     "iopub.status.busy": "2020-10-24T14:27:13.175082Z",
     "iopub.status.idle": "2020-10-24T14:27:13.179928Z",
     "shell.execute_reply": "2020-10-24T14:27:13.179212Z",
     "shell.execute_reply.started": "2020-10-24T14:27:13.175304Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_input_target_pairs(input_targets_pairs):\n",
    "    for i, inp_tgt_pair in enumerate(inputs_targets_pairs):\n",
    "        inps, tgts = inp_tgt_pair\n",
    "        inps, tgts = pretty_decode(inps), pretty_decode(tgts)\n",
    "        print(f'[{i}]\\n\\n'\n",
    "            f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\n",
    "            f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T14:27:30.948581Z",
     "iopub.status.busy": "2020-10-24T14:27:30.948374Z",
     "iopub.status.idle": "2020-10-24T14:27:31.638630Z",
     "shell.execute_reply": "2020-10-24T14:27:31.638014Z",
     "shell.execute_reply.started": "2020-10-24T14:27:30.948560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "\n",
      "inputs:\n",
      "Beginners BBQ Class Taking <Z> in Missoul <Y>! Do you want to get\n",
      "better at making delicious <X>? You will have the opportunity, <W>\n",
      "this on <V> calendar now. Thursday <U> September 22 <T> join<S> Class\n",
      "BBQ Champion, Tony Balay from Lonestar Smoke<R>ers <Q> He will be\n",
      "teaching a beginner<P> class <O> everyone who wants<N> get better with\n",
      "their <M> skills <L> He will teach <K> everything you need to know to\n",
      "<J> in a KCBS BBQ <I> techniques, recipes, timelines, meat<H> and\n",
      "trimming, plus smoker and fire information. The cost to be<G> the\n",
      "class is $35 <F> person, and<E> spectators it is free. Included in the\n",
      "cost will<D> either <C> t- <B> or apron and you will be tasting\n",
      "samples <A> each meat that <z> prepared.\n",
      "\n",
      "targets:\n",
      "<Z> Place <Y>a <X> BBQ <W> put <V> your <U>, <T>nd<S> World<R> Rang\n",
      "<Q>.<P> level <O> for<N> to <M> culinary <L>. <K> you <J> compete <I>\n",
      "competition, including<H> selection<G> in <F> per<E> for<D> be<C>a\n",
      "<B>shirt <A> of <z> is\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1]\n",
      "\n",
      "inputs:\n",
      "<Z> in 'Mac OS X <Y> (10 <X>7)' started by axb <W>i87, Jan 20, 2012.\n",
      "I've got <V>a 500g <U> drive <T> a 240gb SSD. When trying to restore\n",
      "using<S> utility i'm given the error \"Not enough space on disk<R>____\n",
      "to restore <Q> But I shouldn't have to do that!!! Any ideas or\n",
      "work<P>s before <O>ing to the above? Use Carbon Copy Cloner to copy\n",
      "one drive to the other. I'<N> done <M> several times going from <L>D\n",
      "to <K> SSD and I wound <J> a bootable SSD drive. One step you <I>\n",
      "remember not to skip is to use Disk Utility to partition the SSD as\n",
      "GUID partition scheme<H> doing the <G>ne. If it came Apple <F>ition\n",
      "Scheme, even if<E> let<D>CC do the clone, the resulting drive<C> boot\n",
      "<B>. C <A> usually works <z> \"file mode\" and it can easily copy a\n",
      "larger drive (that's mostly empty <y> onto a smaller drive.<x> you<w>\n",
      "CCC to clone a drive you did<v> boot<u>, it can work <t> copy mode <s>\n",
      "destination<r> must be<q> size or larger than the drive you\n",
      "are<p>cloning from <o>if <n> recall <m>ve actually done this somehow\n",
      "on Disk Utility <l> times<k>booting from <j>a different drive (or even\n",
      "the dvd)<i> not running disk utility from the drive your clo<h>ing)\n",
      "and had it work just fine from larger to smaller bootable clo<g>.\n",
      "Definitely format the drive cloning to first <f> as bootable Apple\n",
      "etc.. Thanks for <e> this out. My only experience <d> DU to go larger\n",
      "to smaller was when <c> trying to make  <b> install stick and I was\n",
      "unable to restore InstallESD <a>dmg to a 4 GB Théâtre ofKeep the\n",
      "reason that wouldn't fit isdürftig was slightly moreutti GB of data.\n",
      "\n",
      "targets:\n",
      "<Z> Discussion <Y> Lion <X>. <W>o <V>  <U>b internal <T> and<S>\n",
      "disk<R>  <Q>\"<P>around <O> resort<N>ve <M> this <L> larger HD <K>\n",
      "smaller <J> up with <I> have to<H> HFS+ before<G>clo <F> Part<E>\n",
      "you<D> C<C> won't be <B>able <A>CC <z> in <y>)<x> If<w> tell<v> NOT<u>\n",
      "from <t> in block <s> where the<r> drive<q> the same<p>  <o> ( <n> I\n",
      "<m>). I' <l> several<k> ( <j> <i> so<h>n<g>ne <f>,<e>pointing <d>\n",
      "using <c> I was <b>a Lion <a>. Théâtre USB stick butKeep coursedürftig\n",
      "thereutti than 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2]\n",
      "\n",
      "inputs:\n",
      "<Z>il plaid <Y>lycra <X> spandex shortall with metallic slinky\n",
      "<W>sets. Attache <V> metallic elastic belt with O <U>ring. Head <T>\n",
      "included. Great hip hop<S> jazz dance costume.<R> in the USA.\n",
      "\n",
      "targets:\n",
      "<Z> Fo <Y>  <X> and <W> in <V>d <U>- <T>band<S> or<R> Made\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[3]\n",
      "\n",
      "inputs:\n",
      "How many backlink <Z> per day for new site? Discussion <Y> 'Black <X>\n",
      "SEO' started by Omoplata, Dec 3, 2010. 1) for a <W> created site,\n",
      "what's <V> max <U>links per day I should do to be safe? 2) how <T> do\n",
      "I have<S> let my site<R> before I can start making more blinks? I did\n",
      "about 6000 forum profiles every 24 hours for 10 days for <Q> of my\n",
      "sites<P> had a brand new domain. There is <O> backlinks for every<N>\n",
      "these <M> profile so <L>s 18 000 backlinks every 24 hours and nothing\n",
      "happened in terms of being penalized <K> sandboxed. This is now maybe\n",
      "3 months ago <J> the site <I> ranking on first page for<H>a lot<G> my\n",
      "targeted keywords. build more you can in starting <F> do manual\n",
      "submission and not spammy<E> means manual +<D> to<C> post.. <B> after\n",
      "1 month you can <A> a <z> blast.. Wow, dude, you built 18k backlink\n",
      "<y> a day<x> a brand<w>? How quickly did<v> rank up? What kind of\n",
      "competition/search<u> did <t> keywords have?\n",
      "\n",
      "targets:\n",
      "<Z>s <Y> in <X> Hat <W> newly <V> the <U> # back <T> long<S> to<R> age\n",
      "<Q> one<P> which <O> three<N> of <M> forum <L> that <K> or <J> and <I>\n",
      "is<H> <G> of <F> but<E> type<D> relevant<C> the <B> then <A> make <z>\n",
      "big <y>s<x> on<w> new site<v> you<u>es <t> those\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[4]\n",
      "\n",
      "inputs:\n",
      "The Denver Board of Education opened the 2017-18 school year with an\n",
      "update <Z> projects that include new construction <Y> upgrades, heat\n",
      "mitigation <X> quality learning environments. We <W> excited <V>\n",
      "Denver students will be the beneficiaries <U>a four year, $572 million\n",
      "General Oblig <T> Bond.<S> the passage of the bond, our construction\n",
      "team has worked to schedule<R> projects over <Q> four-year term<P>\n",
      "bond. Denver voters on Tuesday approved bond and mill funding <O>\n",
      "for<N> in Denver Public Schools, agreeing to invest $572 million in\n",
      "bond funding <M> build and improve schools and <L>6.6 million in\n",
      "operating dollars to support proven initiatives, <K> as early <J>\n",
      "Denver voters say <I> to bond and mill levy funding<H> for<G>PS\n",
      "students and schools. Click to learn more about the details of the\n",
      "voter-approved <F> measure. Denver voters<E>. 8 approved bond and mill\n",
      "funding<D> for DPS students and schools. Learn more about what’s\n",
      "included in the mill <C>y measure.\n",
      "\n",
      "targets:\n",
      "<Z> on <Y>, <X> and <W> are <V> that <U> of  <T>ation<S> Since<R> the\n",
      "<Q> the<P> of the <O> measures<N> students <M> to <L> $5 <K> such <J>\n",
      "literacy. <I> yes<H> support<G> D <F> bond<E> on Nov<D> measures<C>lev\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_input_target_pairs(inputs_targets_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T15:59:24.325857Z",
     "iopub.status.busy": "2020-10-24T15:59:24.325592Z",
     "iopub.status.idle": "2020-10-24T15:59:24.357944Z",
     "shell.execute_reply": "2020-10-24T15:59:24.356554Z",
     "shell.execute_reply.started": "2020-10-24T15:59:24.325830Z"
    }
   },
   "source": [
    "## Transformer\n",
    "- Load a transformer model checkpoint that has been pre-trained using the C4 dataset and decode from it\n",
    "- This will save lot of time compare to training the model from scratch  \n",
    "![alt-txt](images/fulltransformer.png)\n",
    "- Load the model\n",
    "- Copy the checkpoint to local dir for speed\n",
    "- Implement encoder, refer below  \n",
    "![alt-txt](images/encoder.png)\n",
    "### Transformer Encoder\n",
    "The `FeedForwardBlock` \n",
    "- `tl.LayerNorm()\n",
    "- `tl.Dense(d_ff)\n",
    "- `activation`\n",
    "- `dropout_middle`\n",
    "- `tl.Dense(d_model)`\n",
    "- `dropout_final`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T16:53:12.071142Z",
     "iopub.status.busy": "2020-10-24T16:53:12.070906Z",
     "iopub.status.idle": "2020-10-24T16:53:12.076380Z",
     "shell.execute_reply": "2020-10-24T16:53:12.075571Z",
     "shell.execute_reply.started": "2020-10-24T16:53:12.071119Z"
    }
   },
   "outputs": [],
   "source": [
    "def FeedForwardBlock(\n",
    "    d_model,\n",
    "    d_ff,\n",
    "    dropout,\n",
    "    dropout_shared_axes,\n",
    "    mode,\n",
    "    activation\n",
    "):\n",
    "    dropout_middle = tl.Dropout(\n",
    "        rate=dropout,\n",
    "        shared_axes=dropout_shared_axes,\n",
    "        mode=mode\n",
    "    )\n",
    "    dropout_final = tl.Dropout(\n",
    "        rate=dropout,\n",
    "        shared_axes=dropout_shared_axes,\n",
    "        mode=mode\n",
    "    )\n",
    "    \n",
    "    ff_block = [\n",
    "        tl.LayerNorm(),\n",
    "        tl.Dense(d_ff),\n",
    "        activation(),\n",
    "        dropout_middle,\n",
    "        tl.Dense(d_model),\n",
    "        dropout_final\n",
    "    ]\n",
    "    \n",
    "    return ff_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T16:53:12.405987Z",
     "iopub.status.busy": "2020-10-24T16:53:12.405749Z",
     "iopub.status.idle": "2020-10-24T16:53:12.411125Z",
     "shell.execute_reply": "2020-10-24T16:53:12.410213Z",
     "shell.execute_reply.started": "2020-10-24T16:53:12.405966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]\n"
     ]
    }
   ],
   "source": [
    "feed_forward_example = FeedForwardBlock(\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    dropout=0.8,\n",
    "    dropout_shared_axes=0,\n",
    "    mode='train',\n",
    "    activation=tl.Relu\n",
    ")\n",
    "print(feed_forward_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 The Encoder Block\n",
    "\n",
    "The encoder block will use the `FeedForwardBlock`. \n",
    "\n",
    "You will have to build two residual connections. Inside the first residual connection you will have the `tl.layerNorm()`, `attention`, and `dropout_` layers. The second residual connection will have the `feed_forward`.  \n",
    "\n",
    "You will also need to implement `feed_forward`, `attention` and `dropout_` blocks. \n",
    "\n",
    "So far you haven't seen the [`tl.Attention()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.Attention) and [`tl.Residual()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) layers so you can check the docs by clicking on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T18:04:56.931839Z",
     "iopub.status.busy": "2020-10-24T18:04:56.931599Z",
     "iopub.status.idle": "2020-10-24T18:04:56.938458Z",
     "shell.execute_reply": "2020-10-24T18:04:56.937651Z",
     "shell.execute_reply.started": "2020-10-24T18:04:56.931810Z"
    }
   },
   "outputs": [],
   "source": [
    "def EncoderBlock(\n",
    "    d_model,\n",
    "    d_ff,\n",
    "    n_heads,\n",
    "    dropout,\n",
    "    dropout_shared_axes,\n",
    "    mode,\n",
    "    ff_activation,\n",
    "    FeedForwardBlock=FeedForwardBlock\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of layers that implements a Transformer encoder block.\n",
    "    The input to the layer is a pair, (activations, mask), where the mask was created\n",
    "    from the original source tokens to prevent attending to the padding part of the input\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        dropout_shared_axes (int): axes on which to share dropout mask.\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "        FeedForwardBlock (function): A function that returns the feed forward block.\n",
    "    Returns:\n",
    "        list: A list of layers that maps (activations, mask) to (activations, mask).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    attention = tl.Attention(\n",
    "        d_feature=d_model,\n",
    "        n_heads=n_heads,\n",
    "        dropout=dropout,\n",
    "        mode=mode\n",
    "    )\n",
    "    feed_forward = FeedForwardBlock(\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        dropout,\n",
    "        dropout_shared_axes,\n",
    "        mode,\n",
    "        ff_activation\n",
    "    )\n",
    "    \n",
    "    # Dropout block\n",
    "    dropout_ = tl.Dropout( \n",
    "        # set it equal to `dropout`\n",
    "        rate=dropout,\n",
    "        # set it equal to the axes on which to share dropout mask\n",
    "        shared_axes=dropout_shared_axes,\n",
    "        # set it equal to `mode`\n",
    "        mode=mode\n",
    "    )\n",
    "    \n",
    "    encoder_block = [\n",
    "        tl.Residual(\n",
    "            tl.LayerNorm(),\n",
    "            attention,\n",
    "            dropout_\n",
    "        ),\n",
    "        tl.Residual(\n",
    "            feed_forward\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return encoder_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T18:04:57.305838Z",
     "iopub.status.busy": "2020-10-24T18:04:57.305582Z",
     "iopub.status.idle": "2020-10-24T18:04:57.311407Z",
     "shell.execute_reply": "2020-10-24T18:04:57.310664Z",
     "shell.execute_reply.started": "2020-10-24T18:04:57.305811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Serial_in2_out2[\n",
      "  Branch_in2_out3[\n",
      "    None\n",
      "    Serial_in2_out2[\n",
      "      LayerNorm\n",
      "      Serial_in2_out2[\n",
      "        Dup_out2\n",
      "        Dup_out2\n",
      "        Serial_in4_out2[\n",
      "          Parallel_in3_out3[\n",
      "            Dense_512\n",
      "            Dense_512\n",
      "            Dense_512\n",
      "          ]\n",
      "          PureAttention_in4_out2\n",
      "          Dense_512\n",
      "        ]\n",
      "      ]\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "], Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Dense_2048\n",
      "      Relu\n",
      "      Dropout\n",
      "      Dense_512\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "encoder_example = EncoderBlock(d_model=512, d_ff=2048, n_heads=6, dropout=0.8, dropout_shared_axes=0, mode = 'train', ff_activation=tl.Relu)\n",
    "print(encoder_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Encoder\n",
    "- Implement the Encoder Block\n",
    "- BERT, Bidirectional Encoder Representation from Transformers \n",
    "- Positional Encoders -> n_layers -> \n",
    "\n",
    "\n",
    "- [`tl.Branch`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch): helps with the branching and has the following sublayers:\n",
    "    - `positional_encoder`.\n",
    "    - [`tl.PaddingMask()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PaddingMask): layer that maps integer sequences to padding masks.\n",
    "- Your list of `EncoderBlock`s\n",
    "- [`tl.Select([0], n_in=2)`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select):  Copies, reorders, or deletes stack elements according to indices.\n",
    "- [`tl.LayerNorm()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm).\n",
    "- [`tl.Mean()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean): Mean along the first axis.\n",
    "- `tl.Dense()` with n_units set to n_classes. \n",
    "- `tl.LogSoftmax()`   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T18:24:20.532289Z",
     "iopub.status.busy": "2020-10-24T18:24:20.532038Z",
     "iopub.status.idle": "2020-10-24T18:24:20.541303Z",
     "shell.execute_reply": "2020-10-24T18:24:20.540524Z",
     "shell.execute_reply.started": "2020-10-24T18:24:20.532262Z"
    }
   },
   "outputs": [],
   "source": [
    "def TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    n_classes=10,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    dropout_shared_axes=None,\n",
    "    max_len=2048,\n",
    "    mode='train',\n",
    "    ff_activation=tl.Relu,\n",
    "    EncoderBlock=EncoderBlock\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a Transformer encoder model.\n",
    "    The input to the model is a tensor of tokens.\n",
    "  \n",
    "    Args:\n",
    "        vocab_size (int): vocab size. Defaults to vocab_size.\n",
    "        n_classes (int): how many classes on output. Defaults to 10.\n",
    "        d_model (int): depth of embedding. Defaults to 512.\n",
    "        d_ff (int): depth of feed-forward layer. Defaults to 2048.\n",
    "        n_layers (int): number of encoder/decoder layers. Defaults to 6.\n",
    "        n_heads (int): number of attention heads. Defaults to 8.\n",
    "        dropout (float): dropout rate (how much to drop out). Defaults to 0.1.\n",
    "        dropout_shared_axes (int): axes on which to share dropout mask. Defaults to None.\n",
    "        max_len (int): maximum symbol length for positional encoding. Defaults to 2048.\n",
    "        mode (str): 'train' or 'eval'. Defaults to 'train'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer. Defaults to tl.Relu.\n",
    "        EncoderBlock (function): Returns the encoder block. Defaults to EncoderBlock.\n",
    "  \n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer model as a layer that maps\n",
    "        from a tensor of tokens to activations over a set of output classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    positional_encoder = [\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode),\n",
    "        tl.PositionalEncoding(max_len=max_len)\n",
    "    ]\n",
    "    \n",
    "    encoder_blocks = [\n",
    "        EncoderBlock(\n",
    "            d_model=d_model, \n",
    "            d_ff=d_ff, \n",
    "            n_heads=n_heads, \n",
    "            dropout=dropout, \n",
    "            dropout_shared_axes=dropout_shared_axes, \n",
    "            mode = mode, \n",
    "            ff_activation=ff_activation\n",
    "        ) for _ in range(n_layers)\n",
    "    ]\n",
    "    \n",
    "    return tl.Serial(\n",
    "    \n",
    "        tl.Branch(positional_encoder, tl.PaddingMask()),\n",
    "        encoder_blocks,\n",
    "        tl.Select([0], n_in=2),\n",
    "        tl.LayerNorm(),\n",
    "        tl.Mean(axis=1),\n",
    "        tl.Dense(n_classes),\n",
    "        tl.LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T18:24:21.110653Z",
     "iopub.status.busy": "2020-10-24T18:24:21.110413Z",
     "iopub.status.idle": "2020-10-24T18:24:21.116705Z",
     "shell.execute_reply": "2020-10-24T18:24:21.115988Z",
     "shell.execute_reply.started": "2020-10-24T18:24:21.110628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Branch_out2[\n",
       "    [Embedding_32000_512, Dropout, PositionalEncoding]\n",
       "    PaddingMask(0)\n",
       "  ]\n",
       "  Serial_in2_out2[\n",
       "    Branch_in2_out3[\n",
       "      None\n",
       "      Serial_in2_out2[\n",
       "        LayerNorm\n",
       "        Serial_in2_out2[\n",
       "          Dup_out2\n",
       "          Dup_out2\n",
       "          Serial_in4_out2[\n",
       "            Parallel_in3_out3[\n",
       "              Dense_512\n",
       "              Dense_512\n",
       "              Dense_512\n",
       "            ]\n",
       "            PureAttention_in4_out2\n",
       "            Dense_512\n",
       "          ]\n",
       "        ]\n",
       "        Dropout\n",
       "      ]\n",
       "    ]\n",
       "    Add_in2\n",
       "  ]\n",
       "  Serial[\n",
       "    Branch_out2[\n",
       "      None\n",
       "      Serial[\n",
       "        LayerNorm\n",
       "        Dense_2048\n",
       "        Relu\n",
       "        Dropout\n",
       "        Dense_512\n",
       "        Dropout\n",
       "      ]\n",
       "    ]\n",
       "    Add_in2\n",
       "  ]\n",
       "  Select[0]_in2\n",
       "  LayerNorm\n",
       "  Mean\n",
       "  Dense_10\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerEncoder(n_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
