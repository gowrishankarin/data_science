{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T07:52:41.727669Z",
     "iopub.status.busy": "2020-10-11T07:52:41.727416Z",
     "iopub.status.idle": "2020-10-11T07:52:41.733467Z",
     "shell.execute_reply": "2020-10-11T07:52:41.732336Z",
     "shell.execute_reply.started": "2020-10-11T07:52:41.727638Z"
    }
   },
   "source": [
    "# Transformer Summarizer\n",
    "\n",
    "- Explore summarization using the transformer model\n",
    "- Implement transformer decoder from scratch\n",
    "![alt_text](images/transformerNews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [Introduction](#0)\n",
    "- [Part 1: Importing the dataset](#1)\n",
    "    - [1.1 Encode & Decode helper functions](#1.1)\n",
    "    - [1.2 Defining parameters](#1.2)\n",
    "    - [1.3 Exploring the data](#1.3)\n",
    "- [Part 2: Summarization with transformer](#2)\n",
    "    - [2.1 Dot product attention](#2.1)\n",
    "        - [Exercise 01](#ex01)\n",
    "    - [2.2 Causal Attention](#2.2)\n",
    "        - [Exercise 02](#ex02)\n",
    "    - [2.3 Transformer decoder block](#2.3)\n",
    "        - [Exercise 03](#ex03)\n",
    "    - [2.4 Transformer Language model](#2.4)\n",
    "        - [Exercise 04](#ex04)\n",
    "- [Part 3: Training](#3)\n",
    "    - [3.1 Training the model](#3.1)\n",
    "        - [Exercise 05](#ex05)\n",
    "- [Part 4: Evaluation](#4)\n",
    "    - [4.1 Loading in a trained model](#4.1)\n",
    "- [Part 5: Testing with your own input](#5) \n",
    "    - [Exercise 6](#ex06)\n",
    "    - [5.1 Greedy decoding](#5.1)\n",
    "        - [Exercise 07](#ex07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T07:56:25.332767Z",
     "iopub.status.busy": "2020-10-11T07:56:25.332522Z",
     "iopub.status.idle": "2020-10-11T07:56:25.338949Z",
     "shell.execute_reply": "2020-10-11T07:56:25.337762Z",
     "shell.execute_reply.started": "2020-10-11T07:56:25.332742Z"
    }
   },
   "source": [
    "<a name='0'></a>\n",
    "### Introduction\n",
    "- Summarization is an important task in NLP and could be useful for a consumer enterprise\n",
    "- Bots can be used to scrape articles, summarize them and then you can use sentiment analysis to identify the sentiment about certain stock\n",
    "- Summarize long emails or articles \n",
    "\n",
    "1. Use built in functions to preprocess data\n",
    "2. Implement DotProductAttention\n",
    "3. Implement Causal Attention\n",
    "4. Understand how attention works\n",
    "5. Build the transformer model\n",
    "6. Evaluate the model\n",
    "7. Summarize the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:06.890225Z",
     "iopub.status.busy": "2020-10-18T15:27:06.889725Z",
     "iopub.status.idle": "2020-10-18T15:27:15.172767Z",
     "shell.execute_reply": "2020-10-18T15:27:15.172020Z",
     "shell.execute_reply.started": "2020-10-18T15:27:06.890179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# To print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.174570Z",
     "iopub.status.busy": "2020-10-18T15:27:15.174227Z",
     "iopub.status.idle": "2020-10-18T15:27:15.178165Z",
     "shell.execute_reply": "2020-10-18T15:27:15.177180Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.174547Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "VOCAB_DIR = os.path.join(DATA_DIR, 'vocab_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.180886Z",
     "iopub.status.busy": "2020-10-18T15:27:15.180559Z",
     "iopub.status.idle": "2020-10-18T15:27:15.344905Z",
     "shell.execute_reply": "2020-10-18T15:27:15.343999Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.180848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing CNN/DailyMail articles dataset\n",
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir=DATA_DIR,\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir=DATA_DIR,\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 Tokenize & Detokenize helper functions\n",
    "\n",
    "Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your [Trax](https://github.com/google/trax) models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
    "\n",
    "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
    "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
    "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
    "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
    "\n",
    "Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n",
    "\n",
    "- <span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
    "- <span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.347128Z",
     "iopub.status.busy": "2020-10-18T15:27:15.346903Z",
     "iopub.status.idle": "2020-10-18T15:27:15.352953Z",
     "shell.execute_reply": "2020-10-18T15:27:15.352138Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.347104Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  \n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(\n",
    "        iter([input_str]),\n",
    "        vocab_dir=VOCAB_DIR,\n",
    "        vocab_file='summarize32k.subword.subwords'\n",
    "    ))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "  \n",
    "    s = trax.data.detokenize(\n",
    "        integers,\n",
    "        vocab_dir=VOCAB_DIR,\n",
    "        vocab_file='summarize32k.subword.subwords'\n",
    "    )\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T08:11:15.764524Z",
     "iopub.status.busy": "2020-10-11T08:11:15.764286Z",
     "iopub.status.idle": "2020-10-11T08:11:15.769108Z",
     "shell.execute_reply": "2020-10-11T08:11:15.768046Z",
     "shell.execute_reply.started": "2020-10-11T08:11:15.764499Z"
    }
   },
   "source": [
    "<a name='1.2'></a>\n",
    "## 1.2 Preprocessing for Language Models: Concatenate It!\n",
    "**Transformer Decoder** Language model to solve an input-output problem\n",
    "- Language models predict the next word, they have no notion of input\n",
    "- Concatenate inputs with targets putting a separator in between\n",
    "- Create a mask - [0, 1] 0s at inputs and 1s at targets, so that the model is not penalized for mis-predicting the article and only focus on summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.354813Z",
     "iopub.status.busy": "2020-10-18T15:27:15.354456Z",
     "iopub.status.idle": "2020-10-18T15:27:15.544407Z",
     "shell.execute_reply": "2020-10-18T15:27:15.543579Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.354784Z"
    }
   },
   "outputs": [],
   "source": [
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate ttokenized inputs and targets sing 0 as separator\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1]* (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "        \n",
    "# You can combine a few data preprocessing steps into a pipeline like this\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # Tokenizes\n",
    "    trax.data.Tokenize(vocab_dir=VOCAB_DIR, vocab_file='summarize32k.subword.subwords'),\n",
    "    # Uses function defined above\n",
    "    preprocess,\n",
    "    # Filters out examples longer than 2048\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to data streams\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0 # They are the same in Language Model (LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.545785Z",
     "iopub.status.busy": "2020-10-18T15:27:15.545601Z",
     "iopub.status.idle": "2020-10-18T15:27:15.550320Z",
     "shell.execute_reply": "2020-10-18T15:27:15.549577Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.545764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.551742Z",
     "iopub.status.busy": "2020-10-18T15:27:15.551323Z",
     "iopub.status.idle": "2020-10-18T15:27:15.679406Z",
     "shell.execute_reply": "2020-10-18T15:27:15.678819Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.551715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " Pope Francis has just landed in the Philippines - to a crowd of\n",
      "thousands, church bells and an airport greeting from President Benigno\n",
      "Aquino. The Pope is in Asia, having begun his trip just days ago with\n",
      "a stop in Sri Lanka, for a week-long tour, and has already caused a\n",
      "huge spike in interest when it comes to travel in the region. Flight\n",
      "searches to Sri Lanka have increased by 19 per cent, while the numbers\n",
      "for the Philippines have increased by a whopping 51 per cent week on\n",
      "week from last year. Pope Francis has just arrived in the Philippines\n",
      "- and already travel searches for the region have spiked . The Pope's\n",
      "six-day Asian tour includes stops in Manila, where adoring crowds\n",
      "waited, and Sri Lanka . Interest in Manila has seen a particular\n",
      "increase, up 60 per cent already, according to Cheapflights.co.uk .\n",
      "Cheapflights.co.uk spokesperson Oonah Shiel attributes this 'halo\n",
      "effect' to the Pope's genuine mass appeal. Of particular interest to\n",
      "travellers is Manila, where His Holiness is expected to deliver a huge\n",
      "open air mass on Sunday, January 18, to over 5 million attendees.\n",
      "'Searches regarding the Filipino capital are up 60 per cent and\n",
      "counting,' she said. Week on week from last year, searches for the\n",
      "Philippines have increased by 51%. Pictured: Palawan . When His\n",
      "Holiness arrived at the Manila airport, Filipino children performed in\n",
      "his honour . Following his stop in Sri Lanka, where the Pope canonised\n",
      "its first saint, travel interest has gone up by 19% . 'As well as\n",
      "driving short-term interest in flights to the region, the Pope's visit\n",
      "is expected to raise awareness of the plight of the more marginalised\n",
      "groups in the area and provide a longer-term boost to ethical\n",
      "tourism,' Shiel added. In fact, Pope Francis has already canonised Sri\n",
      "Lanka's first saint, after calling for unity in the conflict-hit\n",
      "nation. 'Every step he makes, every car ride he takes, every moment he\n",
      "stays with us is precious for us,' said Archbishop Socrates Villegas,\n",
      "president of Catholic Bishops' Conference of the Philippines. 'Seeing\n",
      "him pass by is a grace.'<EOS><pad>Flightsearches for the Philippines\n",
      "up over 50% week on week from 2014 . Pope has landed in Manila,\n",
      "following a visit to Sri Lanka on his Asian tour . Interest in Manila,\n",
      "in particular, has seen an even greater spike of 60% .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "\n",
    "## 1.3 Batching with bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.680799Z",
     "iopub.status.busy": "2020-10-18T15:27:15.680592Z",
     "iopub.status.idle": "2020-10-18T15:27:15.685052Z",
     "shell.execute_reply": "2020-10-18T15:27:15.684456Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.680777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bucketing to crete batched generators\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes\n",
    "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 16 sentences of length < 128, 8 of length < 256\n",
    "# 4 of length < 512 and so on\n",
    "\n",
    "boundaries = [128, 256, 512, 1024]\n",
    "batch_sizes = [16, 8, 4, 2, 1]\n",
    "\n",
    "# Create the streams\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries,\n",
    "    batch_sizes\n",
    ")(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries,\n",
    "    batch_sizes\n",
    ")(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.688765Z",
     "iopub.status.busy": "2020-10-18T15:27:15.688428Z",
     "iopub.status.idle": "2020-10-18T15:27:15.699820Z",
     "shell.execute_reply": "2020-10-18T15:27:15.698973Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.688739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1195)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every execution will result in generation of a different article\n",
    "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
    "input_batch, _, mask_batch = next(train_batch_stream)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.702751Z",
     "iopub.status.busy": "2020-10-18T15:27:15.702554Z",
     "iopub.status.idle": "2020-10-18T15:27:15.709897Z",
     "shell.execute_reply": "2020-10-18T15:27:15.709069Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.702730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2713   132  1668    18   973    28 19300   527    28   157 11703   527\n",
      "  4811    28   968 25789   157  1547   450   412    22   346   126    78\n",
      "  3155  1859     3     9 19300  1353  3174   669   391   145    28  2642\n",
      "  2460  2264  1248 13337  7977     6   968 13944 26939  3975 16981     2\n",
      "  1120     2  1779   229 17654    16   102   144  1496   132   213 22208\n",
      "   186 10255     3     9 10064     2  1779  2232   320  4756   213  1610\n",
      "   132  4910  2793  1344     6  4041  1581    78  1967    78  3155  1859\n",
      "     2   229   897   412    28   763  1718     2   286   320  1120    91\n",
      "   292     2  1248    28  2692  1463   186    28 10722   959 13380   314\n",
      "  1782   129  1435  6304   996  1019   669 27884     4   213 10064 27872\n",
      "  3898 25183   318   136    10   197     3  2598   527   213  1668   676\n",
      "   527  1347  8048   793 27439  9275  1628  7350  1652  4172 27439  9275\n",
      "  7583     7   129    40 21090   185    64 13094     2    35    51   790\n",
      "     7    26   211   116  2574   527 24692     4   101     3   129    49\n",
      "     7    26  1447    64   285    22     7     5    19   132   213   201\n",
      "     2    35    44    74   761     2    22     7     5   346   213   201\n",
      "  2002  9175  6051     4   246  1019   846   379  8260 25067     4    11\n",
      " 13337  7977 25789   157  3975 16981     8   346    12  1504   864   547\n",
      "   500   824 19300     8   231    12   527   213   157  1779  1496   134\n",
      "   412    22   346   213   968  2234    78  3155  1859     3    69   127\n",
      "    22    40   369   553   134   171   379 16981     2   213  1859 25789\n",
      "   157  1019 13337  7977     2  1353  1496   102    22 21540    21    71\n",
      "    15  1081   239   128    10 21415   410    78  3155     3   514  3122\n",
      "   157  9652   134   132   213  4165   621   186    41 23793    21   911\n",
      "     3     9 25789   157  2232   320   179    61   809   208  1869   186\n",
      "  2057    64   527   213  4165   621     3    69  9227   213 17242   587\n",
      "   186 10224   320    28   260   527   986  1405     2  1779  9396    60\n",
      "  2511   186   316  6105    32     3 16030     5   809  2220   186  1251\n",
      "  3818  3171   132  4890   127   285 16981  1353 17654    16   186   132\n",
      "  2558  1785   102  5675  1019    28  5256  1496 15811   320   213 16891\n",
      "   279     3   642    15  2642  2460     2    22  4276   320 22646   186\n",
      "   793   105    22    40   369   553   213   157   171     3  2713    18\n",
      "    19   764   233    28 12749     4  1019   213  4811     3 15149    17\n",
      "   246    11  9597   277   667 16981     2  1120     2   144 22639   651\n",
      "   463   102    22  1353  1496    78  3155   379 21346     4    11     9\n",
      "   157   901   320  7508   809 16981   412    22 10224   463   186 18338\n",
      "  2797   246   399  3020   379  8351     4    11     9 25789   157  7712\n",
      "  5256  1496 22897   320   213 22208   186 10255     2   186    28 26026\n",
      "  3817  1172    15   570   379 13337  7977    43   831   285   864    25\n",
      "   809   213   947    78  3112     2 14454  9043     5   527  1523  3156\n",
      "   132   213  4165   621     3 16981     7     5   938     2  2907 13337\n",
      "  7977 13944 26939 25702  4795  3831  4484     2    43  2441    15  4319\n",
      "   884   320  2057   236   186  5180   246   399  6671    15   177  1782\n",
      " 16030     5   952   320  1399   156  7270   110    22     7     5   981\n",
      "  4689   213  1547  5256  1496 22897  3898   131  1113    78    68  3004\n",
      "   843    78  3155   736  1782  2713  1399   156   285   122   103  1793\n",
      "     7    26  1019    15 21065     5   320  5061   328    61   186  2057\n",
      "   236   186  5180   246   986  1405     2   103   143    18    46    28\n",
      "   769   224   488     3    69   229  3272   130  5162  2002  4795  3831\n",
      "  4484   186    54 13337  7977  2125    43 15769    17  5270   157     7\n",
      "     5   117    59    80 16547    75   320    31  3004  2188   132    28\n",
      "  1868   527   291  1019 16981     3 13337  7977  2099   213  1614  1248\n",
      "   213 19545  8500 20234  4405  8949  3324   157    80   320    50   221\n",
      "   843     3 11063   251    11 16981     7     5   938 25702   127   864\n",
      "  1029    22  6671    15   177   691  3156   236   186 18338  2304   246\n",
      "   399   379 21346     4    11 16981   229 12370    21   346  1248    15\n",
      "  2907 13337  7977  7167    78  3155    70   141   926   171    22  1353\n",
      "  1496   379 16981     2  1779  2492    61   132 13499    79     2  1668\n",
      "   186  2850    15 13944  2673  1054  1838  1522  3293   167     2   995\n",
      "  1838   119  8602   186  1241   213   947   132   493   420     3    69\n",
      "   669   391  1042  4795  3831  4484     2    28  2907 13944 26939   186\n",
      " 13337  7977  3113     2   220   104     3     9   947     7     5   782\n",
      "  1108     2  3306  6318   309     2   973    28  1602  2685   213  4811\n",
      "    78  3155  1782  2353   824  1859 13337  7977     6 12089  1652   625\n",
      "   213 17682   782   285    36   527    89   221     2  1668  2157 21679\n",
      "  6475 18981  3975 16981     2    40    46  1496   132   979   527    89\n",
      "   474  3898    22   127    78 13337  7977  1782    69   229   950   809\n",
      "  5824  8373  2220   186  1251  3171   132  4890   102 25491  5675     3\n",
      "    69   229   132  4249  1785   186 22268  1782     9  9514    81     2\n",
      "   412   527   824  1087     2    23    19    46  3925     3   129   124\n",
      "    19   288  1779   213  9514    81  1353   181   122    22   229  2532\n",
      "   132   116   138   320 13337  7977   181    50  2125  2002  4019  3258\n",
      "    11  1668   676   527  1347  8048  2021 22742  2598     2   346   186\n",
      "  2024  8748 26032     5     2   616    28 13627    16    78   213  4811\n",
      "    78  3155  4979     3   567  3112  1859     2    28 10064    40   234\n",
      "    19    46   576    71 16994   379 17489     5    11  1396  4606  2021\n",
      "   284    61   542    28  2510   756   186   615  1019   213  5256   157\n",
      "    78  3155   379 19719     4    11  1668   676   527  1347  8048  2021\n",
      "   273   132    28 15980    61   320    28  2510   756   192    54  2021\n",
      "   615  1019   213  5256   157  5754   527  4811 16981   872   213   947\n",
      "    78  3155   379 11993    11     9  5256   157  6112   412 16981  1353\n",
      " 17444    71    15  1081   872   213 13337  7977 10422   132  1668   379\n",
      "    69   969   285  4795  3831  4484  1353   691    68  2136     7     5\n",
      "   384   132   213  2642  1782    69   229    28  3115   516   527   213\n",
      " 13337  7977   228   186    89  5784   186 13904   273    64   320   134\n",
      "     2 25702   186    31   228  3898    22  1113     3   252  3155  1859\n",
      "     2   141   926   171    22  1353  1496     2 16981  1606   236    15\n",
      "  1436  4078 15876    81  1019   213   947     7     5   117 20413   114\n",
      " 17284 12323 26113    80   186  2099  1783   852   527   134  5475    28\n",
      " 16379     3     9   513   229  2685  1696  1059   770   527  8134    10\n",
      "     1     0 21679  6475 18981 27439  9275  1628  3975 16981     2  1779\n",
      "  1353  1496   412    22   346   213   947    78  3155     2  1504   864\n",
      " 19300   213  4674   669   391  1838    15  2642  2460 16346 27439  6774\n",
      "  1628     9 10064   229   897   412    28   763  1718     2   286   320\n",
      "  1120    91   292     2  1248    28  2692  1463   186    28 10722   959\n",
      " 13380   314 16346 27439  6774  1628 16981     2  1120  4617 27439  9275\n",
      "  1628  1353  1496   132   213 22208   186 10255   102  7887  9501   911\n",
      "  1248   213  5256   157     2    35    22  2232   320  2057   236   186\n",
      "   211   399 16346 27439  6774  1628    69   793  2021   285    22    40\n",
      "   369   553   213   157   171  2104     1]\n"
     ]
    }
   ],
   "source": [
    "# print corresponding integer values\n",
    "print(input_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.711546Z",
     "iopub.status.busy": "2020-10-18T15:27:15.711174Z",
     "iopub.status.idle": "2020-10-18T15:27:15.841003Z",
     "shell.execute_reply": "2020-10-18T15:27:15.840339Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.711490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\n",
      " Police in Texas have released a sketch of a man suspected of shooting\n",
      "a TV weatherman multiple times as he left work on Wednesday morning.\n",
      "The sketch was drawn  during a hospital bed interview with KCEN-TV\n",
      "meteorologist Patrick Crawford, 35, who is recovering after being shot\n",
      "in the stomach and shoulder. The suspect, who managed to escape the\n",
      "scene in Bruceville-Eddy on foot on Wednesday morning, is described as\n",
      "a white male, 30 to 35 years old, with a medium build and a receding\n",
      "hairline. 'We are actively looking for [the suspect],' Trooper D.L.\n",
      "Wilson of the Texas Department of Public Safety told ABC News. 'We had\n",
      "troopers out overnight, but we didn't get any calls of suspicious\n",
      "people. We can't rule out that he's not in the area, but more than\n",
      "likely, he's left the area.' Scroll down for video . Suspect: KCEN\n",
      "weatherman Patrick Crawford (left) helped police put together this\n",
      "sketch (right) of the man who shot him as he left the TV studio on\n",
      "Wednesday morning. He said he had never seen him before . Crawford,\n",
      "the morning weatherman for KCEN, was shot after he climbed into his\n",
      "car around 9.15am on Wednesday. An unknown man approached him in the\n",
      "parking lot and they exchanged words. The weatherman managed to back\n",
      "up at high speed and drive out of the parking lot. He crossed the\n",
      "interstate and drove to a group of construction workers, who\n",
      "administered first aid and called 911. Doctors at Scott and White\n",
      "Memorial Hospital in Temple said that Crawford was recovering and in\n",
      "fair condition after surgery for a gunshot wound to the abdomen. From\n",
      "his hospital bed, he spoke to investigators and told them he had never\n",
      "seen the man before. Police have not yet found a motive for the\n",
      "shooting. Gunned down: Footage shows Crawford, 35, being stretchered\n",
      "away after he was shot on Wednesday . Attack: The man continued to\n",
      "shoot at Crawford as he drove away and flagged down help nearby . Hit:\n",
      "The weatherman sustained gunshot wounds to the stomach and shoulder,\n",
      "and a bullet grazed his head . KCEN also reported that police were at\n",
      "the station on Thursday, checking IDs of everyone driving in the\n",
      "parking lot. Crawford's wife, fellow KCEN meteorologist Heather\n",
      "Brinkmann, also revealed his quick decision to drive off and flag down\n",
      "help saved his life. 'Doctors continue to tell me how well he's doing\n",
      "considering the multiple gunshot wounds,' she wrote on her Facebook\n",
      "page on Wednesday night. 'Police tell me that if it wasn't for his\n",
      "smarts to hurry up and drive off and flag down construction workers,\n",
      "it could have been a whole different story. He is truly my hero.'\n",
      "Brinkmann and other KCEN employees also uploaded Superman's 'S' emblem\n",
      "to their Facebook pages in a sign of support for Crawford. KCEN shared\n",
      "the image with the hashtag '#oursuperman' to its own page. Heroic:\n",
      "Crawford's wife Heather said police believe he saved his life by\n",
      "driving off and flagging down help . Attack: Crawford is pictured left\n",
      "with his fellow KCEN hosts on Wednesday - just hours before he was\n",
      "shot . Crawford, who grew up in Plano, Texas and earned his\n",
      "meteorology degree from Northern Illinois University, moved from New\n",
      "Orleans and joined the station in September 2012. He  married\n",
      "Brinkmann, a fellow meteorologist and KCEN producer, last year. The\n",
      "station's news director, Jim Hice, released a statement about the\n",
      "shooting on Wednesday. 'Early this morning KCEN-HD News received the\n",
      "devastating news that one of our own, Texas Today Meteorologist\n",
      "Patrick Crawford, had been shot in front of our building,' he said on\n",
      "KCEN. 'He is currently at Baylor Scott and White Hospital in Temple\n",
      "after undergoing surgery. He is in stable condition and resting. 'The\n",
      "shooter, as of this note, has not been caught. We do not know who the\n",
      "shooter was or if he is connected in any way to KCEN or its\n",
      "employees.' Updates: Texas Department of Public Safety officers DL\n",
      "Wilson, left and Harpin Myers, give a briefing on the shooting on\n",
      "Wednesday afternoon. By Thursday morning, a suspect had still not been\n",
      "taken into custody . Concerns: Law enforcement officers set up near a\n",
      "command post and look for the gunman on Wednesday . Response: Texas\n",
      "Department of Public Safety officers go in a pickup to a command post\n",
      "while other officers look for the gunman accused of shooting Crawford\n",
      "outside the station on Wednesday . Rural: The gunman attacked as\n",
      "Crawford was climbing into his car outside the KCEN studios in Texas .\n",
      "He added that Brinkmann was by her husband's side in the hospital. 'He\n",
      "is a loved member of the KCEN family and our thoughts and prayers go\n",
      "out to him, Heather and their family,' he wrote. On Wednesday morning,\n",
      "just hours before he was shot, Crawford showed off his winning\n",
      "Christmas sweater for the station's 'Ugly Sweater Contest' and shared\n",
      "images online of him raising a trophy. The town is about 75 miles\n",
      "north of Austin.<EOS><pad>Meteorologist Patrick Crawford, who was shot\n",
      "as he left the station on Wednesday, helped police sketch the drawing\n",
      "from his hospital bed . The suspect is described as a white male, 30\n",
      "to 35 years old, with a medium build and a receding hairline .\n",
      "Crawford, 35, was shot in the stomach and shoulder after exchanging\n",
      "words with the gunman, but he managed to drive off and get help . He\n",
      "told officers that he had never seen the man before .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# print the article and its summary\n",
    "print('Article:\\n\\n', detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T15:52:27.029686Z",
     "iopub.status.busy": "2020-10-11T15:52:27.029448Z",
     "iopub.status.idle": "2020-10-11T15:52:27.035893Z",
     "shell.execute_reply": "2020-10-11T15:52:27.034128Z",
     "shell.execute_reply.started": "2020-10-11T15:52:27.029660Z"
    }
   },
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Summarization with transformer\n",
    "![alt_text](images/transformer_decoder_zoomin.png)\n",
    "\n",
    "<a name='2.1'></a>\n",
    "## 2.1 Dot product attention \n",
    "\n",
    "Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output.   \n",
    "![alt_text](images/dotproduct.png)\n",
    "\n",
    "Here are some helper functions that will help you create tensors and display useful information:\n",
    "   - `create_tensor`  creates a `jax numpy array` from a list of lists.\n",
    "   - `display_tensor` prints out the shape and the actual tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.842295Z",
     "iopub.status.busy": "2020-10-18T15:27:15.841945Z",
     "iopub.status.idle": "2020-10-18T15:27:15.846614Z",
     "shell.execute_reply": "2020-10-18T15:27:15.846019Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.842271Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    \"\"\"\n",
    "    Create tensor from list of lists\n",
    "    \"\"\"\n",
    "    return jnp.array(t)\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \"\"\"Display shape and tensor\"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for attention is this one:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$\n",
    "\n",
    "$d_{k}$ stands for the dimension of queries and keys.\n",
    "\n",
    "The `query`, `key`, `value` and `mask` vectors are provided for this example.\n",
    "\n",
    "Notice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.847897Z",
     "iopub.status.busy": "2020-10-18T15:27:15.847630Z",
     "iopub.status.idle": "2020-10-18T15:27:15.868672Z",
     "shell.execute_reply": "2020-10-18T15:27:15.867948Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.847872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankar/dev/tools/anaconda3/envs/trax/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.870053Z",
     "iopub.status.busy": "2020-10-18T15:27:15.869690Z",
     "iopub.status.idle": "2020-10-18T15:27:15.930921Z",
     "shell.execute_reply": "2020-10-18T15:27:15.930335Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.870017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query dot key shape: (2, 2)\n",
      "\n",
      "[[0.57735026 2.309401  ]\n",
      " [1.1547005  2.8867514 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
    "display_tensor(q_dot_k, 'query dot key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.932140Z",
     "iopub.status.busy": "2020-10-18T15:27:15.931836Z",
     "iopub.status.idle": "2020-10-18T15:27:15.949347Z",
     "shell.execute_reply": "2020-10-18T15:27:15.948461Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.932114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key shape: (2, 2)\n",
      "\n",
      "[[ 5.7735026e-01  2.3094010e+00]\n",
      " [-1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked = q_dot_k + m\n",
    "display_tensor(masked, 'masked query dot key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.951098Z",
     "iopub.status.busy": "2020-10-18T15:27:15.950816Z",
     "iopub.status.idle": "2020-10-18T15:27:15.997800Z",
     "shell.execute_reply": "2020-10-18T15:27:15.997159Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.951072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key dot value shape: (2, 3)\n",
      "\n",
      "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
      " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(masked @ v, 'masked query dot key dot value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. \n",
    "- The mask is also replaced by a version of it that resembles the one that is used by trax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:15.999717Z",
     "iopub.status.busy": "2020-10-18T15:27:15.999361Z",
     "iopub.status.idle": "2020-10-18T15:27:16.012623Z",
     "shell.execute_reply": "2020-10-18T15:27:16.011914Z",
     "shell.execute_reply.started": "2020-10-18T15:27:15.999681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_with_batch = q[None, :]\n",
    "display_tensor(q_with_batch, 'query with batch dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.014199Z",
     "iopub.status.busy": "2020-10-18T15:27:16.013973Z",
     "iopub.status.idle": "2020-10-18T15:27:16.022020Z",
     "shell.execute_reply": "2020-10-18T15:27:16.021267Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.014171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "\n",
      "value with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[0 1 0]\n",
      "  [1 0 1]]]\n",
      "\n",
      "boolean mask shape: (2, 2)\n",
      "\n",
      "[[ True  True]\n",
      " [False  True]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_with_batch = k[None,:]\n",
    "display_tensor(k_with_batch, 'key with batch dim')\n",
    "v_with_batch = v[None,:]\n",
    "display_tensor(v_with_batch, 'value with batch dim')\n",
    "m_bool = create_tensor([[True, True], [False, True]])\n",
    "display_tensor(m_bool, 'boolean mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 01\n",
    "\n",
    "**Instructions:** Implement the dot product attention. Concretely, implement the following equation\n",
    "\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$\n",
    "\n",
    "$Q$ - query, \n",
    "$K$ - key, \n",
    "$V$ - values, \n",
    "$M$ - mask, \n",
    "${d_k}$ - depth/dimension of the queries and keys (used for scaling down)\n",
    "\n",
    "You can implement this formula either by `trax` numpy (trax.math.numpy) or regular `numpy` but it is recommended to use `jnp`.\n",
    "\n",
    "Something to take into consideration is that within trax, the masks are tensors of `True/False` values not 0's and $-\\infty$ as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use `jnp.where()` and treat the **mask as a tensor of boolean values with `False` for values that need to be masked and True for the ones that don't.**\n",
    "\n",
    "Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as `@` for dot product or `.T` for transposing. Use `jnp.matmul()` and `jnp.swapaxes()` instead.\n",
    "\n",
    "This is the self-attention block for the transformer decoder. Good luck!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.023509Z",
     "iopub.status.busy": "2020-10-18T15:27:16.023201Z",
     "iopub.status.idle": "2020-10-18T15:27:16.034664Z",
     "shell.execute_reply": "2020-10-18T15:27:16.033790Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.023473Z"
    }
   },
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: DotProductAttention\n",
    "def DotProductAttention(query, key, value, mask):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
    "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
    "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    depth = query.shape[-1]\n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
    "    \n",
    "    # Apply the mask\n",
    "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "    \n",
    "    # Softmax formula implementation\n",
    "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
    "    # Hint: Last axis should be used and keepdims should be True\n",
    "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use jnp.exp()\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use jnp.matmul()\n",
    "    attention = jnp.matmul(dots, value)\n",
    "\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.036324Z",
     "iopub.status.busy": "2020-10-18T15:27:16.035987Z",
     "iopub.status.idle": "2020-10-18T15:27:16.323195Z",
     "shell.execute_reply": "2020-10-18T15:27:16.322533Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.036292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
       "              [1.        , 0.        , 1.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "\n",
    "## 2.2 Causal Attention\n",
    "Causal Attention: Multi-Headed attention with a mask to attend only to words that occured before\n",
    "![alt_text](images/causal.png)\n",
    "A word can see everything that was there before. Done by transforming vectors through reshaping.\n",
    "\n",
    "<a name='ex02'></a>\n",
    "### Exercise 02\n",
    "- Compute Attention Heads: Gets an input x of dimension(batch_size, saqlen, n_heads X d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size x n_heads, saqlen, d_head)\n",
    "- Dot Product Self Attention: Creates a mask matrix with False values above the diagonal and True values below and calls DotProductAttention which implements dot product self attention\n",
    "- Compute Attention Output: Undoes compute_attention_heads by splitting first(Vertical) dimension and stacking in the last(depth) dimension (batch_size, saqlen, n_heads x d_head). These operations concatenate the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.324613Z",
     "iopub.status.busy": "2020-10-18T15:27:16.324388Z",
     "iopub.status.idle": "2020-10-18T15:27:16.389759Z",
     "shell.execute_reply": "2020-10-18T15:27:16.389065Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.324588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query matrix (2D tensor) shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
      "\n",
      "[[[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]\n",
      "\n",
      "\n",
      " [[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]]\n",
      "\n",
      "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor2d = create_tensor(q)\n",
    "display_tensor(tensor2d, 'query matrix (2D tensor)')\n",
    "\n",
    "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
    "display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n",
    "\n",
    "tensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n",
    "\n",
    "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[[[0, 0, 0] ---> 3\n",
    "[0 0 0]] ----> 2\n",
    "[[1 1 1]\n",
    "[1 1 1]]] ----> 2\n",
    "[[[2 2 2]\n",
    "[2 2 2]]\n",
    "[[3 3 3]\n",
    "[3 3 3]]]] ----> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T14:54:45.042133Z",
     "iopub.status.busy": "2020-10-14T14:54:45.041884Z",
     "iopub.status.idle": "2020-10-14T14:54:45.050199Z",
     "shell.execute_reply": "2020-10-14T14:54:45.048787Z",
     "shell.execute_reply.started": "2020-10-14T14:54:45.042109Z"
    }
   },
   "source": [
    "It is important to know that the following 3 functions would normally be defined within the `CausalAttention` function further below. \n",
    "\n",
    "However this makes these functions harder to test. Because of this, these functions are shown individually using a `closure` (when necessary) that simulates them being inside of the `CausalAttention` function. This is done because they rely on some variables that can be accessed from within `CausalAttention`.\n",
    "\n",
    "### Support Functions\n",
    "\n",
    "<span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
    "\n",
    "**For the closures you only have to fill the inner function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.391515Z",
     "iopub.status.busy": "2020-10-18T15:27:16.391220Z",
     "iopub.status.idle": "2020-10-18T15:27:16.397777Z",
     "shell.execute_reply": "2020-10-18T15:27:16.396963Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.391487Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    \"\"\"\n",
    "    Function that simulates environment inside CausalAttention function\n",
    "    Args:\n",
    "        d_head (int): Dimensionality of heads\n",
    "        n_heads (int): number of attention heads\n",
    "    Returns\n",
    "        function: compute_attention_heads function\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\"\n",
    "        Compute the attention heads\n",
    "        Args\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head)\n",
    "        Returns\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Size of the x's batch dimension\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Length of the sequence\n",
    "        # Should be the size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
    "        x = jnp.reshape(x, [batch_size, seqlen, n_heads, d_head])\n",
    "        # Transpose x using jnp.transpose()\n",
    "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
    "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
    "        x = jnp.transpose(x, [0, 2, 1, 3])\n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
    "        x = jnp.reshape(x, [batch_size * n_heads, seqlen, d_head])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.399914Z",
     "iopub.status.busy": "2020-10-18T15:27:16.399454Z",
     "iopub.status.idle": "2020-10-18T15:27:16.446927Z",
     "shell.execute_reply": "2020-10-18T15:27:16.445779Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.399883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "output tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(tensor3dc3b, \"input tensor\")\n",
    "result_cah = compute_attention_heads_closure(2,3)(tensor3dc3b)\n",
    "display_tensor(result_cah, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dot_product_self_attention** : Creates a mask matrix with False values above the diagonal and True values below and calls DotProductAttention which implements dot product self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.448827Z",
     "iopub.status.busy": "2020-10-18T15:27:16.448495Z",
     "iopub.status.idle": "2020-10-18T15:27:16.453767Z",
     "shell.execute_reply": "2020-10-18T15:27:16.452944Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.448797Z"
    }
   },
   "outputs": [],
   "source": [
    "def dot_product_self_attention(q, k, v):\n",
    "    \"\"\"\n",
    "    Masked dot product self attention\n",
    "    Args: \n",
    "        q: Queries\n",
    "        k: Keys\n",
    "        v: Values\n",
    "    Returns:\n",
    "        DeviceArray: Masked dot product self attention tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hint: mask size should be equal to L_q. Remember that q has shame batch_size, L_q, d\n",
    "    mask_size = q.shape[1]\n",
    "    \n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
    "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.455982Z",
     "iopub.status.busy": "2020-10-18T15:27:16.455143Z",
     "iopub.status.idle": "2020-10-18T15:27:16.510832Z",
     "shell.execute_reply": "2020-10-18T15:27:16.510247Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.455919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.        , 1.        , 0.        ],\n",
       "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compute_attention_output** : Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads × d_head). These operations concatenate (stack/merge) the heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.512156Z",
     "iopub.status.busy": "2020-10-18T15:27:16.511943Z",
     "iopub.status.idle": "2020-10-18T15:27:16.517707Z",
     "shell.execute_reply": "2020-10-18T15:27:16.516932Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.512129Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_output function\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \"\"\" Compute the attention output.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
    "        x = jnp.reshape(x, [int(x.shape[0] / n_heads), n_heads, seqlen, d_head])\n",
    "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
    "        x = jnp.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Reshape to allow to concatenate the heads\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.526323Z",
     "iopub.status.busy": "2020-10-18T15:27:16.525528Z",
     "iopub.status.idle": "2020-10-18T15:27:16.547981Z",
     "shell.execute_reply": "2020-10-18T15:27:16.547387Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.526266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "output tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(result_cah, \"input tensor\")\n",
    "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
    "display_tensor(result_cao, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T16:56:34.136914Z",
     "iopub.status.busy": "2020-10-14T16:56:34.136700Z",
     "iopub.status.idle": "2020-10-14T16:56:34.280511Z",
     "shell.execute_reply": "2020-10-14T16:56:34.279826Z",
     "shell.execute_reply.started": "2020-10-14T16:56:34.136891Z"
    }
   },
   "source": [
    "### Causal Attention Function\n",
    "![alt_text](images/masked-attention.png)\n",
    "\n",
    "Implement and return a causal attention, through `tl.Serial` with the following\n",
    "\n",
    "- `tl.Branch`: consisting of 3 [tl.Densed(d_feature), ComputeAttentionHeads] to account for queries, keys and values\n",
    "- `tl.Fn`: Takes in dot_product_self_attention function and uses it to compute the dot product using Q, K, V\n",
    "- `tl.Fn`: Takes in compute_attention_output_closure to allow for parallel computing\n",
    "- `tl.Dense`: Final dense layer with dimension d_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.550298Z",
     "iopub.status.busy": "2020-10-18T15:27:16.550011Z",
     "iopub.status.idle": "2020-10-18T15:27:16.557238Z",
     "shell.execute_reply": "2020-10-18T15:27:16.555573Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.550271Z"
    }
   },
   "outputs": [],
   "source": [
    "def CausalAttention(\n",
    "    d_feature,\n",
    "    n_heads,\n",
    "    compute_attenention_heads_closure=compute_attention_heads_closure,\n",
    "    dot_product_self_attention=dot_product_self_attention,\n",
    "    compute_attention_output_closure=compute_attention_output_closure,\n",
    "    mode='train'\n",
    "):\n",
    "    \"\"\"\n",
    "    Transformer style multi-headed causal attention\n",
    "    Args:\n",
    "    d_feature(int): Dimensionality of feature embedding\n",
    "    n_heads: Number of attention bias\n",
    "    compute_attention_head_closure(func): Closure around compute_attention heads\n",
    "    dot_product_self_attention (function): dot_product_self_attention function. \n",
    "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
    "        mode (str): 'train' or 'eval'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "    \n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
    "    \n",
    "    return tl.Serial(\n",
    "        tl.Branch(\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads],\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads],\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads]\n",
    "        ),\n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
    "        tl.Dense(d_feature) # Final dense layer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.561005Z",
     "iopub.status.busy": "2020-10-18T15:27:16.560333Z",
     "iopub.status.idle": "2020-10-18T15:27:16.566299Z",
     "shell.execute_reply": "2020-10-18T15:27:16.565446Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.560959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Branch_out3[\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "  ]\n",
      "  DotProductAttn_in3\n",
      "  AttnOutput\n",
      "  Dense_512\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(CausalAttention(d_feature=512, n_heads=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T14:54:54.326748Z",
     "iopub.status.busy": "2020-10-15T14:54:54.326494Z",
     "iopub.status.idle": "2020-10-15T14:54:54.332598Z",
     "shell.execute_reply": "2020-10-15T14:54:54.330977Z",
     "shell.execute_reply.started": "2020-10-15T14:54:54.326721Z"
    }
   },
   "source": [
    "<a name='2.3'></a>\n",
    "\n",
    "## 2.3 Transformer decoder block\n",
    "![alt_text](images/transformer_decoder_1.png)\n",
    "\n",
    "To implement this function, you will have to call the `CausalAttention` or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: \n",
    "\n",
    "- `tl.LayerNorm`: used to layer normalize\n",
    "- `tl.Dense`: the dense layer\n",
    "- `ff_activation`: feed forward activation here(ReLU)\n",
    "- `tl.Dropout`: dropout layer\n",
    "- `tl.Dense`: dense layer\n",
    "- `dl.Dropout`: dropout layer\n",
    "\n",
    "Implement the entire block using 2 `tl.Residual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.568140Z",
     "iopub.status.busy": "2020-10-18T15:27:16.567710Z",
     "iopub.status.idle": "2020-10-18T15:27:16.574743Z",
     "shell.execute_reply": "2020-10-18T15:27:16.573721Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.568110Z"
    }
   },
   "outputs": [],
   "source": [
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Create masked multi-head attention block using CausalAttention function\n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    feed_forward = [ \n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(n_units=d_ff),\n",
    "        # Add activation function passed in as a parameter (you need to call it!)\n",
    "        ff_activation(), # Generally ReLU\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(dropout, mode=mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(n_units=d_model),\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(dropout, mode=mode)\n",
    "    ]\n",
    "\n",
    "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.LayerNorm(),\n",
    "          # Add causal attention block previously defined (without parentheses)\n",
    "          causal_attention,\n",
    "          #tlAdd dropout with rate and mode specified\n",
    "          tl.Dropout(dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          # Add feed forward block (without parentheses)\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.576942Z",
     "iopub.status.busy": "2020-10-18T15:27:16.576706Z",
     "iopub.status.idle": "2020-10-18T15:27:16.582466Z",
     "shell.execute_reply": "2020-10-18T15:27:16.581576Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.576918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Serial[\n",
      "        Branch_out3[\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "        ]\n",
      "        DotProductAttn_in3\n",
      "        AttnOutput\n",
      "        Dense_512\n",
      "      ]\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "], Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Dense_2048\n",
      "      Relu\n",
      "      Dropout\n",
      "      Dense_512\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "## 2.4 Transformer Language Model\n",
    "\n",
    "You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing.\n",
    "![alt_text](images/transformer_decoder.png)\n",
    "\n",
    "Transformer Language Model..\n",
    "- Positional Encoder\n",
    "    - tl.Embedding\n",
    "    - tl.Dropout\n",
    "    - tl.PositionalEncoding\n",
    "- A list of n_layers decoder blocks\n",
    "- tl.Serial: takes in the following layers or lists of layers\n",
    "    - tl.ShiftRight: shift the tensor to the right by adding on axis 1\n",
    "    - positional_encoder: encodes the text positions\n",
    "    - decoder_blocks: the ones you created\n",
    "    - tl.LayerNorm: a layer norm\n",
    "    - tl.Dense: takes in the voab size\n",
    "    - tl.LogSoftmax: to predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.584124Z",
     "iopub.status.busy": "2020-10-18T15:27:16.583745Z",
     "iopub.status.idle": "2020-10-18T15:27:16.594019Z",
     "shell.execute_reply": "2020-10-18T15:27:16.591794Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.584094Z"
    }
   },
   "outputs": [],
   "source": [
    "def TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_len=4096,\n",
    "    mode='train',\n",
    "    ff_activation=tl.Relu\n",
    "):\n",
    "    \n",
    "    \"\"\"Returns a Transformer language model.\n",
    "\n",
    "    The input to the model is a tensor of tokens. (This model uses only the\n",
    "    decoder part of the overall Transformer.)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    # Embedding inputs and positional encoder\n",
    "    positional_encoder = [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(dropout, mode=mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(1), # Specify the mode!\n",
    "        # Add positional encoder\n",
    "        positional_encoder,\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
    "        tl.Dense(vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.595855Z",
     "iopub.status.busy": "2020-10-18T15:27:16.595541Z",
     "iopub.status.idle": "2020-10-18T15:27:16.615327Z",
     "shell.execute_reply": "2020-10-18T15:27:16.614452Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.595822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  ShiftRight(1)\n",
      "  Embedding_33300_512\n",
      "  Dropout\n",
      "  PositionalEncoding\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Serial[\n",
      "          Branch_out3[\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "          ]\n",
      "          DotProductAttn_in3\n",
      "          AttnOutput\n",
      "          Dense_512\n",
      "        ]\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Dense_2048\n",
      "        Relu\n",
      "        Dropout\n",
      "        Dense_512\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  LayerNorm\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(TransformerLM(n_layers=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:27:16.616842Z",
     "iopub.status.busy": "2020-10-18T15:27:16.616607Z",
     "iopub.status.idle": "2020-10-18T15:27:16.624853Z",
     "shell.execute_reply": "2020-10-18T15:27:16.623580Z",
     "shell.execute_reply.started": "2020-10-18T15:27:16.616819Z"
    }
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "# UNQ_C8\n",
    "# GRADED FUNCTION: train_model\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
    "    '''\n",
    "    Input:\n",
    "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
    "        train_gen (generator): Training stream of data.\n",
    "        eval_gen (generator): Evaluation stream of data.\n",
    "        output_dir (str): folder to save your file.\n",
    "        \n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop.\n",
    "    '''\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
    "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=10\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loop = training.Loop(TransformerLM(d_model=4,\n",
    "                                       d_ff=16,\n",
    "                                       n_layers=1,\n",
    "                                       n_heads=2,\n",
    "                                       mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:45:35.195114Z",
     "iopub.status.busy": "2020-10-18T15:45:35.194873Z",
     "iopub.status.idle": "2020-10-18T15:46:41.649244Z",
     "shell.execute_reply": "2020-10-18T15:46:41.648457Z",
     "shell.execute_reply.started": "2020-10-18T15:45:35.195091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Ran 1 train steps in 11.99 secs\n",
      "Step      1: train CrossEntropyLoss |  10.41387844\n",
      "Step      1: eval  CrossEntropyLoss |  10.41582870\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 45.31 secs\n",
      "Step     10: train CrossEntropyLoss |  10.41400909\n",
      "Step     10: eval  CrossEntropyLoss |  10.41204834\n",
      "Step     10: eval          Accuracy |  0.00000000\n"
     ]
    }
   ],
   "source": [
    "!rm -f model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream, output_dir=\"model\")\n",
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# Part 4:  Evaluation  \n",
    "\n",
    "<a name='4.1'></a>\n",
    "### 4.1 Loading in a trained model\n",
    "\n",
    "    \n",
    "   `Original (pretrained) model: `                                 \n",
    "                                       \n",
    "    TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, \n",
    "                   dropout=0.1, max_len=4096, ff_activation=tl.Relu)\n",
    "                   \n",
    "   `Your model:`\n",
    "   \n",
    "    TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:47:35.318630Z",
     "iopub.status.busy": "2020-10-18T15:47:35.318300Z",
     "iopub.status.idle": "2020-10-18T15:47:35.321746Z",
     "shell.execute_reply": "2020-10-18T15:47:35.320950Z",
     "shell.execute_reply.started": "2020-10-18T15:47:35.318596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the model architecture\n",
    "# model = TransformerLM(mode='eval')\n",
    "\n",
    "# load the pre-trained weights\n",
    "# model.init_from_file('model/model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "# Part 5: Testing with your own input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:47:49.276568Z",
     "iopub.status.busy": "2020-10-18T15:47:49.276327Z",
     "iopub.status.idle": "2020-10-18T15:47:49.282699Z",
     "shell.execute_reply": "2020-10-18T15:47:49.281835Z",
     "shell.execute_reply.started": "2020-10-18T15:47:49.276543Z"
    }
   },
   "outputs": [],
   "source": [
    "def next_symbol(cur_output_tokens, model):\n",
    "    \"\"\"\n",
    "    Returns the next symbol for a given sentence.\n",
    "\n",
    "    Args:\n",
    "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
    "        model (trax.layers.combinators.Serial): The transformer model.\n",
    "\n",
    "    Returns:\n",
    "        int: tokenized symbol.\n",
    "    \"\"\"\n",
    "    \n",
    "    # current output tokens length\n",
    "    token_length = len(cur_output_tokens)\n",
    "    # calculate the minimum power of 2 big enough to store token_length\n",
    "    # HINT: use np.ceil() and np.log2()\n",
    "    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length+1)))\n",
    "\n",
    "    # Fill cur_output_tokens with 0's until it reaches padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
    "\n",
    "    # model expects a tuple containing two padded tensors (with batch)\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # HINT: output has shape (1, padded_length, vocab_size)\n",
    "    # To get log_probs you need to index output with 0 in the first dim\n",
    "    # token_length in the second dim and all of the entries for the last dim.\n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    return int(np.argmax(log_probs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:47:50.214011Z",
     "iopub.status.busy": "2020-10-18T15:47:50.213801Z",
     "iopub.status.idle": "2020-10-18T15:47:56.083485Z",
     "shell.execute_reply": "2020-10-18T15:47:56.082687Z",
     "shell.execute_reply.started": "2020-10-18T15:47:50.213989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neither'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out!\n",
    "sentence_test_nxt_symbl = \"I want to fly in the sky.\"\n",
    "detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:48:05.060393Z",
     "iopub.status.busy": "2020-10-18T15:48:05.060156Z",
     "iopub.status.idle": "2020-10-18T15:48:05.066287Z",
     "shell.execute_reply": "2020-10-18T15:48:05.064810Z",
     "shell.execute_reply.started": "2020-10-18T15:48:05.060368Z"
    }
   },
   "source": [
    "<a name='5.1'></a>\n",
    "### 5.1 Greedy decoding\n",
    "\n",
    "Now you will implement the greedy_decode algorithm that will call the `next_symbol` function. It takes in the input_sentence, the trained model and returns the decoded sentence. \n",
    "\n",
    "<a name='ex07'></a>\n",
    "### Exercise 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:48:31.956164Z",
     "iopub.status.busy": "2020-10-18T15:48:31.955845Z",
     "iopub.status.idle": "2020-10-18T15:48:31.961132Z",
     "shell.execute_reply": "2020-10-18T15:48:31.960471Z",
     "shell.execute_reply.started": "2020-10-18T15:48:31.956137Z"
    }
   },
   "outputs": [],
   "source": [
    "# UNQ_C10\n",
    "# Decoding functions.\n",
    "def greedy_decode(input_sentence, model):\n",
    "    \"\"\"Greedy decode function.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (string): a sentence or article.\n",
    "        model (trax.layers.combinators.Serial): Transformer model.\n",
    "\n",
    "    Returns:\n",
    "        string: summary of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Use tokenize()\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        # Get next symbol\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        # Append next symbol to original sentence\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        # Append next symbol to generated sentence\n",
    "        generated_output.append(cur_output)\n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-10-18T15:51:19.563540Z",
     "iopub.status.idle": "2020-10-18T15:51:19.563863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test it out on a sentence!\n",
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T15:51:33.799409Z",
     "iopub.status.busy": "2020-10-18T15:51:33.799158Z",
     "iopub.status.idle": "2020-10-18T15:51:33.802724Z",
     "shell.execute_reply": "2020-10-18T15:51:33.801881Z",
     "shell.execute_reply.started": "2020-10-18T15:51:33.799382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test it out with a whole article!\n",
    "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "print(wrapper.fill(article), '\\n')\n",
    "print(greedy_decode(article, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
