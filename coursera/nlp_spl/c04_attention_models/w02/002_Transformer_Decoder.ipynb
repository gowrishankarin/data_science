{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T20:05:17.967931Z",
     "iopub.status.busy": "2020-10-10T20:05:17.967709Z",
     "iopub.status.idle": "2020-10-10T20:05:17.973503Z",
     "shell.execute_reply": "2020-10-10T20:05:17.972385Z",
     "shell.execute_reply.started": "2020-10-10T20:05:17.967904Z"
    }
   },
   "source": [
    "# The Transformer Decoder\n",
    "Implement transformer decoder using Trax\n",
    "\n",
    "- Translate the mathematics of attention into numpy code\n",
    "- How multi-head causal attention fits inot a GPT-2 transformer decoder\n",
    "- How to build one with Trax layers\n",
    "- Implementation of causal attention from scratch\n",
    "- Exploit the handy-dandy tl.CausalAttention() layer\n",
    "\n",
    "Components and Flow of a Transformer Decoder  \n",
    "![alt-text](images/C4_W2_L6_transformer-decoder_S01_transformer-decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2020-10-10T21:28:38.841790Z",
     "shell.execute_reply": "2020-10-10T21:28:38.840599Z",
     "shell.execute_reply.started": "2020-10-10T21:28:33.386318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gin\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to pring the entire np arry\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence gets embedded, add positional encoding\n",
    "Embed the words, then create vectors representing ech word's position in each sentence $\\in {0, 1, 2, \\cdots, K}$ = `range(max_len)`, where max_len=K+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T21:28:38.843213Z",
     "iopub.status.busy": "2020-10-10T21:28:38.842939Z",
     "iopub.status.idle": "2020-10-10T21:28:38.849159Z",
     "shell.execute_reply": "2020-10-10T21:28:38.848339Z",
     "shell.execute_reply.started": "2020-10-10T21:28:38.843188Z"
    }
   },
   "outputs": [],
   "source": [
    "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\n",
    "    \"\"\"\n",
    "    Returns a list of layers that:\n",
    "    1. Takes a block of text as input\n",
    "    2. Embeds the words in that text and\n",
    "    3. Adds positional encoding\n",
    "        i.e. Associates a number in range(max_len) with\n",
    "        each word in each sentence of embeded input text\n",
    "    The input is a list of tokenized blocks of text\n",
    "    \n",
    "    Args\n",
    "    vocab_size (int): vocab_size\n",
    "    d_model: depth of embedding\n",
    "    dropout(float): dropout rate \n",
    "    max_len: maximum symbol length for positional encoding\n",
    "    mode(str): 'train' or 'eval'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Embeddings inputs and positional encoder\n",
    "    return [\n",
    "        # Add embedding layer of dimension(vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add positional encoding layer with minimum input length and mode specified\n",
    "        tl.PositionalEncoding(max_max_len, mode=mode)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Causal Attention\n",
    "The layers and array dimension involved in multi-head causal attention -  which looks at previous words in the input tex\n",
    "![alt-text](images/C4_W2_L5_multi-head-attention_S05_multi-head-attention-concatenation_stripped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tl.CausalAttention()` does all the above\n",
    "- [Q, K, V] is all input text, no need to pass 3 times, `tl.Branch` is used to handle this\n",
    "-  each branch within a tl.Branch() layer performs parallel operations on copies of the layer's inputs. \n",
    "- For causal attention, each branch (representing Q, K, and V) applies a linear transformation (i.e. a dense layer without a subsequent activation) to its copy of the input, then splits that result into heads. \n",
    "- You can see the syntax for this in the screenshot from the trax.layers.attention.py source code below: \n",
    "![alt_text](images/use-of-tl-Branch-in-tl-CausalAttention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward Layer\n",
    "- Typically ends with a ReLU activtion, but we will leave open the possibility of a different activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T21:28:38.853056Z",
     "iopub.status.busy": "2020-10-10T21:28:38.852724Z",
     "iopub.status.idle": "2020-10-10T21:28:38.858159Z",
     "shell.execute_reply": "2020-10-10T21:28:38.857239Z",
     "shell.execute_reply.started": "2020-10-10T21:28:38.853025Z"
    }
   },
   "outputs": [],
   "source": [
    "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\n",
    "    \"\"\"\n",
    "    Returns a list of layers that implements a FF bloc\n",
    "    \n",
    "    The input is an activation tensor\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "        \n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    return [\n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_ff),\n",
    "        # Add activtion function passed in as a parameter\n",
    "        ff_activation(), # ~ReLU\n",
    "        # Add dropout with rate and mode specified (ie. do not use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_model),\n",
    "        # Add dropout with rate and mode speficied (i.e. dont use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder block\n",
    "- Here, we return a list containing two residual blocks. \n",
    "- The first wraps around the causal attention layer, whose inputs are normalized and to which we apply dropout regulation. \n",
    "- The second wraps around the feed-forward layer. \n",
    "- You may notice that the second call to `tl.Residual()` doesn't call a normalization layer before calling the feed-forward layer. \n",
    "- This is because the normalization layer is included in the feed-forward layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T21:28:38.859968Z",
     "iopub.status.busy": "2020-10-10T21:28:38.859698Z",
     "iopub.status.idle": "2020-10-10T21:28:38.864774Z",
     "shell.execute_reply": "2020-10-10T21:28:38.864036Z",
     "shell.execute_reply.started": "2020-10-10T21:28:38.859939Z"
    }
   },
   "outputs": [],
   "source": [
    "def DecoderBlock(\n",
    "    d_model, d_ff, n_heads,\n",
    "    dropout, mode, ff_activation\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of layers that implements a Transformer decoder block\n",
    "    \n",
    "    The input is an activation tensor\n",
    "    \n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add list of two Residual Blocks: the attention with normalization and dropout and FF blocks\n",
    "    \n",
    "    return [\n",
    "        tl.Residual(\n",
    "            tl.LayerNorm(),\n",
    "            # Add causal attention\n",
    "            tl.CausalAttention(d_feature, n_heads=n_heads, dropout=dropout, mode=mode)\n",
    "        ),\n",
    "        tl.Residual(\n",
    "            # Add feed-forwards block\n",
    "            # We dont need to normalize the layer inputs here.\n",
    "            # The feed-forward block takes care of that for us\n",
    "            FeedForward(d_modl, d_ff, dropout, mode, ff_activation)\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Decoder\n",
    "aka. repead N times Dense Layer and Softmax for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T21:28:44.636447Z",
     "iopub.status.busy": "2020-10-10T21:28:44.636221Z",
     "iopub.status.idle": "2020-10-10T21:28:44.642772Z",
     "shell.execute_reply": "2020-10-10T21:28:44.642084Z",
     "shell.execute_reply.started": "2020-10-10T21:28:44.636424Z"
    }
   },
   "outputs": [],
   "source": [
    "def TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_len=4096,\n",
    "    mode='train',\n",
    "    ff_activation=tl.Relu\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a Transformer Language Model\n",
    "    \n",
    "    The input to the model is a tensor of tokens (this model uses only the\n",
    "    decoder part of the verall Transformer)\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "        \n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer  language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [\n",
    "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)\n",
    "    ]\n",
    "    \n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        # Add embedding inputs and positional encoder\n",
    "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a logits layer, Note: activation already set by ff_activation)\n",
    "        tl.Dense(vocab_size),\n",
    "        # Get Probabilities with LogSoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
