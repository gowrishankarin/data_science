{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 3 ways of Attention and Dot Product Attention\n",
    "Implementation of later 2 attentions\n",
    "- Encoder - Decoder Attentions\n",
    "- Causal Attention\n",
    "- Bi-Directional Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T17:26:21.349480Z",
     "iopub.status.busy": "2020-10-10T17:26:21.349228Z",
     "iopub.status.idle": "2020-10-10T17:26:21.356812Z",
     "shell.execute_reply": "2020-10-10T17:26:21.355588Z",
     "shell.execute_reply.started": "2020-10-10T17:26:21.349446Z"
    }
   },
   "source": [
    "1. Attention model constitute powerful tools in the NLP practitioner toolkit\n",
    "2. Like LSTMs, they learn which words are most important to phrases, sentences, paragraphs and so on\n",
    "3. They mitigate the vanishing gradient problem better than LSTMs\n",
    "4. Attention combined with LSTMs to form encoder-decoder models \n",
    "\n",
    "![alt text](images/C4_W2_L3_dot-product-attention_S01_introducing-attention_stripped.png)\n",
    "\n",
    "Covers, Integrate attention into transformers. Transformers are not sequence models, easier to parallelize and accelerate. Applictions of transformers are\n",
    "- Auto-completion\n",
    "- Named Entity Recognition\n",
    "- Chatbots\n",
    "- Question-Answering\n",
    "\n",
    "1. Along with embedding, positional encoding, dense layers and residual connection - attention is a crucial component of transformers. \n",
    "2. At the heart of any attention scheme used in a transformer is dot product attention of which the figures below display simplified pic\n",
    "\n",
    "![alt text](images/C4_W2_L3_dot-product-attention_S03_concept-of-attention_stripped.png)\n",
    "![alt text](images/C4_W2_L3_dot-product-attention_S04_attention-math_stripped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With basic dot product attention,\n",
    "    - Interaction between every word(embedding) is captured in query\n",
    "    - Every word is the key\n",
    "    - queries and keys belong to the same sentences - constitutes bi-directional self-attention\n",
    "    - sometimes, its more appropriate to consider only words which have come before the current one\n",
    "    - a case like queries and keys come from the same sentences, fall into the category of causal attention\n",
    "    \n",
    "![alt text](images/C4_W2_L4_causal-attention_S02_causal-attention_stripped.png)\n",
    "for causal attention, we add a mask to the argument of our softmax function\n",
    "![alt text](images/C4_W2_L4_causal-attention_S03_causal-attention-math_stripped.png)\n",
    "![alt text](images/C4_W2_L4_causal-attention_S04_causal-attention-math-2_stripped.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T18:12:26.809527Z",
     "iopub.status.busy": "2020-10-10T18:12:26.809284Z",
     "iopub.status.idle": "2020-10-10T18:12:27.281426Z",
     "shell.execute_reply": "2020-10-10T18:12:27.280496Z",
     "shell.execute_reply.started": "2020-10-10T18:12:26.809500Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "# To pring the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T18:14:26.415444Z",
     "iopub.status.busy": "2020-10-10T18:14:26.415051Z",
     "iopub.status.idle": "2020-10-10T18:14:26.421242Z",
     "shell.execute_reply": "2020-10-10T18:14:26.420047Z",
     "shell.execute_reply.started": "2020-10-10T18:14:26.415386Z"
    }
   },
   "source": [
    "Here are some helper functions that will help you create tensors and display useful information:\n",
    "\n",
    "* `create_tensor()` creates a numpy array from a list of lists.\n",
    "* `display_tensor()` prints out the shape and the actual tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T18:15:34.742892Z",
     "iopub.status.busy": "2020-10-10T18:15:34.742640Z",
     "iopub.status.idle": "2020-10-10T18:15:34.747660Z",
     "shell.execute_reply": "2020-10-10T18:15:34.746762Z",
     "shell.execute_reply.started": "2020-10-10T18:15:34.742857Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    \"\"\"\n",
    "    Create tensor from list of lists\n",
    "    \"\"\"\n",
    "    return np.array(t)\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \"\"\"\n",
    "    Display shape and tensor\n",
    "    \"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create some tensors and display their shapes. \n",
    "- The query, key, and value arrays must all have the same embedding dimensions (number of columns), and \n",
    "- the mask array must have the same shape as `np.dot(query, key.T)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T19:47:47.324820Z",
     "iopub.status.busy": "2020-10-10T19:47:47.324578Z",
     "iopub.status.idle": "2020-10-10T19:47:47.386044Z",
     "shell.execute_reply": "2020-10-10T19:47:47.384855Z",
     "shell.execute_reply.started": "2020-10-10T19:47:47.324794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product Attention\n",
    "$$softmax \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right)$$\n",
    "\n",
    "where the scaling factor $\\sqrt{d}$ is the squre root of the embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T19:34:41.317687Z",
     "iopub.status.busy": "2020-10-10T19:34:41.317441Z",
     "iopub.status.idle": "2020-10-10T19:34:41.325295Z",
     "shell.execute_reply": "2020-10-10T19:34:41.324416Z",
     "shell.execute_reply.started": "2020-10-10T19:34:41.317660Z"
    }
   },
   "outputs": [],
   "source": [
    "def DotProductAttention(query, key, value, mask, scale=True):\n",
    "    \"\"\"\n",
    "    Dot Product Self Attention\n",
    "    Args:\n",
    "        query (numpy.ndarray): array of query representations with shape (L_q by d)\n",
    "        key (numpy.ndarray): array of key representations with shape (L_k by d)\n",
    "        value (numpy.ndarray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (numpy.ndarray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "        scale (bool): whether to scale the dot product of the query and transposed key\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "    \n",
    "    # save depth/dimension of the query embedding for scaling down the dot product\n",
    "    if(scale):\n",
    "        depth = query.shape[-1]\n",
    "    else:\n",
    "        depth = 1\n",
    "        \n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth)\n",
    "    \n",
    "    # Apply the mask\n",
    "    if(mask is not None):\n",
    "        dots = np.where(mask, dots, np.full_like(dots, -1e9))\n",
    "        \n",
    "    # Softmax formula implementation\n",
    "    # Use scipy.special.logsumexp of masked_qkT to avoid underflow by division by large numbers\n",
    "    # Note softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = scipy.special.logsumexp(dots, axis=1, keepdims=True)\n",
    "    \n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use np.exp()\n",
    "    dots = np.exp(dots - logsumexp)\n",
    "    \n",
    "    # Multiply dots by value to get self attention\n",
    "    # use np.matmul()\n",
    "    attention = np.matmul(dots, value)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T19:29:41.207607Z",
     "iopub.status.busy": "2020-10-10T19:29:41.207293Z",
     "iopub.status.idle": "2020-10-10T19:29:41.212490Z",
     "shell.execute_reply": "2020-10-10T19:29:41.211222Z",
     "shell.execute_reply.started": "2020-10-10T19:29:41.207578Z"
    }
   },
   "source": [
    "Now let's implement the *masked* dot product self-attention (at the heart of causal attention) as a special case of dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T19:34:42.308170Z",
     "iopub.status.busy": "2020-10-10T19:34:42.307903Z",
     "iopub.status.idle": "2020-10-10T19:34:42.313273Z",
     "shell.execute_reply": "2020-10-10T19:34:42.312463Z",
     "shell.execute_reply.started": "2020-10-10T19:34:42.308131Z"
    }
   },
   "outputs": [],
   "source": [
    "def dot_product_self_attention(q, k, v, scale=True):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    Args:\n",
    "        q (numpy.ndarray): queries.\n",
    "        k (numpy.ndarray): keys.\n",
    "        v (numpy.ndarray): values.\n",
    "    Returns:\n",
    "        numpy.ndarray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "    # Size of the penultimate dimension of the query\n",
    "    mask_size = q.shape[-2]\n",
    "    \n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Use np.tril() - Lower triangle of an array and np.ones()\n",
    "    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T19:34:42.853232Z",
     "iopub.status.busy": "2020-10-10T19:34:42.852988Z",
     "iopub.status.idle": "2020-10-10T19:34:42.863429Z",
     "shell.execute_reply": "2020-10-10T19:34:42.862616Z",
     "shell.execute_reply.started": "2020-10-10T19:34:42.853206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.35954252, 0.        ],\n",
       "        [1.        , 0.64045748, 1.        ]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_self_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
