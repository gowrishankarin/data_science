{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from utils2 import get_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The continuous bag-of-words model\n",
    "The CBOW model is based on a neural network, the architecture of which looks like the figure below, as you'll recall from the lecture.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='cbow_model_architecture.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:917;height:337;\" /> Figure 1 </div>\n",
    "\n",
    "This part of the notebook will walk you through:\n",
    "\n",
    "- The two activation functions used in the neural network.\n",
    "\n",
    "- Forward propagation.\n",
    "\n",
    "- Cross-entropy loss.\n",
    "\n",
    "- Backpropagation.\n",
    "\n",
    "- Gradient descent.\n",
    "\n",
    "- Extracting the word embedding vectors from the weight matrices once the neural network has been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "**Softmax**\n",
    "The second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n",
    " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n",
    "\\end{align}\n",
    "\n",
    "To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n",
    "\n",
    "$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n",
    "\n",
    "Let's work through an example.\n",
    "\n",
    "**ReLU**  \n",
    "ReLU is used to calculate the values of the hidden layer, in the following formulas:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n",
    " \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    # BEGIN your code here\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    # END your code here\n",
    "    \n",
    "    return result\n",
    "\n",
    "def softmax(z):\n",
    "    # BEGIN your code here\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z\n",
    "    # END your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because I am learning\n",
      "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
      "Size of vocabulary:  5\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.'\n",
    "             or emoji.get_emoji_regexp().search(ch)\n",
    "           ]\n",
    "    return data\n",
    "\n",
    "\n",
    "corpus = 'I am happy because I am learning'\n",
    "print(f'Corpus:  {corpus}')\n",
    "words = tokenize(corpus)\n",
    "print(f'Words (tokens):  {words}')\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='cbow_model_dimensions_single_input.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:839;height:349;\" /> Figure 2 </div>\n",
    "Set $N$ equal to 3. Remember that $N$ is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Weights and Biases\n",
    "- Initialize the weights and biases\n",
    "- Here prepopulated matrices and vectors are presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "Size of W1: (3, 5) (NxV)\n",
      "Size of b1: (3, 1) (Nx1)\n",
      "Size of W2: (5, 3) (VxN)\n",
      "Size of b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'Size of W1: {W1.shape} (NxV)')\n",
    "print(f'Size of b1: {b1.shape} (Nx1)')\n",
    "print(f'Size of W2: {W2.shape} (VxN)')\n",
    "print(f'Size of b2: {b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    # BEGIN your code here\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    # END your code here\n",
    "    return one_hot_vector\n",
    "\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    # BEGIN your code here\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    # END your code here\n",
    "    return context_words_vectors\n",
    "\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n",
    "\n",
    "training_examples = get_training_example(words, 2, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array, y_array = next(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.5 ]\n",
      " [0.  ]]\n",
      "\n",
      "y\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "x = x_array.copy()\n",
    "x.shape = (V, 1)\n",
    "print('x')\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "y = y_array.copy()\n",
    "y.shape = (V, 1)\n",
    "print('y')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values of Hidden Layer\n",
    "\n",
    "$$z_1 = W_1x + b_1$$\n",
    "$$h = ReLU\\left( z_1 \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1\n",
      "[[ 0.36483875]\n",
      " [ 0.63710329]\n",
      " [-0.3236647 ]]\n",
      "h\n",
      "[[0.36483875]\n",
      " [0.63710329]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "z1 = np.dot(W1, x) + b1\n",
    "print(\"z1\")\n",
    "print(z1)\n",
    "h = relu(z1)\n",
    "print(\"h\")\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values of Output Layer\n",
    "$$z_2 = W_2h + b_2$$\n",
    "$$\\hat y = softmax\\left( z_2 \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z2\n",
      "[[-0.31973737]\n",
      " [-0.28125477]\n",
      " [-0.09838369]\n",
      " [-0.33512159]\n",
      " [-0.19919612]]\n",
      "y_hat\n",
      "[[0.18519074]\n",
      " [0.19245626]\n",
      " [0.23107446]\n",
      " [0.18236353]\n",
      " [0.20891502]]\n"
     ]
    }
   ],
   "source": [
    "z2 = np.dot(W2, h) + b2\n",
    "print(\"z2\")\n",
    "print(z2)\n",
    "y_hat = softmax(z2)\n",
    "print(\"y_hat\")\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "$$J = - \\sum\\limits_{k=1}^V y_k \\log\\hat y_k $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_actual):\n",
    "    loss = np.sum(-np.log(y_hat)*y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4650152923611106"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "$$\\frac{\\partial J}{\\partial W_1} = ReLU\\left( W^T_2(\\hat y - y)\\right)x^T \\tag{1}$$\n",
    "$$\\frac{\\partial J}{\\partial W_2} = (\\hat y - y)h^T \\tag{2}$$\n",
    "$$\\frac{\\partial J}{\\partial b_1} = ReLU\\left(W^T_2(\\hat y - y)\\right) \\tag{3}$$\n",
    "$$\\frac{\\partial J}{\\partial b_2} = \\hat y - y \\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate partial derivative of the loss function wrt $b_2$**\n",
    "$$\\frac{\\partial J}{\\partial b_2} = \\hat y - y \\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_b2\n",
      "[[ 0.18519074]\n",
      " [ 0.19245626]\n",
      " [-0.76892554]\n",
      " [ 0.18236353]\n",
      " [ 0.20891502]]\n"
     ]
    }
   ],
   "source": [
    "grad_b2 = y_hat - y\n",
    "print(\"grad_b2\")\n",
    "print(grad_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate partial derivative of loss function wrt $W_2$**\n",
    "$$\\frac{\\partial J}{\\partial W_2} = (\\hat y - y)h^T \\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_W2\n",
      "[[ 0.06756476  0.11798563  0.        ]\n",
      " [ 0.0702155   0.12261452  0.        ]\n",
      " [-0.28053384 -0.48988499  0.        ]\n",
      " [ 0.06653328  0.1161844   0.        ]\n",
      " [ 0.07622029  0.13310045  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "grad_W2 = np.dot(y_hat - y, h.T)\n",
    "print(\"grad_W2\")\n",
    "print(grad_W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate partial derivative of loss function wrt $b_1$**\n",
    "$$\\frac{\\partial J}{\\partial b_1} = ReLU\\left(W^T_2(\\hat y - y)\\right) \\tag{3}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_b1\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.17045858]]\n"
     ]
    }
   ],
   "source": [
    "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
    "print(\"grad_b1\")\n",
    "print(grad_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate partial derivative of loss function wrt $W_1$**\n",
    "$$\\frac{\\partial J}{\\partial W_1} = ReLU\\left( W^T_2(\\hat y - y)\\right)x^T \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_W1\n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.04261464 0.04261464 0.         0.08522929 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n",
    "print(\"grad_W1\")\n",
    "print(grad_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of grad_W1: (3, 5) (NxV)\n",
      "size of grad_b1: (3, 1) (Nx1)\n",
      "size of grad_W2: (3, 5) (VxN)\n",
      "size of grad_b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
    "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
    "print(f'size of grad_W2: {grad_W1.shape} (VxN)')\n",
    "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "During gradient descent phase, update the weights and biases by subtracting $\\alpha$ times the gradient from the original matrtices and vectors\n",
    "\n",
    "$$W_1 := W1 - \\alpha\\frac{\\partial J}{\\partial W_1} \\tag{5}$$\n",
    "$$W_2 := W1 - \\alpha\\frac{\\partial J}{\\partial W_2} \\tag{5}$$\n",
    "$$b_1 := b1 - \\alpha\\frac{\\partial J}{\\partial b_1} \\tag{5}$$\n",
    "$$b_2 := b2 - \\alpha\\frac{\\partial J}{\\partial b_2} \\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "W1_new = W1 - alpha * grad_W1\n",
    "W2_new = W2 - alpha * grad_W2\n",
    "b1_new = b1 - alpha * grad_b1\n",
    "b2_new = b2 - alpha * grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Values\n",
    "#### $W_1, W_{1_{new}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
      "W1 New\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.25359163 -0.25125325 -0.37770863 -0.13956325  0.34008124]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W1\")\n",
    "print(W1)\n",
    "print(\"W1 New\")\n",
    "print(W1_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $W_2, W_{2_{new}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2\n",
      "[[-0.22182064 -0.43008631  0.13310965]\n",
      " [ 0.08476603  0.08123194  0.1772054 ]\n",
      " [ 0.1871551  -0.06107263 -0.1790735 ]\n",
      " [ 0.07055222 -0.02015138  0.36107434]\n",
      " [ 0.33480474 -0.39423389 -0.43959196]]\n",
      "W2 New\n",
      "[[-0.24209007 -0.465482    0.13310965]\n",
      " [ 0.06370138  0.04444758  0.1772054 ]\n",
      " [ 0.27131525  0.08589287 -0.1790735 ]\n",
      " [ 0.05059224 -0.0550067   0.36107434]\n",
      " [ 0.31193865 -0.43416402 -0.43959196]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W2\")\n",
    "print(W2)\n",
    "print(\"W2 New\")\n",
    "print(W2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $b_1, b_{1_{new}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1\n",
      "[[ 0.09688219]\n",
      " [ 0.29239497]\n",
      " [-0.27364426]]\n",
      "b1 New\n",
      "[[ 0.09688219]\n",
      " [ 0.29239497]\n",
      " [-0.32478183]]\n"
     ]
    }
   ],
   "source": [
    "print(\"b1\")\n",
    "print(b1)\n",
    "print(\"b1 New\")\n",
    "print(b1_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $b_2, b_{2_{new}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2\n",
      "[[ 0.0352008 ]\n",
      " [-0.36393384]\n",
      " [-0.12775555]\n",
      " [-0.34802326]\n",
      " [-0.07017815]]\n",
      "b2 New\n",
      "[[-0.02035642]\n",
      " [-0.42167072]\n",
      " [ 0.10292211]\n",
      " [-0.40273232]\n",
      " [-0.13285266]]\n"
     ]
    }
   ],
   "source": [
    "print(\"b2\")\n",
    "print(b2)\n",
    "print(\"b2 New\")\n",
    "print(b2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
