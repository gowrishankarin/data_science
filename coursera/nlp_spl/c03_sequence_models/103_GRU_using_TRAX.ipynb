{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import trax\n",
    "from trax import layers as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Dense_128\n",
      "  Relu\n",
      "  Dense_10\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "mlp = tl.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(10),\n",
    "    tl.LogSoftmax()\n",
    ")\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU MODEL\n",
    "- **ShiftRight** Shifts the tensor to the right by padding on axis 1. The mode should be specified and it refers to the context in which the model is being used. Possible values are `train`, `eval` or `predict`, predict mode is for fast inference. Defaults to train\n",
    "- **Embedding** Maps discrete tokens to vectors. It will have shape `(vocabulary length X dimension of output vectors)`. The dimension of output vectors is the number of elements in the word embedding\n",
    "- **GRU** The GRU layer, it leverages another Tax layer called GRUCell. The number of GRU units should be specified and should match the number of elements in the word embedding. If you want to stack two consecutive GRU layers, it can be done by using python's list comprehension\n",
    "- **Dense** Vanilla Dense Layer\n",
    "- **LogSoftMax** Log Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "vocab_size = 256\n",
    "model_dimension = 512\n",
    "n_layers = 2\n",
    "\n",
    "GRU = tl.Serial(\n",
    "    tl.ShiftRight(mode=mode), # mode parameter to be passed if it is used for inference/test as default\n",
    "    tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n",
    "    [tl.GRU(n_units=model_dimension) for _ in range(n_layers)],\n",
    "    tl.Dense(n_units=vocab_size),\n",
    "    tl.LogSoftmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers: 6\n",
      "\n",
      "=============\n",
      "Serial.sublayers_0: ShiftRight(1)\n",
      "\n",
      "=============\n",
      "Serial.sublayers_1: Embedding_256_512\n",
      "\n",
      "=============\n",
      "Serial.sublayers_2: GRU_512\n",
      "\n",
      "=============\n",
      "Serial.sublayers_3: GRU_512\n",
      "\n",
      "=============\n",
      "Serial.sublayers_4: Dense_256\n",
      "\n",
      "=============\n",
      "Serial.sublayers_5: LogSoftmax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_layers(model, layer_prefix=\"Serial.sublayers\"):\n",
    "    print(f'Total layers: {len(model.sublayers)}\\n')\n",
    "    for i in range(len(model.sublayers)):\n",
    "        print('=============')\n",
    "        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n",
    "        \n",
    "show_layers(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelism\n",
    "dag_concurrency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
