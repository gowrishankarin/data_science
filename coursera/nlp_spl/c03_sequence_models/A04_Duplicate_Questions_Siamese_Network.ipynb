{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Questions: Siamese Network\n",
    "- Siamese Network\n",
    "- Triplet Loss\n",
    "- Accuracy Evaluation\n",
    "- Cosine Similarity between Model's Vector\n",
    "- Data Generators for Question Batches\n",
    "- Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankar/dev/tools/anaconda3/envs/trax/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "from trax.fastmath import numpy as fastnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "# Set random seeds\n",
    "trax.supervised.trainer_lib.init_random_number_generators(34)\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question pairs: 404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"questions.csv\")\n",
    "N = len(data)\n",
    "print(f'Number of question pairs: {N}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 10240\n"
     ]
    }
   ],
   "source": [
    "N_train = 300000\n",
    "N_test = 10 * 1024\n",
    "data_train = data[:N_train]\n",
    "data_test = data[N_train:N_train+N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
    "del(data) # remove to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate questions:  111486\n",
      "indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
     ]
    }
   ],
   "source": [
    "td_index = (data_train['is_duplicate'] == 1).to_numpy()\n",
    "td_index = [i for i, x in enumerate(td_index) if x]\n",
    "\n",
    "print('number of duplicate questions: ', len(td_index))\n",
    "print('indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][5])  #  Example of question duplicates (first one in data)\n",
    "print(data_train['question2'][5])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some tips on making it through the job interview process at Medicines?\n",
      "What are some tips on making it through the job interview process at Foundation Medicine?\n",
      "is_duplicate:  0\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][25])  #  Example of question duplicates (first one in data)\n",
    "print(data_train['question2'][25])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does China block sanctions at the UN against the Jaish-e-Mohammad (JeM) chief, Masood Azhar?\n",
      "Why does China support Masood Azhar?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][125])  #  Example of question duplicates (first one in data)\n",
    "print(data_train['question2'][125])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train_words = np.array(data_train['question1'][td_index])\n",
    "Q2_train_words = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test_words = np.array(data_test['question1'])\n",
    "Q2_test_words = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international masterâ€™s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How do I prepare for interviews for cse?\n",
      "Question 2:  What is the best way to prepare for cse? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train_words[0])\n",
    "print('Question 2: ', Q2_train_words[0], '\\n')\n",
    "print('Question 1: ', Q1_train_words[5])\n",
    "print('Question 2: ', Q2_train_words[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test_words[0])\n",
    "print('Question 2: ', Q2_test_words[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encode each word of the selected duplicate pairs with an index\n",
    "- It is encoded as list of numbers\n",
    "- Tokenize the question using `nltk.word.tokenize`\n",
    "- Use a dictionary, during inference, assing 0 to alll out of vocab OOV words\n",
    "- Encode each word of the selected duplicate pairs with an index\n",
    "- Given a question, you can encode it as a list of numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Arrays\n",
    "Q1_train = np.empty_like(Q1_train_words)\n",
    "Q2_train = np.empty_like(Q2_train_words)\n",
    "\n",
    "Q1_test = np.empty_like(Q1_test_words)\n",
    "Q2_test = np.empty_like(Q2_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111486,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "Q1_train\n",
    "print(Q1_train.shape)\n",
    "print(type(Q1_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {'<PAD>': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building vocabulary with the train set\n",
    "from collections import defaultdict\n",
    "\n",
    "vocab = defaultdict(lambda: 0)\n",
    "vocab['<PAD>'] = 1\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 36306\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(Q1_train_words)):\n",
    "    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n",
    "    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n",
    "    \n",
    "    q = Q1_train[idx] + Q2_train[idx]\n",
    "    for word in q:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "print(f'The length of the vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocab['<PAD>'])\n",
    "print(vocab['Astrology'])\n",
    "print(vocab['Astronomy'])  #not i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(Q1_test_words)): \n",
    "    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n",
    "    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has reduced to:  111486\n",
      "Test set length:  10240\n"
     ]
    }
   ],
   "source": [
    "print('Train set has reduced to: ', len(Q1_train) ) \n",
    "print('Test set length: ', len(Q1_test) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to Tensors: CONVERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Q1_train)):\n",
    "    Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
    "    Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
    "\n",
    "        \n",
    "for i in range(len(Q1_test)):\n",
    "    Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
    "    Q2_test[i] = [vocab[word] for word in Q2_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "[32, 38, 4, 107, 65, 1015, 65, 11514, 21]\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(Q1_train_words[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(Q1_train[0],'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(Q1_test_words[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(Q1_test[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "The length of the training set is:   89188\n",
      "The length of the validation set is:  22298\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "cut_off = int(len(Q1_train)*.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Iterator\n",
    "- In AI/NLP, we uses batches of data while training\n",
    "- SGD with one example, will train for ever\n",
    "- Build a generator takes Q1 and Q2 and returns batches in the format $([q1_1, q1_2, q1_3, ...]$, $[q2_1, q2_2,q2_3, ...])$\n",
    "- The tuple consists of two arrays and each array has `batch_size` questions\n",
    "- $q1_i$ and $q2_i$ are duplicates, but they are not duplicates with any other elements in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Q1: List of transformed questions(tensors)\n",
    "    Q2: List of transformed questions(tensors)\n",
    "    batch_size: Num elems per batch\n",
    "    pad(int): Pad char\n",
    "    shuffle(bool): If the batchs should be randomized or not. \n",
    "    \n",
    "    Yields:\n",
    "    tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
    "    NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates\n",
    "              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates\n",
    "    \"\"\"\n",
    "    \n",
    "    input1 = []\n",
    "    input2 = []\n",
    "    \n",
    "    idx = 0\n",
    "    len_q = len(Q1)\n",
    "    question_indexes = [*range(len_q)]\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(question_indexes)\n",
    "        \n",
    "    while(True):\n",
    "        if(idx >= len_q):\n",
    "            idx = 0\n",
    "            \n",
    "        if(shuffle):\n",
    "            rnd.shuffle(question_indexes)\n",
    "            \n",
    "        # Get questions at the `question_indexes[idx]` position in Q1 and Q2\n",
    "        q1 = Q1[question_indexes[idx]]\n",
    "        q2 = Q2[question_indexes[idx]]\n",
    "        \n",
    "        idx += 1\n",
    "        input1.append(q1)\n",
    "        input2.append(q2)\n",
    "        \n",
    "        if(len(input1) == batch_size):\n",
    "            max_1 = max([len(input1[i]) for i in range(batch_size)])\n",
    "            max_2 = max([len(input2[i]) for i in range(batch_size)])\n",
    "            max_len = max(max_1, max_2)\n",
    "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
    "            \n",
    "            b1 = []\n",
    "            b2 = []\n",
    "            for q1, q2 in zip(input1, input2):\n",
    "                q1 = q1 + ((max_len - len(q1)) * [pad])\n",
    "                q2 = q2 + ((max_len - len(q2)) * [pad])\n",
    "                \n",
    "                b1.append(q1)\n",
    "                # append q2\n",
    "                b2.append(q2)\n",
    "                \n",
    "            yield np.array(b1), np.array(b2)\n",
    "            \n",
    "            input1, input2 = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First questions  :  \n",
      " [[   30    87   116   516   517   990    39   389 14767    21     1     1\n",
      "      1     1     1     1]\n",
      " [   30    87    78   216   685    65  1715   208    21     1     1     1\n",
      "      1     1     1     1]] \n",
      "\n",
      "Second questions :  \n",
      " [[ 1611    53 22783     6   516   612    17   254   990    39   619   389\n",
      "   4306   622    21     1]\n",
      " [   30    87   116    35   685    65  1715   208    21     1     1     1\n",
      "      1     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\n",
    "print(\"First questions  : \",'\\n', res1, '\\n')\n",
    "print(\"Second questions : \",'\\n', res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Siamese Network\n",
    "\n",
    "- Get the question embedding, run it through an LSTM layer, \n",
    "- Normalize $v_1$ and $v_2$, and \n",
    "- Finally use a triplet loss (explained below) to get the corresponding cosine similarity for each pair of questions. \n",
    "\n",
    "As usual, you will start by importing the data set. \n",
    "- The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. \n",
    "- The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. \n",
    "\n",
    "In math equations, you are trying to maximize the following.\n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right)$$\n",
    "\n",
    "$A$ is the anchor input, for example $q1_1$, $P$ the duplicate input, for example, $q2_1$, and $N$ the negative input (the non duplicate question), for example $q2_2$.<br>\n",
    "$\\alpha$ is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Siamese(vocab_size=len(vocab), d_model=128, mode='train'):\n",
    "    \n",
    "    def normalize(x):\n",
    "        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n",
    "    \n",
    "    q_processor = tl.Serial(\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=d_model),\n",
    "        tl.LSTM(d_model),\n",
    "        tl.Mean(axis=1),\n",
    "        tl.Fn('Normalize', lambda x: normalize(x))\n",
    "    )\n",
    "    \n",
    "    model = tl.Parallel(q_processor, q_processor)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel_in2_out2[\n",
      "  Serial[\n",
      "    Embedding_41748_128\n",
      "    LSTM_128\n",
      "    Mean\n",
      "    Normalize\n",
      "  ]\n",
      "  Serial[\n",
      "    Embedding_41748_128\n",
      "    LSTM_128\n",
      "    Mean\n",
      "    Normalize\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = Siamese()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Negative Mining\n",
    "\n",
    "You will now implement the `TripletLoss`.<br>\n",
    "As explained in the lecture, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*. Our loss expression is then:\n",
    " \n",
    "\\begin{align}\n",
    " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
    " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
    "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    # Pairwise cosine similarity\n",
    "    scores = fastnp.dot(v1, v2.T)\n",
    "    # Calculate new batch_size\n",
    "    batch_size = len(scores)\n",
    "    # The positive ones in the diagonal\n",
    "    positive = fastnp.diag(scores)\n",
    "    # multiply `fnp.eye(batch_size) with 2.0 and sub it out of `scores`\n",
    "    negative_without_positive = scores - fastnp.multiply(fastnp.eye(batch_size), 2.0)\n",
    "    # Take row by row max of nwp\n",
    "    closest_negative = negative_without_positive.max(axis=1)\n",
    "    # Sub fnp.eye(batch_size) out of 1.0 and do element wise mul woth scores\n",
    "    negative_zero_on_duplicate = fastnp.multiply((1.0 - fastnp.eye(batch_size) ), scores)\n",
    "    # use fnp.sum on neg_0_on_dup for axis=1 and divide it by batch_size - 1\n",
    "    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
    "    # compute fnp.max among 0.0 and A\n",
    "    # A = sub positive from margin and add clossest negative\n",
    "    #print(margin.shape)\n",
    "    triplet_loss1 = fastnp.maximum(0.0,  margin - positive + closest_negative)\n",
    "    # Compute fnp.max among 0.0 and B\n",
    "    # B = sub positive from margin and add mean_negative\n",
    "    triplet_loss2 = fastnp.maximum(0.0, margin - positive + mean_negative)\n",
    "    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
    "    \n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "TripletLossFn(v2,v1)\n",
    "print(\"Triplet Loss:\", TripletLossFn(v2,v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLoss(margin=0.25):\n",
    "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
    "    return tl.Fn('TripletLoss', triplet_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "- Train a model \n",
    "- Define a cost function\n",
    "- An optimizer\n",
    "- Feed data to the model\n",
    "- Build a dataset using data generator\n",
    "- Lambda function acts as a seed to remember the last batch that was given\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Q1.shape  (89188,)\n",
      "val_Q1.shape    (22298,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab['<PAD>'])\n",
    "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab['<PAD>'])\n",
    "print('train_Q1.shape ', train_Q1.shape)\n",
    "print('val_Q1.shape   ', val_Q1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
    "\n",
    "def train_model(\n",
    "    Siamese, TripletLoss, lr_schedule, \n",
    "    train_generator=train_generator, \n",
    "    val_generator=val_generator, output_dir='model/'\n",
    "):\n",
    "    output_dir = os.path.expanduser(output_dir)\n",
    "    \n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator,\n",
    "        loss_layer=TripletLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "    \n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator,\n",
    "        metrics=[TripletLoss()]\n",
    "    )\n",
    "    \n",
    "    training_loop = training.Loop(\n",
    "        Siamese(),\n",
    "        train_task,\n",
    "        eval_task=eval_task,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step      1: train TripletLoss |  0.49900973\n",
      "Step      1: eval  TripletLoss |  0.49906957\n"
     ]
    }
   ],
   "source": [
    "train_steps = 5\n",
    "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
    "training_loop.run(train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the saved model\n",
    "model = Siamese()\n",
    "model.init_from_file('model.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(\n",
    "    test_Q1, test_Q2, y, threshold, \n",
    "    model, vocab, data_generator=data_generator, \n",
    "    batch_size=64\n",
    "):\n",
    "    \n",
    "    accuracy = 0\n",
    "    for i in range(0, len(test_Q1), batch_size):\n",
    "        # Call the data generator with shuffle=False using next()\n",
    "        # Use batch_size chunks of questions as Q1 and Q2 arguments of the data generator\n",
    "        q1, q2 = next(data_generator(\n",
    "            test_Q1[i:i+batch_size], test_Q2[i:i+batch_size], \n",
    "            batch_size, pad=vocab['<PAD>'], shuffle=False\n",
    "        ))\n",
    "        y_test = y[i:i+batch_size]\n",
    "        v1, v2 = model([q1, q2])\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            # Take dot product to compute cos similarity of each pair\n",
    "            # of entries v1[j], v2[j]\n",
    "            d = fastnp.dot(v1[j], v2[j].T)\n",
    "            # id d greate than the threshold\n",
    "            res = d > threshold\n",
    "            if(y_test[j] == res):\n",
    "                accuracy += 1\n",
    "    accuracy = accuracy / y.shape[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7044921875\n"
     ]
    }
   ],
   "source": [
    "# this takes around 1 minute\n",
    "accuracy = classify(Q1_test,Q2_test, y_test, 0.7, model, vocab, batch_size = 512) \n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    question1, question2, threshold, \n",
    "    model, vocab, data_generator=data_generator, \n",
    "    verbose=False\n",
    "):\n",
    "    q1 = nltk.word_tokenize(question1)\n",
    "    q2 = nltk.word_tokenize(question2)\n",
    "    Q1, Q2 = [], []\n",
    "    for word in q1:\n",
    "        Q1.append(vocab[word])\n",
    "    for word in q2:\n",
    "        Q2.append(vocab[word])\n",
    "        \n",
    "    Q1, Q2 = next(data_generator(\n",
    "        [Q1], [Q2], batch_size, pad=vocab['<PAD>'], shuffle=False\n",
    "    ))\n",
    "    v1, v2 = model([Q1, Q2])\n",
    "    d = fastnp.dot(v1, v2.T)\n",
    "    res = (d > threshold)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Q1  = \", Q1, \"\\nQ2  = \", Q2)\n",
    "        print(\"d   = \", d)\n",
    "        print(\"res = \", res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1  =  [[585  76   4 ...  21   1   1]\n",
      " [585  76   4 ...  21   1   1]\n",
      " [585  76   4 ...  21   1   1]\n",
      " ...\n",
      " [585  76   4 ...  21   1   1]\n",
      " [585  76   4 ...  21   1   1]\n",
      " [585  76   4 ...  21   1   1]] \n",
      "Q2  =  [[ 585   33    4 ... 7282   21    1]\n",
      " [ 585   33    4 ... 7282   21    1]\n",
      " [ 585   33    4 ... 7282   21    1]\n",
      " ...\n",
      " [ 585   33    4 ... 7282   21    1]\n",
      " [ 585   33    4 ... 7282   21    1]\n",
      " [ 585   33    4 ... 7282   21    1]]\n",
      "d   =  [[0.8561271 0.8561271 0.8561271 ... 0.8561271 0.8561271 0.8561271]\n",
      " [0.8561271 0.8561271 0.8561271 ... 0.8561271 0.8561271 0.8561271]\n",
      " [0.8561271 0.8561271 0.8561271 ... 0.8561271 0.8561271 0.8561271]\n",
      " ...\n",
      " [0.8561271 0.8561271 0.8561271 ... 0.8561271 0.8561271 0.8561271]\n",
      " [0.8561271 0.8561271 0.8561271 ... 0.8561271 0.8561271 0.8561271]\n",
      " [0.8561271 0.8561271 0.8561271 ... 0.8561271 0.8561271 0.8561271]]\n",
      "res =  [[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ True,  True,  True, ...,  True,  True,  True],\n",
       "             [ True,  True,  True, ...,  True,  True,  True],\n",
       "             [ True,  True,  True, ...,  True,  True,  True],\n",
       "             ...,\n",
       "             [ True,  True,  True, ...,  True,  True,  True],\n",
       "             [ True,  True,  True, ...,  True,  True,  True],\n",
       "             [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, vocab, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'they', 'enjoy', 'eating', 'the', 'dessert', '?'] ['Do', 'they', 'like', 'hiking', 'in', 'the', 'desert', '?']\n",
      "Q1  =  [[  443  1145  3158 ... 29039    21     1]\n",
      " [  443  1145  3158 ... 29039    21     1]\n",
      " [  443  1145  3158 ... 29039    21     1]\n",
      " ...\n",
      " [  443  1145  3158 ... 29039    21     1]\n",
      " [  443  1145  3158 ... 29039    21     1]\n",
      " [  443  1145  3158 ... 29039    21     1]] \n",
      "Q2  =  [[ 443 1145   60 ...   78 7433   21]\n",
      " [ 443 1145   60 ...   78 7433   21]\n",
      " [ 443 1145   60 ...   78 7433   21]\n",
      " ...\n",
      " [ 443 1145   60 ...   78 7433   21]\n",
      " [ 443 1145   60 ...   78 7433   21]\n",
      " [ 443 1145   60 ...   78 7433   21]]\n",
      "d   =  [[0.542211 0.542211 0.542211 ... 0.542211 0.542211 0.542211]\n",
      " [0.542211 0.542211 0.542211 ... 0.542211 0.542211 0.542211]\n",
      " [0.542211 0.542211 0.542211 ... 0.542211 0.542211 0.542211]\n",
      " ...\n",
      " [0.542211 0.542211 0.542211 ... 0.542211 0.542211 0.542211]\n",
      " [0.542211 0.542211 0.542211 ... 0.542211 0.542211 0.542211]\n",
      " [0.542211 0.542211 0.542211 ... 0.542211 0.542211 0.542211]]\n",
      "res =  [[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[False, False, False, ..., False, False, False],\n",
       "             [False, False, False, ..., False, False, False],\n",
       "             [False, False, False, ..., False, False, False],\n",
       "             ...,\n",
       "             [False, False, False, ..., False, False, False],\n",
       "             [False, False, False, ..., False, False, False],\n",
       "             [False, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to try with your own questions\n",
    "question1 = \"Do they enjoy eating the dessert?\"\n",
    "question2 = \"Do they like hiking in the desert?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, vocab, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
