{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNNs, GRUs and the `scan` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import random\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward method for Vanilla RNNs and GRUs\n",
    "- Embedding Size (`emb`): 128\n",
    "- Hidden state size(`h_dim`): (16, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "emb = 128\n",
    "T = 256\n",
    "h_dim = 16\n",
    "h_0 = np.zeros((h_dim, 1))\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "w1 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w2 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w3 = random.standard_normal((h_dim, emb+h_dim))\n",
    "\n",
    "b1 = random.standard_normal((h_dim, 1))\n",
    "b2 = random.standard_normal((h_dim, 1))\n",
    "b3 = random.standard_normal((h_dim, 1))\n",
    "\n",
    "X = random.standard_normal((T, emb, 1))\n",
    "weights = [w1, w2, w3, b1, b2, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN\n",
    "Structure of Vanilla RNN\n",
    "<img src=\"RNN.PNG\" width=\"400\"/>\n",
    "\\begin{equation}\n",
    "h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)\n",
    "\\label{eq: htRNN}\n",
    "\\end{equation}\n",
    "    \n",
    "\\begin{equation}\n",
    "\\hat{y}^{<t>}=g(W_{yh}h^{<t>} + b_y)\n",
    "\\label{eq: ytRNN}\n",
    "\\end{equation}\n",
    "\n",
    "where $[h^{<t-1>},x^{<t>}]$ means that $h^{<t-1>}$ and $x^{<t>}$ are concatenated together. In the next cell we provide the implementation of the forward method for a vanilla RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_V_RNN(inputs, weights):\n",
    "    x, h_t = inputs\n",
    "    \n",
    "    # Weights\n",
    "    wh, _, _, bh, _, _ = weights\n",
    "    \n",
    "    # New hidden state\n",
    "    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n",
    "    h_t = sigmoid(h_t)\n",
    "    \n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}^{<t>}$ is omitted for simplicity\n",
    "\n",
    "## Forward method for GRUs\n",
    "<img src=\"GRU.PNG\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRUs have 2 more gates\n",
    "    - Relevance Gate $\\Gamma_r$\n",
    "    - Update Gate $\\Gamma_u$\n",
    "- They controll how the hidden state $h^{<t>}$ is updated on every time step\n",
    "- With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences\n",
    "- The equations needed for the forward methods in GRUs are here\n",
    "\n",
    "$$\\Gamma_r = \\sigma\\left(W_r[h^{<t-1>}, x^{<t>}] + b_r\\right)$$\n",
    "$$\\Gamma_u = \\sigma\\left(W_u[h^{<t-1>}, x^{<t>}] + b_u\\right)$$\n",
    "$$c^{<t>} = \\tanh\\left(W_h[\\Gamma_r * h^{<t-1>}, x^{<t>}] + b_h\\right)$$\n",
    "$$h^{<t>} = \\Gamma_u * c^{<t>} + \\left(1 - \\Gamma_u \\right) * h^{<t-1>} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_GRU(inputs, weights):\n",
    "    x, h_t = inputs\n",
    "    wu, wr, wc, bu, br, bc = weights\n",
    "    \n",
    "    z = np.dot(wr, np.concatenate([h_t, x])) + br\n",
    "    r = sigmoid(z)\n",
    "    \n",
    "    z = np.dot(wu, np.concatenate([h_t, x])) + bu\n",
    "    u = sigmoid(z)\n",
    "    \n",
    "    z = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n",
    "    c = np.tanh(z)\n",
    "    \n",
    "    h_t = u * c + (1 - u) * h_t\n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.77779014e-01],\n",
       "       [-9.97986240e-01],\n",
       "       [-5.19958083e-01],\n",
       "       [-9.99999886e-01],\n",
       "       [-9.99707004e-01],\n",
       "       [-3.02197037e-04],\n",
       "       [-9.58733503e-01],\n",
       "       [ 2.10804828e-02],\n",
       "       [ 9.77365398e-05],\n",
       "       [ 9.99833090e-01],\n",
       "       [ 1.63200940e-08],\n",
       "       [ 8.51874303e-01],\n",
       "       [ 5.21399924e-02],\n",
       "       [ 2.15495959e-02],\n",
       "       [ 9.99878828e-01],\n",
       "       [ 9.77165472e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_GRU([X[1],h_0], weights)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation of RNNs\n",
    "def scan(fn, elems, weights, h_0=None): \n",
    "    h_t = h_0\n",
    "    ys = []\n",
    "    for x in elems:\n",
    "        y, h_t = fn([x, h_t], weights)\n",
    "        ys.append(y)\n",
    "        \n",
    "    return ys, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs vs GRUs\n",
    "You have already seen how forward propagation is computed for vanilla RNNs and GRUs. As a quick recap, you need to have a forward method for the recurrent cell and a function like `scan` to go through all the elements from a sequence using a forward method. You saw that GRUs performed more computations than vanilla RNNs, and you can check that they have 3 times more parameters. In the next two cells, we compute forward propagation for a sequence with 256 time steps (`T`) for an RNN and a GRU with the same hidden state `h_t` size (`h_dim`=16).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 4.54ms to run the forward method for the vanilla RNN\n"
     ]
    }
   ],
   "source": [
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_V_RNN, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "RNN_time = (toc - tic) * 1000\n",
    "print(f'It took {RNN_time:.2f}ms to run the forward method for the vanilla RNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9.00ms to run the forward method for the vanilla GRU\n"
     ]
    }
   ],
   "source": [
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_GRU, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "GRU_time = (toc - tic) * 1000\n",
    "print(f'It took {GRU_time:.2f}ms to run the forward method for the vanilla GRU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
