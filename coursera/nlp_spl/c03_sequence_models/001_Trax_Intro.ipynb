{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import shapes\n",
    "from trax import fastmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trax                          1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "- Layers are the core building blocks in Trax\n",
    "- They take inputs, compute functions/custom calculations and return outputs\n",
    "- One can interospect layer properties\n",
    "\n",
    "### ReLU Lyer\n",
    "- No object initializaton, it works just like a math function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Properties--\n",
      "Name: Concatenate\n",
      "Expected Inputs: 2\n",
      "Promised Ouputs: 1\n",
      "\n",
      "x1: [-10 -20 -30], x2: [1. 2. 3.]\n",
      "\n",
      "Outputs\n",
      "y: [-10. -20. -30.   1.   2.   3.]\n"
     ]
    }
   ],
   "source": [
    "concat = tl.Concatenate()\n",
    "print(\"--Properties--\")\n",
    "print(f\"Name: {concat.name}\")\n",
    "print(f'Expected Inputs: {concat.n_in}')\n",
    "print(f'Promised Ouputs: {concat.n_out}\\n')\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "print(f'x1: {x1}, x2: {x2}\\n')\n",
    "\n",
    "# Outputs\n",
    "y = concat([x1, x2])\n",
    "print(\"Outputs\")\n",
    "print(f'y: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10. -20. -30.   1.   2.   3.]\n"
     ]
    }
   ],
   "source": [
    "# Create a concatenate trax layer\n",
    "concat = tl.Concatenate()\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat.name)\n",
    "print(\"expected inputs :\", concat.n_in)\n",
    "print(\"promised outputs :\", concat.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat([x1, x2])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers are Configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 3\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.]\n",
      "x3 : [0.99 1.98 2.97] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n"
     ]
    }
   ],
   "source": [
    "concat_3 = tl.Concatenate(n_items=3)\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat_3.name)\n",
    "print(\"expected inputs :\", concat_3.n_in)\n",
    "print(\"promised outputs :\", concat_3.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "x3 = x2 * 0.99\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2)\n",
    "print(\"x3 :\", x3, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat_3([x1, x2, x3])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tl.Concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers can have Weights\n",
    "- Some layer types include mutable weights and biases that are used in computation and training\n",
    "- These type of layers require initialization\n",
    "- For e.g. `LayerNorm` layer calculates normalized data, that is caled by weights adn biases\n",
    "- During initialization, pass the data shape and data type of the inputs - to initialize compatible arrays of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function signature in module trax.shapes:\n",
      "\n",
      "signature(obj)\n",
      "    Returns a `ShapeDtype` signature for the given `obj`.\n",
      "    \n",
      "    A signature is either a `ShapeDtype` instance or a tuple of `ShapeDtype`\n",
      "    instances. Note that this function is permissive with respect to its inputs\n",
      "    (accepts lists or tuples or dicts, and underlying objects can be any type\n",
      "    as long as they have shape and dtype attributes) and returns the corresponding\n",
      "    nested structure of `ShapeDtype`.\n",
      "    \n",
      "    Args:\n",
      "      obj: An object that has `shape` and `dtype` attributes, or a list/tuple/dict\n",
      "          of such objects.\n",
      "    \n",
      "    Returns:\n",
      "      A corresponding nested structure of `ShapeDtype` instances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(tl.LayerNorm)\n",
    "help(shapes.signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Shape: (4,), Data Type: <class 'tuple'>\n",
      "Shapes Trax: ShapeDtype{shape:(4,), dtype:float64}, Data type: <class 'trax.shapes.ShapeDtype'>\n",
      "-- Properties --\n",
      "name: LayerNorm\n",
      "Expected Inputs: 1\n",
      "Promised Outputs: 1\n",
      "weights : [1. 1. 1. 1.]\n",
      "biases : [0. 0. 0. 0.] \n",
      "\n",
      "-- Inputs --\n",
      "x : [0. 1. 2. 3.]\n",
      "-- Outputs --\n",
      "y : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankar/dev/tools/anaconda3/envs/trax/lib/python3.8/site-packages/jax/lax/lax.py:5905: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n",
      "/Users/shankar/dev/tools/anaconda3/envs/trax/lib/python3.8/site-packages/jax/lax/lax.py:5905: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
     ]
    }
   ],
   "source": [
    "# Layer Initialization\n",
    "norm = tl.LayerNorm()\n",
    "# Input data\n",
    "x = np.array([0, 1, 2, 3], dtype=\"float\")\n",
    "\n",
    "# Use the input data signature to get shape and tyep for initializing weights and biases\n",
    "norm.init(shapes.signature(x)) # Need to convert the input datatype from usual tuple to trax ShapeDType\n",
    "\n",
    "print(f'Normal Shape: {x.shape}, Data Type: {type(x.shape)}')\n",
    "print(f'Shapes Trax: {shapes.signature(x)}, Data type: {type(shapes.signature(x))}')\n",
    "\n",
    "# Inspect Properties\n",
    "print(\"-- Properties --\")\n",
    "print(f'name: {norm.name}')\n",
    "print(f'Expected Inputs: {norm.n_in}')\n",
    "print(f'Promised Outputs: {norm.n_out}')\n",
    "# Weights and biases\n",
    "print(\"weights :\", norm.weights[0])\n",
    "print(\"biases :\", norm.weights[1], \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x)\n",
    "\n",
    "# Outputs\n",
    "y = norm(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "- Create custom layers too and define custom functions for computations by using `tl.Fn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Fn in module trax.layers.base:\n",
      "\n",
      "Fn(name, f, n_out=1)\n",
      "    Returns a layer with no weights that applies the function `f`.\n",
      "    \n",
      "    `f` can take and return any number of arguments, and takes only positional\n",
      "    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).\n",
      "    The following, for example, would create a layer that takes two inputs and\n",
      "    returns two outputs -- element-wise sums and maxima:\n",
      "    \n",
      "        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`\n",
      "    \n",
      "    The layer's number of inputs (`n_in`) is automatically set to number of\n",
      "    positional arguments in `f`, but you must explicitly set the number of\n",
      "    outputs (`n_out`) whenever it's not the default value 1.\n",
      "    \n",
      "    Args:\n",
      "      name: Class-like name for the resulting layer; for use in debugging.\n",
      "      f: Pure function from input tensors to output tensors, where each input\n",
      "          tensor is a separate positional arg, e.g., `f(x0, x1) --> x0 + x1`.\n",
      "          Output tensors must be packaged as specified in the `Layer` class\n",
      "          docstring.\n",
      "      n_out: Number of outputs promised by the layer; default value 1.\n",
      "    \n",
      "    Returns:\n",
      "      Layer executing the function `f`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tl.Fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : TimesTwo\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [1 2 3] \n",
      "\n",
      "-- Outputs --\n",
      "y : [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Define Custom Layer\n",
    "# In this example you will create a layer to calculate the input times 2\n",
    "\n",
    "def TimesTwo():\n",
    "    layer_name = \"TimesTwo\"\n",
    "    \n",
    "    # Custom function for the custom layer\n",
    "    def func(x):\n",
    "        return x * 2\n",
    "    \n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "# Test it\n",
    "times_two = TimesTwo()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", times_two.name)\n",
    "print(\"expected inputs :\", times_two.n_in)\n",
    "print(\"promised outputs :\", times_two.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = times_two(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinators\n",
    "- Combine layers to build more complex layers\n",
    "- Trax provides a set of objects named combinator layers to make this happen\n",
    "- Combinators are themselves layers, so behavior commutes\n",
    "\n",
    "**Serial Combinators**.  \n",
    "- Simple NN can be built by combining layers into a single layer using the `Serial` combinator\n",
    "- This new layer then acts just like a single layer\n",
    "- One can inspect inputs, outputs and weights\n",
    "- Or combine it into another layer\n",
    "- Combinators can then be used as trainable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tl.Serial)\n",
    "# help(tl.Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankar/dev/tools/anaconda3/envs/trax/lib/python3.8/site-packages/jax/lax/lax.py:5905: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n",
      "/Users/shankar/dev/tools/anaconda3/envs/trax/lib/python3.8/site-packages/jax/lax/lax.py:5905: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial[\n",
      "  LayerNorm\n",
      "  Relu\n",
      "  TimesTwo\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "sublayers : [LayerNorm, Relu, TimesTwo]\n",
      "expected inputs : 1\n",
      "promised outputs : 1\n",
      "weights & biases: [(DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), (), ()] \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0.        0.        0.        1.4142132 2.8284264]\n"
     ]
    }
   ],
   "source": [
    "# Serial Cominator\n",
    "serial = tl.Serial(\n",
    "    tl.LayerNorm(),\n",
    "    tl.Relu(),\n",
    "    times_two\n",
    ")\n",
    "# Initialization\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "serial.init(shapes.signature(x))\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial,\"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out)\n",
    "print(\"weights & biases:\", serial.weights, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good old numpy: <class 'numpy.ndarray'>\n",
      "JAX Trax Numpy: <class 'jax.interpreters.xla.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "x_numpy = np.array([1, 2, 3])\n",
    "print(f\"Good old numpy: {type(x_numpy)}\")\n",
    "\n",
    "# Fastmath and JAX numpy\n",
    "x_jax = fastmath.numpy.array([1, 2, 3])\n",
    "print(f'JAX Trax Numpy: {type(x_jax)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
