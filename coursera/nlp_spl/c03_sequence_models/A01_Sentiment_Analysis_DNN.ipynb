{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Neural Networks\n",
    "\n",
    "- Understand how to build/design a model using layers\n",
    "- Train a model using a training loop\n",
    "- Use a binary cross entropy loss function\n",
    "- Compute the accuracy of your model\n",
    "- Predict using your own input\n",
    "\n",
    "Note the following,  \n",
    "- Model Architecture\n",
    "- Inputs\n",
    "- Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankar/dev/tools/anaconda3/envs/trax/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/shankar/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shankar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random as rnd\n",
    "import trax\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "trax.supervised.trainer_lib.init_random_number_generators(31)\n",
    "\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "\n",
    "from utils import Layer, load_tweets, process_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax.interpreters.xla.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# Create an array using trax.fastmath.numpy\n",
    "a = np.array(5.0)\n",
    "display(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0 is 25.0\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "print(f\"f(a) for a={a} is {f(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient (Derivative)\n",
    "Gradient of a function is calculated using `trax.fastmath.grad(fun=)` and passing the name of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directly use trax.fastmath.grad to calculate the gradient (derivative) of the function\n",
    "grad_f = trax.fastmath.grad(fun=f)\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(10., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the newly created func and pass in a value for x\n",
    "grad_calculation = grad_f(a)\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 8000\n",
      "length of val_x 2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load positive and negative tweets\n",
    "all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "# View the total number of positive and negative tweets.\n",
    "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "# Split positive set into validation and training\n",
    "val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "# Split negative set into validation and training\n",
    "val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "\n",
    "# Combine training data into one set\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# Combine validation data into one set\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "# Set the labels for the training set (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tweet at training position 0\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Tweet at training position 0 after processing:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a function that processes the tweets\n",
    "# from utils import process_tweet\n",
    "\n",
    "# Try out function that processes tweets\n",
    "print(\"original tweet at training position 0\")\n",
    "print(train_pos[0])\n",
    "\n",
    "print(\"Tweet at training position 0 after processing:\")\n",
    "process_tweet(train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9092\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "# Unit Test Note - There is no test set here only train/val\n",
    "\n",
    "# Include special tokens \n",
    "# started with pad, end of line and unk tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "#display(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a Tweet to a Tensor\n",
    "Input a tweet:\n",
    "```CPP\n",
    "'@happypuppy, is Maria happy?'\n",
    "```\n",
    "\n",
    "The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)\n",
    "```CPP\n",
    "['maria', 'happi']\n",
    "```\n",
    "\n",
    "Then it will convert each word into its unique integer\n",
    "\n",
    "```CPP\n",
    "[2, 56]\n",
    "```\n",
    "- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the `__UNK__` token, because it is considered \"unknown.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    \n",
    "    # Process the tweet into a list of words\n",
    "    # where only important words are kept\n",
    "    word_l = process_tweet(tweet)\n",
    "    if(verbose):\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integers IDs of each word\n",
    "    tensor_l = []\n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")    \n",
    "        \n",
    "    for word in word_l:\n",
    "        # Get the unique integer ID\n",
    "        # If the word doesnt exist in the vocab dictionary\n",
    "        # use the unique ID for __UNK__ instead\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "        \n",
    "        tensor_l.append(word_ID)\n",
    "        \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1065, 136, 479, 2351, 745, 8146, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# test tweet_to_tensor\n",
    "\n",
    "def test_tweet_to_tensor():\n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": [val_pos[1], Vocab],\n",
    "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
    "            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"datatype_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":type([]),\n",
    "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"without_unk_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":6,\n",
    "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
    "        }\n",
    "    ]\n",
    "    count = 0\n",
    "    for test_case in test_cases:\n",
    "        \n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"datatype_check\":\n",
    "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"without_unk_check\":\n",
    "                assert None not in tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "    if count == 3:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(count,\" Tests passed out of 3\")\n",
    "test_tweet_to_tensor()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Batch Generator\n",
    "- Training single models will take a lot of time, so batches are preferred\n",
    "- A generator is built that takes in +ve/-ve tweets and returns a batch of training examples\n",
    "- It returns model inputs, the targets and the weights\n",
    "- ie Input vector, Labels(+ve, -ve) and weight of each target\n",
    "\n",
    "Single batch can be obtained using next operator  \n",
    "- The generator returns the data in a format that you could directly use in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        data_pos - Set of positive examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch, even\n",
    "        loop - True/False\n",
    "        vocab_dict\n",
    "        shuffle\n",
    "        \n",
    "    Yield:\n",
    "        inputs - Subset of +ve and -ve examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - An array specifying the importance of each example\n",
    "    \"\"\"\n",
    "    # Make sure the batch size is an even umber\n",
    "    # to allow an equal number of positive and negative sampels\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of +ve examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk trhought the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get an array with data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # Shuffle lines if shuffle is set to True\n",
    "    if(shuffle):\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:\n",
    "        # Create a batch with +ve and -ve examples\n",
    "        batch = []\n",
    "        #Pack n_to_take positive examples\n",
    "        # Start from pos_index and increment i upto n_to_take\n",
    "        \n",
    "        for i in range(n_to_take):\n",
    "            # If the positive index goes past the positive dataset lenght,\n",
    "            if pos_index >= len_data_pos: \n",
    "            # If the positive index goes past the positive dataset length\n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break\n",
    "                    # If user wants to keep reusing the data, reset the index\n",
    "                pos_index = 0\n",
    "                if(shuffle):\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "            \n",
    "            # Get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            # Convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            # Append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "            \n",
    "        for i in range(n_to_take):\n",
    "            if(neg_index >= len_data_neg):\n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break\n",
    "                neg_index = 0\n",
    "                if(shuffle):\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "                    \n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            batch.append(tensor)\n",
    "            neg_index = neg_index + 1\n",
    "            \n",
    "        if(stop):\n",
    "            break\n",
    "            \n",
    "        # Update the start index for +ve/-ve data\n",
    "        # So that its n_to_take positions after the current pos_index\n",
    "        pos_index += n_to_take\n",
    "        neg_index += n_to_take\n",
    "        \n",
    "        # Get the max tweet length (the length of the longest tweet)\n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch])\n",
    "        \n",
    "        # Initialize the input_l, which will\n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        for tensor in batch:\n",
    "            # Get the number of positions to pad for this tensor\n",
    "            # so taht it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            pad_l = [0] * n_pad\n",
    "            # concatenate the tensor\n",
    "            tensor_pad = tensor + pad_l\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "            \n",
    "        # Convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "        \n",
    "        # Generate the list of targets for the +Ve samples - a list of ones\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = [1] * n_to_take\n",
    "        target_neg = [0] * n_to_take\n",
    "        \n",
    "        target_l = target_pos + target_neg\n",
    "        targets = np.array(target_l)\n",
    "        \n",
    "        # Treat all examples equally important. IT \n",
    "        example_weigths = np.ones_like(targets)\n",
    "                \n",
    "        yield inputs, targets, example_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0]\n",
      " [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n",
      " [3761  109  136  583 2930 3969    0    0    0    0    0]\n",
      " [ 250 3761    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "def train_generator(batch_size, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs shape is (4, 14)\n",
      "The targets shape is (4,)\n",
      "The example weights shape is (4,)\n",
      "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
      "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
      "input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generator\n",
    "\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective targets)\n",
    "tmp_data_gen = train_generator(batch_size = 4)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "print(f\"The targets shape is {tmp_targets.shape}\")\n",
    "print(f\"The example weights shape is {tmp_example_weights.shape}\")\n",
    "\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def forward(self, x):\n",
    "        activation = np.maximum(0, x)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data is:\n",
      "[[-2. -1.  0.]\n",
      " [ 0.  1.  2.]]\n",
      "Output of Relu is:\n",
      "[[0. 0. 0.]\n",
      " [0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Test your relu function\n",
    "x = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)\n",
    "relu_layer = Relu()\n",
    "print(\"Test data is:\")\n",
    "print(x)\n",
    "print(\"Output of Relu is:\")\n",
    "print(relu_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Class\n",
    "#### Forward Function\n",
    "$$forward\\left(x, W\\right)=xW$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax import fastmath\n",
    "np = fastmath.numpy\n",
    "# use the fastmath.random module from trax\n",
    "random = fastmath.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed generated by random.get_prng\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1], dtype=uint32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose a matrix with 2 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix generated with a normal distribution with mean 0 and stdev of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.957307  , -0.9699291 ,  1.0070664 ],\n",
       "             [ 0.36619022,  0.17294823,  0.29092228]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See how the fastmath.trax.random.normal function works\n",
    "tmp_key = random.get_prng(seed=1)\n",
    "print(\"The random seed generated by random.get_prng\")\n",
    "display(tmp_key)\n",
    "\n",
    "print(\"choose a matrix with 2 rows and 3 columns\")\n",
    "tmp_shape=(2,3)\n",
    "display(tmp_shape)\n",
    "\n",
    "# Generate a weight matrix\n",
    "# Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow\n",
    "# Just avoid setting the dtype and allow it to use the default data type\n",
    "tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)\n",
    "\n",
    "print(\"Weight matrix generated with a normal distribution with mean 0 and stdev of 1\")\n",
    "display(tmp_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method normal in module trax.fastmath.ops:\n",
      "\n",
      "normal(*args, **kwargs) method of trax.fastmath.ops.RandomBackend instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, n_units, init_stdev=0.1):\n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = init_stdev\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dense = np.dot(x, self.weights)\n",
    "        return dense\n",
    "        \n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        input_shape = input_signature.shape\n",
    "        w = random.normal(key=random_key, shape=(input_shape[-1], self._n_units))\n",
    "        self.weights = w * self._init_stdev\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are\n",
      "  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126\n",
      "  -0.04265672  0.0986188  -0.05575325  0.00153249]\n",
      " [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617\n",
      "  -0.03237354  0.16234995  0.02450038 -0.13809784]\n",
      " [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459\n",
      "  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]\n",
      "Foward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209\n",
      "  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing your Dense layer \n",
    "#sets  number of units in dense layer\n",
    "dense_layer = Dense(n_units=10)  \n",
    "# sets random seed\n",
    "random_key = random.get_prng(seed=0)  \n",
    "# input array \n",
    "z = np.array([[2.0, 7.0, 25.0]]) \n",
    "\n",
    "dense_layer.init(z, random_key)\n",
    "#Returns randomly generated weights\n",
    "print(\"Weights are\\n \",dense_layer.weights) \n",
    "# Returns multiplied values of units and weights\n",
    "print(\"Foward function output is \", dense_layer(z)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 0 creates a vector whose length equals the vocabulary size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2.5, 3.5, 4.5], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 5.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pretend the embedding matrix uses \n",
    "# 2 elements for embedding the meaning of a word\n",
    "# and has a vocabulary size of 3\n",
    "# So it has shape (2,3)\n",
    "tmp_embed = np.array([[1,2,3,],\n",
    "                    [4,5,6]\n",
    "                   ])\n",
    "\n",
    "# take the mean along axis 0\n",
    "print(\"The mean along axis 0 creates a vector whose length equals the vocabulary size\")\n",
    "display(np.mean(tmp_embed,axis=0))\n",
    "\n",
    "print(\"The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\")\n",
    "display(np.mean(tmp_embed,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
    "    embed_layer = tl.Embedding(\n",
    "        vocab_size=vocab_size,\n",
    "        d_feature=embedding_dim\n",
    "    )\n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    dense_output_layer = tl.Dense(n_units=2)\n",
    "    log_softmax_layer = tl.LogSoftmax()\n",
    "    \n",
    "    model = tl.Serial(\n",
    "        embed_layer,\n",
    "        mean_layer,\n",
    "        dense_output_layer,\n",
    "        log_softmax_layer\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trax.layers.combinators.Serial'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_9092_256\n",
       "  Mean\n",
       "  Dense_2\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(tmp_model))\n",
    "display(tmp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "To train a model on a task, Trax defines an abstraction [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) which packages the train data, loss and optimizer (among other things) together into an object.\n",
    "\n",
    "Similarly to evaluate a model, Trax defines an abstraction [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) which packages the eval data and metrics (among other things) into another object.\n",
    "\n",
    "The final piece tying things together is the [`trax.supervised.training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.\n",
    "Using `Loop` will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "batch_size = 16\n",
    "rnd.seed(271)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    n_steps_per_checkpoint=10\n",
    ")\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
    ")\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model\n"
     ]
    }
   ],
   "source": [
    "output_dir = './model'\n",
    "output_dir_expand = os.path.expanduser(output_dir)\n",
    "print(output_dir_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    training_loop = training.Loop(\n",
    "        classifier,\n",
    "        train_task,\n",
    "        eval_task=eval_task,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step      1: train CrossEntropyLoss |  0.60789275\n",
      "Step      1: eval  CrossEntropyLoss |  0.76009548\n",
      "Step      1: eval          Accuracy |  0.56250000\n",
      "Step     10: train CrossEntropyLoss |  0.64948821\n",
      "Step     10: eval  CrossEntropyLoss |  0.45854422\n",
      "Step     10: eval          Accuracy |  0.87500000\n",
      "Step     20: train CrossEntropyLoss |  0.40723333\n",
      "Step     20: eval  CrossEntropyLoss |  0.33322647\n",
      "Step     20: eval          Accuracy |  0.93750000\n",
      "Step     30: train CrossEntropyLoss |  0.25177222\n",
      "Step     30: eval  CrossEntropyLoss |  0.18621588\n",
      "Step     30: eval          Accuracy |  1.00000000\n",
      "Step     40: train CrossEntropyLoss |  0.24520011\n",
      "Step     40: eval  CrossEntropyLoss |  0.44792810\n",
      "Step     40: eval          Accuracy |  0.62500000\n",
      "Step     50: train CrossEntropyLoss |  0.17519739\n",
      "Step     50: eval  CrossEntropyLoss |  0.15775831\n",
      "Step     50: eval          Accuracy |  0.93750000\n",
      "Step     60: train CrossEntropyLoss |  0.11804956\n",
      "Step     60: eval  CrossEntropyLoss |  0.11095993\n",
      "Step     60: eval          Accuracy |  1.00000000\n",
      "Step     70: train CrossEntropyLoss |  0.08826172\n",
      "Step     70: eval  CrossEntropyLoss |  0.15991844\n",
      "Step     70: eval          Accuracy |  0.93750000\n",
      "Step     80: train CrossEntropyLoss |  0.08097079\n",
      "Step     80: eval  CrossEntropyLoss |  0.02399438\n",
      "Step     80: eval          Accuracy |  1.00000000\n",
      "Step     90: train CrossEntropyLoss |  0.07040589\n",
      "Step     90: eval  CrossEntropyLoss |  0.02183634\n",
      "Step     90: eval          Accuracy |  1.00000000\n",
      "Step    100: train CrossEntropyLoss |  0.04796972\n",
      "Step    100: eval  CrossEntropyLoss |  0.00146092\n",
      "Step    100: eval          Accuracy |  1.00000000\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
      "The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)\n",
      "The shape of the labels is (16,), which is the batch size.\n",
      "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
     ]
    }
   ],
   "source": [
    "tmp_train_generator = train_generator(16)\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs - tweets as tensors\n",
    "# Position 1 has the targets - the actual labels\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
      "Column 0 is the probability of a negative sentiment (class 0)\n",
      "Column 1 is the probability of a positive sentiment (class 1)\n",
      "\n",
      "View the prediction array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-6.5867472e+00, -1.3794899e-03],\n",
       "             [-4.9628477e+00, -7.0173740e-03],\n",
       "             [-5.6064124e+00, -3.6809444e-03],\n",
       "             [-5.3729963e+00, -4.6510696e-03],\n",
       "             [-4.6580648e+00, -9.5300674e-03],\n",
       "             [-5.2760839e+00, -5.1255226e-03],\n",
       "             [-4.9576702e+00, -7.0540905e-03],\n",
       "             [-3.9602642e+00, -1.9242048e-02],\n",
       "             [-6.3717365e-03, -5.0590839e+00],\n",
       "             [-3.7033916e-02, -3.3143821e+00],\n",
       "             [-1.1131763e-02, -4.5035138e+00],\n",
       "             [-4.4822693e-05, -1.0009674e+01],\n",
       "             [-7.7550411e-03, -4.8632913e+00],\n",
       "             [-4.0259361e-03, -5.5169826e+00],\n",
       "             [-1.7308950e-02, -4.0651746e+00],\n",
       "             [-9.0484619e-03, -4.7096877e+00]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg log prob -6.5867\tPos log prob -0.0014\t is positive?True\t actual 1\n",
      "Neg log prob -4.9628\tPos log prob -0.0070\t is positive?True\t actual 1\n",
      "Neg log prob -5.6064\tPos log prob -0.0037\t is positive?True\t actual 1\n",
      "Neg log prob -5.3730\tPos log prob -0.0047\t is positive?True\t actual 1\n",
      "Neg log prob -4.6581\tPos log prob -0.0095\t is positive?True\t actual 1\n",
      "Neg log prob -5.2761\tPos log prob -0.0051\t is positive?True\t actual 1\n",
      "Neg log prob -4.9577\tPos log prob -0.0071\t is positive?True\t actual 1\n",
      "Neg log prob -3.9603\tPos log prob -0.0192\t is positive?True\t actual 1\n",
      "Neg log prob -0.0064\tPos log prob -5.0591\t is positive?False\t actual 0\n",
      "Neg log prob -0.0370\tPos log prob -3.3144\t is positive?False\t actual 0\n",
      "Neg log prob -0.0111\tPos log prob -4.5035\t is positive?False\t actual 0\n",
      "Neg log prob -0.0000\tPos log prob -10.0097\t is positive?False\t actual 0\n",
      "Neg log prob -0.0078\tPos log prob -4.8633\t is positive?False\t actual 0\n",
      "Neg log prob -0.0040\tPos log prob -5.5170\t is positive?False\t actual 0\n",
      "Neg log prob -0.0173\tPos log prob -4.0652\t is positive?False\t actual 0\n",
      "Neg log prob -0.0090\tPos log prob -4.7097\t is positive?False\t actual 0\n"
     ]
    }
   ],
   "source": [
    "# Turn probabilities into category predictions\n",
    "tmp_is_positive = tmp_pred[:, 1] > tmp_pred[:, 0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i, 0]:.4f}\\tPos log prob {tmp_pred[i, 1]:.4f}\\t is positive?{p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of Booleans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "             False, False, False, False, False, False, False, False],            dtype=bool)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of Integers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of floats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "             0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the array of booleans\n",
    "print(\"Array of Booleans\")\n",
    "display(tmp_is_positive)\n",
    "\n",
    "# Convert boolean to type int32\n",
    "# True is converted to 1\n",
    "# False is converted to 0\n",
    "tmp_is_positive_int = tmp_is_positive.astype(np.int32)\n",
    "\n",
    "# View the array of integers\n",
    "print(\"Array of Integers\")\n",
    "display(tmp_is_positive_int)\n",
    "\n",
    "# Convert boolean to type float32\n",
    "tmp_is_positive_float = tmp_is_positive.astype(np.float32)\n",
    "\n",
    "# View the array of floats\n",
    "print(\"Array of floats\")\n",
    "display(tmp_is_positive_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True == 1: True\n",
      "True == 2: False\n",
      "False == 0: True\n",
      "False == 2: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"True == 1: {True == 1}\")\n",
    "print(f\"True == 2: {True == 2}\")\n",
    "print(f\"False == 0: {False == 0}\")\n",
    "print(f\"False == 2: {False == 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Computing the Accuracy on a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, y, y_weights):\n",
    "    # Create an array of booleans\n",
    "    # True if the probability of positive sentiment is greater than\n",
    "    # teh probability of negative sentiment\n",
    "    # else False\n",
    "    is_pos = preds[:, 1] > preds[:, 0]\n",
    "    is_pos_int = is_pos.astype(np.int32)\n",
    "    correct = is_pos_int != y\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    correct_float = correct.astype(np.float32)\n",
    "    \n",
    "    # Multiply each predictions with its corresponding weight\n",
    "    weigthed_correct_float = np.multiply(y_weights, correct_float)\n",
    "    \n",
    "    # Sum up the weighted correct predictions to go in the denominator\n",
    "    weighted_num_correct = np.sum(weigthed_correct_float)\n",
    "    \n",
    "    # Divide the number of weighted correct predictions by the sum of the weights\n",
    "    accuracy = weighted_num_correct / np.sum(y_weights)\n",
    "    \n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's prediction accuracy on a single training batch is: 1.5625%\n",
      "Weighted number of correct predictions 1.0; weighted number of total observations predicted 64\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "tmp_val_generator = val_generator(64)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_val_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "\n",
    "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
    "\n",
    "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
    "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 4, 10, 18], dtype=int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "np.multiply(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "             1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(generator, model):\n",
    "    accuracy = 0\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "    \n",
    "    for batch in generator:\n",
    "        # Retrieve the inputs from the batch\n",
    "        inputs = batch[0]\n",
    "        targets = batch[1]\n",
    "        example_weight = batch[2]\n",
    "        \n",
    "        pred = model(inputs)\n",
    "        \n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)\n",
    "        \n",
    "        total_num_correct = batch_num_correct\n",
    "        total_num_pred += batch_num_pred\n",
    "        \n",
    "    accuracy = total_num_correct / total_num_pred\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model on the validation set is 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-e5c9403477cf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-e5c9403477cf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    model.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
