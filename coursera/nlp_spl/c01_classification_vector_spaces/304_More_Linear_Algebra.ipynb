{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some More Linear Algebra\n",
    "There is one area in Linear Algebra covers almost all fascinating topics of matrix manipulation is Principal Component Analysis(PCA). They are\n",
    "- Matrix Multiplication\n",
    "- Matrix Transposition\n",
    "- Matrix Inverses\n",
    "- Matrix Decomposition\n",
    "- Eigen Values\n",
    "- Eigen Vectors\n",
    "- Eigen Spaces, so and so forth\n",
    "\n",
    "Furth it also touch bases on statistics with\n",
    "- Standard Deviations\n",
    "- Variance\n",
    "- Covariance\n",
    "- Independence \n",
    "- Linear Regression\n",
    "- Feature Selection\n",
    "\n",
    "With the clarity of above list, It is evident PCA is critical for solving Machine Learning Problems. Precisely, when you have too much of feature... how to reduce the feature list so that unwanted/unnecessary features are eliminated in the quest of training a good model\n",
    "\n",
    "- The process of reducing the features from a large feature set is called ass dimensionality reduction.\n",
    "- Imagine each feature is a dimension\n",
    "- For e.g. A point is the hyperplane on a 1 dimensional space\n",
    "- Similarly, A line is the hyperplane on a 2 dimensional space and\n",
    "- A 2D plane is the hyperplane on a 3 dimensional space\n",
    "\n",
    "This kind of gives an idea why we call a feature a dimension... There are two ways dimensionality reduction can be achieved\n",
    "- Feature Elimination, e.g. Correlation Analysis\n",
    "- Feature Extraction, e.g. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA\n",
    "From Wiki, Given a collection of points in 2, 3 or higher dimensional space, a best fitting line can be defined as on that minimizes the average squared distance from a point to the line. \n",
    "The next best fitting line can be similarly chosen from directions perpendicular to the first. Repeated this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. \n",
    "These basis vectors are called as principal components, and the related procedures are principal component analysis.\n",
    "\n",
    "\n",
    "## How PCA is done?\n",
    "- Through Singular Value Decomposition (SVD) or \n",
    "- Performing Eigen Value Decomposition over the Covariance Matrix of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Decomposition Salience\n",
    "- Computing Eigen Vectors and Values is the foundation of PCA\n",
    "- Eigen Vectors are the Principal Components that deternines the direction of the new dimension(feature space)\n",
    "- Eigen Values determine their magnitude, it explains the variance of the data along the new feature axes\n",
    "\n",
    "### Covariance Matrix\n",
    "- Eigen decomposistion on the covariance matrix(a square matrix) is the classical PCA\n",
    "\n",
    "$$\\large \\mathbb{A} = \\sigma_{jk} = \\frac {1} {n - 1} \\sum_{i=0}^n \\left( x_{ij} - \\overline{x}_j \\right) \\left( x_{ik} - \\overline{x}_k \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where,\n",
    "- $j$ and $k$ are the 2 features or the dimensions for which the covariance is calculated\n",
    "- $i$ is the feature element and $n$ is the size of the feature vectors.\n",
    "- $\\overline{x}_j$ and $\\overline{x}_k$ are the mean of $j$ and $k$ vectors respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Decomposition\n",
    "\n",
    "This we calculated the covariance of the feature vectors, Now Eigen Decomposition\n",
    "$$\\large \\mathbb{A}\\normalsize \\mathbb{v} = \\lambda \\normalsize \\mathbb{v}$$\n",
    "where \n",
    "- $\\mathbb{A}$ $\\epsilon$ $\\mathbb{R}^{mxm}$ - Square Matrix, Covariance Matrix  \n",
    "- $\\mathbb{v}$ $\\epsilon$ $\\mathbb{R}^{mx1}$ - Column Vector, Eigen Vectors  \n",
    "- $\\lambda$ $\\epsilon$ $\\mathbb{R}^{mxm}$ - Diagonal Matrix, Eigen Values  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then,\n",
    "$$\\large \\mathbb{A}\\normalsize \\mathbb{v} - \\lambda \\normalsize \\mathbb{v} = 0$$\n",
    "$$i.e.$$\n",
    "$$\\mathbb{v} \\left(\\large \\mathbb{A} - \\lambda \\mathbb{I} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be true only when the below is true\n",
    "$$det \\left(\\large \\mathbb{A} - \\lambda \\mathbb{I} \\right) = 0$$\n",
    "$$i.e.$$\n",
    "$$\\vert \\large \\mathbb{A} - \\lambda \\mathbb{I} \\vert = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the Principal Component\n",
    "We got \n",
    "- $\\mathbb{v}$ $\\epsilon$ $\\mathbb{R}^{mx1}$ - Column Vector, Eigen Vectors  \n",
    "- $\\lambda$ $\\epsilon$ $\\mathbb{R}^{mxm}$ - Diagonal Matrix, Eigen Values \n",
    "\n",
    "Let us say, we want to reduce the dimensionality to 3 then the top 3 eigen vectors are the Principal Components of our interest. ie.\n",
    "$$\\LARGE \\mathbb{v} \\epsilon \\mathbb{R}^{3x1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
