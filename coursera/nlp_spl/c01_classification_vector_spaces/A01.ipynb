{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11346\n"
     ]
    }
   ],
   "source": [
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "print(f\"type(freqs) = {str(type(freqs))}\")\n",
    "print(f\"len(freqs) = {str(len(freqs.keys()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976294"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When the model predicts close to 1\n",
    "# but the actual label is 0\n",
    "# the loss is a large positive value\n",
    "y_hat = 0.9999 # Close to 1\n",
    "y = 0\n",
    "-1 * ((y * np.log(y_hat)) + ((1 - y)*np.log(1 - y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976182"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model predicts close to zero\n",
    "# but the actual label is 1\n",
    "# Again the loss is large positive\n",
    "y_hat = 0.0001\n",
    "y = 1\n",
    "-1 * ((y * np.log(y_hat)) + ((1 - y)*np.log(1 - y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010000500033334732"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = 0.0001\n",
    "y = 0\n",
    "-1 * ((y * np.log(y_hat)) + ((1 - y)*np.log(1 - y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010000500033334732"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = 0.9999\n",
    "y = 1\n",
    "-1 * ((y * np.log(y_hat)) + ((1 - y)*np.log(1 - y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions: Implement gradient descent function\n",
    "* The number of iterations `num_iters` is the number of times that you'll use the entire training set.\n",
    "* For each iteration, you'll calculate the cost function using all training examples (there are `m` training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    m = len(x)\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "        J = (-1/float(m)) * (np.dot(np.transpose(y), np.log(h)) + np.dot(np.transpose(1 - y), np.log(1 - h)))\n",
    "        \n",
    "        theta = theta - ((alpha )/float(m)) * np.dot(np.transpose(x), h - y)\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.67094970.\n",
      "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "# Construct a synthetic test case using numpy PRNG functions\n",
    "np.random.seed(1)\n",
    "# X input is 10 x 3 with ones for the bias terms\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y Labels are 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Extracting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    word_1 = process_tweet(tweet)\n",
    "    \n",
    "    x = np.zeros((1, 3))\n",
    "    x[0, 0] = 1\n",
    "    \n",
    "    for word in word_1:\n",
    "        x[0, 1] += freqs[(word, 1.0)] if (word, 1.0) in freqs else 0\n",
    "        x[0, 2] += freqs[(word, 0.0)] if (word, 0.0) in freqs else 0\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Check your function\n",
    "\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.24216529.\n",
      "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "    \n",
    "Y = train_y\n",
    "\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    x = extract_features(tweet, freqs)\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81636424]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \n",
    "    y_hat = []\n",
    "    for tweet in test_x:\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if(y_pred > 0.5):\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            y_hat.append(0.0)\n",
    "            \n",
    "    accuracy = np.sum(y_hat == np.squeeze(test_y))/float(len(test_x))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\n",
      "THE PROCESSED TWEET IS: ['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n",
      "1\t0.49996890\tb'truli later move know queen bee upward bound movingonup'\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48622857\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370665\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370665\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370665\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
      "1\t0.49578765\tb'park get sunlight'\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48199810\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t0.50020353\tb'u prob fun david'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50039294\tb'pat jay'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000002\tb'belov grandmoth'\n"
     ]
    }
   ],
   "source": [
    "print('Label Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "    \n",
    "    if(np.abs(y -( y_hat > 0.5)) > 0):\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
      "[[0.48139087]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fafadc09b50>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
