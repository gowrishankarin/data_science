{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Coding tutorials\n",
    " #### [1. Keras datasets](#coding_tutorial_1)\n",
    " #### [2. Dataset generators](#coding_tutorial_2)\n",
    " #### [3. Keras image data augmentation](#coding_tutorial_3)\n",
    " #### [4. The Dataset class](#coding_tutorial_4)\n",
    " #### [5. Training with Datasets](#coding_tutorial_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## Keras datasets\n",
    "\n",
    "For a list of Keras datasets and documentation on recommended usage, see [this link](https://keras.io/datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CIFAR-100 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR-100 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data(label_mode=\"fine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that reloading the dataset does not require a download\n",
    "\n",
    "train_images.shape, test_images.shape, train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the shape of the data.\n",
    "\n",
    "plt.imshow(train_images[500])\n",
    "print(train_labels[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine one of the images and its corresponding label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of labels from a JSON file\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/cifar100_fine_labels.json', 'r') as fine_labels:\n",
    "    cifar100_fine_labels = json.load(fine_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of labels for the CIFAR-100 dataset are available [here](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few of the labels\n",
    "\n",
    "cifar100_fine_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the corresponding label for the example above\n",
    "\n",
    "cifar100_fine_labels[41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data using different label modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples from category 87 (index 86) and the list of labels\n",
    "\n",
    "examples = train_images[(train_labels.T == 86)[0]][:3]\n",
    "fig, ax = plt.subplots(1,3)\n",
    "ax[0].imshow(examples[0])\n",
    "ax[1].imshow(examples[1])\n",
    "ax[2].imshow(examples[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data using the 'coarse' label mode\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data(label_mode=\"coarse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display three images from the dataset with the label 6 (index 5)\n",
    "\n",
    "examples = train_images[(train_labels.T == 5)[0]][:3]\n",
    "fig, ax = plt.subplots(1,3)\n",
    "ax[0].imshow(examples[0])\n",
    "ax[1].imshow(examples[1])\n",
    "ax[2].imshow(examples[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of coarse labels from a JSON file\n",
    "\n",
    "with open('data/cifar100_coarse_labels.json', 'r') as coarse_labels:\n",
    "    cifar100_coarse_labels = json.load(coarse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a few of the labels\n",
    "cifar100_coarse_labels[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the corresponding label for the example above\n",
    "\n",
    "print(cifar100_coarse_labels[5])\n",
    "print(cifar100_fine_labels[86])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example from the training dataset, along with its corresponding label\n",
    "\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lengths of the input sequences\n",
    "\n",
    "sequence_lengths = [len(seq) for seq in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum and minimum sequence length\n",
    "\n",
    "np.max(sequence_lengths), np.min(sequence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Keyword Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data ignoring the 50 most frequent words, use oov_char=2 (this is the default)\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(skip_top=50, oov_char=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lengths of the input sequences\n",
    "\n",
    "sequence_lengths = [len(seq) for seq in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum and minimum sequence length\n",
    "\n",
    "np.max(sequence_lengths), np.min(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for filtering the sequences\n",
    "\n",
    "def remove_oov_char(element):\n",
    "    ''' Filter function for removing the oov_char. '''\n",
    "    return [word for word in element if word!=2]\n",
    "\n",
    "def filter_list(lst):\n",
    "    ''' Run remove_oov_char on elements in a list. '''\n",
    "    return [remove_oov_char(element) for element in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the oov_char from the sequences using the filter_list function\n",
    "\n",
    "train_data = filter_list(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lengths of the input sequences\n",
    "\n",
    "sequence_lengths = [len(seq) for seq in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum and minimum sequence length\n",
    "\n",
    "np.max(sequence_lengths), np.min(sequence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Dataset generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the UCI Fertility Dataset\n",
    "\n",
    "We will be using a dataset available at https://archive.ics.uci.edu/ml/datasets/Fertility from UC Irvine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fertility dataset\n",
    "\n",
    "headers = ['Season', 'Age', 'Diseases', 'Trauma', 'Surgery', 'Fever', 'Alcohol', 'Smoking', 'Sitting', 'Output']\n",
    "fertility = pd.read_csv('data/fertility_diagnosis.txt', delimiter=',', header=None, names=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame\n",
    "\n",
    "fertility.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "fertility.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the 'Output' feature from 'N' to 0 and from 'O' to 1\n",
    "\n",
    "fertility['Output'] = fertility['Output'].map(lambda x : 0.0 if x=='N' else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "fertility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame so that the features are mapped to floats\n",
    "\n",
    "fertility = fertility.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "\n",
    "fertility = fertility.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "fertility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the field Season to a one-hot encoded vector\n",
    "\n",
    "fertility = pd.get_dummies(fertility, prefix='Season', columns=['Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "fertility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the Output column such that it is the last column in the DataFrame\n",
    "\n",
    "fertility.columns = [col for col in fertility.columns if col != 'Output'] + ['Output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "fertility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a numpy array.\n",
    "\n",
    "fertility = fertility.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation set\n",
    "\n",
    "training = fertility[0:70]\n",
    "validation = fertility[70:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shape of the training data\n",
    "\n",
    "training.shape, validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and labels for the validation and training data\n",
    "\n",
    "training_features = training[:,0:-1]\n",
    "training_labels = training[:,-1]\n",
    "validation_features = validation[:,0:-1]\n",
    "validation_labels = validation[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns a generator producing inputs and labels\n",
    "\n",
    "def get_generator(features, labels, batch_size=1):\n",
    "    for n in range(int(len(features)/batch_size)):\n",
    "        yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to our training features and labels with a batch size of 10\n",
    "\n",
    "train_generator = get_generator(training_features, training_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator using the next() function\n",
    "\n",
    "next(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using Keras with 3 layers\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
    "\n",
    "input_shape = (12,)\n",
    "output_shape = (1,)\n",
    "\n",
    "model_input = Input(input_shape)\n",
    "batch_1 = BatchNormalization(momentum=0.8)(model_input)\n",
    "dense_1 = Dense(100, activation='relu')(batch_1)\n",
    "batch_2 = BatchNormalization(momentum=0.8)(dense_1)\n",
    "output = Dense(1, activation='sigmoid')(batch_2)\n",
    "\n",
    "model = Model([model_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary to show the resultant structure\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer object\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with loss function and metric\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the model using the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of training steps per epoch for the given batch size.\n",
    "\n",
    "batch_size = 5\n",
    "train_steps = len(training) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the epochs to 3\n",
    "\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_generator = get_generator(training_features, training_labels, batch_size=batch_size)\n",
    "    validation_generator = get_generator(validation_features, validation_labels, batch_size=30)\n",
    "    \n",
    "    model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=train_steps, \n",
    "        validation_data=validation_generator, \n",
    "        validation_steps=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to run the fit_generator function once more; observe what happens\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make an infinitely looping generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns an infinitely looping generator\n",
    "def get_generator_cyclic(features, labels, batch_size=1):\n",
    "    while True:\n",
    "        for n in range(int(len(features)/batch_size)):\n",
    "            yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])\n",
    "            \n",
    "        permuted = np.random.permutation(len(features))\n",
    "        features = features[permuted]\n",
    "        labels = labels[permuted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator using this function.\n",
    "\n",
    "train_generator_cyclic = get_generator_cyclic(training_features, training_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the new cyclic generator does not raise a StopIteration\n",
    "\n",
    "for i in range(2*train_steps):\n",
    "    next(train_generator_cyclic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a cyclic validation generator\n",
    "\n",
    "validation_generator_cyclic = get_generator_cyclic(validation_features, validation_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator_cyclic, \n",
    "    steps_per_epoch=train_steps, \n",
    "    validation_data=validation_generator_cyclic,\n",
    "    validation_steps=1,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's obtain a validation data generator.\n",
    "\n",
    "validation_generator = get_generator(validation_features, validation_labels, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the validation data\n",
    "\n",
    "predictions = model.predict(validation_generator, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the corresponding validation labels\n",
    "\n",
    "print(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a validation data generator\n",
    "\n",
    "validation_generator = get_generator(validation_features, validation_labels, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "print(model.evaluate(validation_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## Keras image data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "\n",
    "(training_features, training_labels), (test_features, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to a one-hot encoding\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "training_labels = tf.keras.utils.to_categorical(training_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns a data generator\n",
    "\n",
    "def get_generator(features, labels, batch_size=1):\n",
    "    for n in range(int(len(features)/batch_size)):\n",
    "        yield (features[n*batch_size:(n+1)*batch_size], labels[n*batch_size:(n+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function we created to get a training data generator with a batch size of 1\n",
    "\n",
    "training_generator = get_generator(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the shape of the items generated by training_generator using the `next` function to yield an item.\n",
    "\n",
    "image, label = next(training_generator)\n",
    "print(image.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the training generator by obtaining an image using the `next` generator function, and then using imshow to plot it.\n",
    "# Print the corresponding label\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "image, label = next(training_generator)\n",
    "image_unbatched = image[0,:,:,:]\n",
    "imshow(image_unbatched)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the generator by re-running the `get_generator` function.\n",
    "\n",
    "training_generator = get_generator(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a data augmention generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to convert an image to monochrome\n",
    "\n",
    "def monochrome(x):\n",
    "    def func_bw(a):\n",
    "        average_colour = np.mean(a)\n",
    "        return [average_colour, average_colour, average_colour]\n",
    "    x = np.apply_along_axis(func_bw, -1, x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator object\n",
    "\n",
    "image_generator = ImageDataGenerator(\n",
    "    preprocessing_function=monochrome,\n",
    "    rotation_range=180,\n",
    "    rescale=(1/255.0)\n",
    ")\n",
    "image_generator.fit(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [the documentation](https://keras.io/preprocessing/image/) for the full list of image data augmentation options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterable generator using the `flow` function\n",
    "\n",
    "image_generator_iterable = image_generator.flow(\n",
    "    training_features, training_labels, batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample from the generator and compare with the original\n",
    "\n",
    "image, label = next(image_generator_iterable)\n",
    "image_orig, label_orig = next(train_generator)\n",
    "figs, axes = plt.subplots(1,2)\n",
    "axes[0].imshow(image[0,:,:,:])\n",
    "axes[0].set_title('Transformed')\n",
    "axes[1].imshow(image_orig[0,:,:,:])\n",
    "axes[1].set_title('Original')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flow from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/flowers-recognition-split/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the directory structure\n",
    "\n",
    "train_path = 'data/flowers-recognition-split/train'\n",
    "val_path = 'data/flowers-recognition-split/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator object\n",
    "\n",
    "datagenerator = ImageDataGenerator(rescale=(1/255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training data generator\n",
    "train_generator = datagenerator.flow_from_directory(\n",
    "    train_path, \n",
    "    batch_size=64,\n",
    "    classes=classes,\n",
    "    target_size=(16, 16)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation data generator\n",
    "val_generator = datagenerator.flow_from_directory(\n",
    "    val_path, \n",
    "    batch_size=64,\n",
    "    classes=classes,\n",
    "    target_size=(16, 16)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display an image and label from the training generator\n",
    "\n",
    "x = next(train_generator)\n",
    "imshow(x[0][4])\n",
    "print(x[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the training generator\n",
    "\n",
    "train_generator = datagenerator.flow_from_directory(\n",
    "    train_path,\n",
    "    batch_size=64,\n",
    "    classes=classes,\n",
    "    target_size=(16, 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Input((16,16,3)))\n",
    "model.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((4,4)))\n",
    "model.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(4, (4, 4), padding='same', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer object\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training generator and test generator steps per epoch\n",
    "\n",
    "train_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "val_steps = val_generator.n // val_generator.batch_size\n",
    "print(train_steps_per_epoch, val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model.evaluate_generator(\n",
    "    val_generator,\n",
    "    steps=val_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels with the model\n",
    "\n",
    "predictions = model.predict(\n",
    "    val_generator,\n",
    "    steps=1\n",
    ")\n",
    "print(np.round(predictions, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## The Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((100,10,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the tensor x\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "dataset1.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [np.zeros((10,2,2)), np.zeros((5,2,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try creating a dataset from the tensor x2\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [np.zeros((10,1)), np.zeros((10,1)), np.zeros((10,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataset from the new x2 and inspect the Dataset object\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "print(dataset2.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a zipped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two datasets into one larger dataset\n",
    "dataset_zipped = tf.data.Dataset.zip((dataset1, dataset2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "print(dataset_zipped.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the number of batches in a dataset\n",
    "\n",
    "def get_batches(dataset):\n",
    "    iter_dataset = iter(dataset)\n",
    "    i = 0\n",
    "    try:\n",
    "        while next(iter_dataset):\n",
    "            i = i+1\n",
    "    except:\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of batches in the zipped Dataset\n",
    "\n",
    "get_batches(dataset_zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset from numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(type(train_features), type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset from the MNIST data\n",
    "\n",
    "mnist_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_features, train_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "print(mnist_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the length of an element using the take method\n",
    "\n",
    "element = next(iter(mnist_dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the shapes of the data\n",
    "\n",
    "print(element[0].shape)\n",
    "print(element[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of text files\n",
    "\n",
    "text_files = sorted([f.path for f in os.scandir('data/shakespeare')])\n",
    "\n",
    "print(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first file using python and print the first 5 lines.\n",
    "\n",
    "with open(text_files[0], 'r') as fil:\n",
    "    contents = [fil.readline() for i in range(5)]\n",
    "    for line in contents:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lines from the files into a dataset using TextLineDataset\n",
    "shakespeare_dataset = tf.data.TextLineDataset(text_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the take method to get and print the first 5 lines of the dataset\n",
    "\n",
    "first_5_lines_dataset = iter(shakespeare_dataset.take(5))\n",
    "lines = [line for line in first_5_lines_dataset]\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of lines in the first file\n",
    "\n",
    "lines = []\n",
    "with open(text_files[0], 'r') as fil:\n",
    "    line = fil.readline()\n",
    "    while line:\n",
    "        lines.append(line)\n",
    "        line = fil.readline()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of lines in the shakespeare dataset we created\n",
    "\n",
    "shakespeare_dataset_iterator = iter(shakespeare_dataset)\n",
    "lines = [line for line in shakespeare_dataset_iterator]\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interleave lines from the text data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataset of the text file strings\n",
    "\n",
    "text_files_dataset = tf.data.Dataset.from_tensor_slices(text_files)\n",
    "files = [file for file in text_files_dataset]\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleave the lines from the text files\n",
    "\n",
    "interleaved_shakespeare_dataset = text_files_dataset.interleave(\n",
    "    tf.data.TextLineDataset, cycle_length=9\n",
    ")\n",
    "interleaved_shakespeare_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 elements of the interleaved dataset\n",
    "\n",
    "lines = [line for line in iter(interleaved_shakespeare_dataset.take(10))]\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## Training with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the UCI Bank Marketing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "\n",
    "bank_dataframe = pd.read_csv('data/bank/bank-full.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "bank_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame\n",
    "\n",
    "print(bank_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features from the DataFrame\n",
    "\n",
    "features = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "            'loan', 'contact', 'campaign', 'pdays', 'poutcome']\n",
    "labels = ['y']\n",
    "\n",
    "bank_dataframe = bank_dataframe.filter(features + labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "bank_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical features in the DataFrame to one-hot encodings\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "categorical_features = ['default', 'housing', 'job', 'loan', 'education', 'contact', 'poutcome']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    bank_dataframe[feature] = tuple(encoder.fit_transform(bank_dataframe[feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "bank_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "\n",
    "bank_dataframe = bank_dataframe.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a Dataset\n",
    "\n",
    "bank_dataset = tf.data.Dataset.from_tensor_slices(dict(bank_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check that there are records in the dataset for non-married individuals\n",
    "\n",
    "def check_divorced():\n",
    "    bank_dataset_iterable = iter(bank_dataset)\n",
    "    for x in bank_dataset_iterable:\n",
    "        print(x['marital'])\n",
    "        if x['marital'] != 'divorced':\n",
    "            print('Found a person with marital status: {}'.format(x['marital']))\n",
    "            return\n",
    "    print('No non-divorced people were found!')\n",
    "\n",
    "check_divorced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the Dataset to retain only entries with a 'divorced' marital status\n",
    "\n",
    "bank_dataset = bank_dataset.filter(lambda x : tf.equal(x['marital'], tf.constant([b'divorced']))[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the records in the dataset again\n",
    "\n",
    "check_divorced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map a function over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the label ('y') to an integer instead of 'yes' or 'no'\n",
    "\n",
    "def map_label(x):\n",
    "    x['y'] = 0 if (x['y'] == tf.constant([b'no'], dtype=tf.string)) else 1\n",
    "    return x\n",
    "\n",
    "bank_dataset = bank_dataset.map(map_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'marital' column\n",
    "\n",
    "bank_dataset = bank_dataset.map(lambda x: {\n",
    "    key:val for key, val in x.items() if key != \"marital\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input and output data tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input and output tuple for the dataset\n",
    "\n",
    "def map_feature_label(x):\n",
    "    features = [[x['age']], [x['balance']], [x['campaign']], x['contact'], x['default'],\n",
    "                x['education'], x['housing'], x['job'], x['loan'], [x['pdays']], x['poutcome']]\n",
    "    return (tf.concat(features, axis=0), x['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map this function over the dataset\n",
    "\n",
    "bank_dataset = bank_dataset.map(map_feature_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into a training and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length of the Dataset\n",
    "\n",
    "dataset_length = 0\n",
    "for _ in bank_dataset:\n",
    "    dataset_length += 1\n",
    "print(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and validation sets from the dataset\n",
    "\n",
    "training_elements = int(dataset_length * 0.7)\n",
    "train_dataset = bank_dataset.take(training_elements)\n",
    "validation_dataset = bank_dataset.skip(training_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a classification model\n",
    "\n",
    "Now let's build a model to classify the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classifier model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, BatchNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(30,)))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batched training and validation datasets\n",
    "\n",
    "train_dataset = train_dataset.batch(20, drop_remainder=True)\n",
    "validation_dataset = validation_dataset.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training data\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "plt.plot(history.epoch, history.history['accuracy'], label='training')\n",
    "plt.plot(history.epoch, history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
