{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising Text Data\n",
    "In this notebook, you will learn how to tokenise text data using `tf.keras.preprocessing.text.Tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now downloaded and experimented with the IMDb dataset of labelled movie reviews. You will have noticed that the words have been mapped to integers. Converting a sequence of words to a sequence of numbers is called _tokenisation_. The numbers themselves are called _tokens_. Tokenisation is handy because it allows numerical operations to be applied to text data.\n",
    "\n",
    "The IMDb reviews were tokenised by mapping each word to a positive integer that indicated its frequency rank.  Tokenisation could also have been applied at the level of characters rather than words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The text dataset\n",
    "The text we will work with in this notebook is Three Men in a Boat by Jerome K. Jerome, a comical short story about the perils of going outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "with open('data/ThreeMenInABoat.txt', 'r', encoding='utf-8') as file:\n",
    "    text_string = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some simple preprocessing, replacing dashes with empty spaces\n",
    "\n",
    "text_string = text_string.replace('—', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View an excerpt of the data\n",
    "\n",
    "text_string[0:2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into sentences.\n",
    "\n",
    "sentence_strings = text_string.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of the dataset\n",
    "\n",
    "sentence_strings[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Tokenizer object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Tokenizer` object allows you to easily tokenise words or characters from a text document. It has several options to allow you to adjust the tokenisation process. Documentation is available for the `Tokenizer` [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any additional characters that we want to filter out (ignore) from the text\n",
    "\n",
    "additional_filters = '—’‘“”'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer has a `filters` keyword argument, that determines which characters will be filtered out from the text. The cell below shows the default characters that are filtered, to which we are adding our additional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tokenizer object\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + additional_filters,\n",
    "                      lower=True,\n",
    "                      split=' ',\n",
    "                      char_level=False,\n",
    "                      oov_token='<UNK>',\n",
    "                      document_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all, the `Tokenizer` has the following keyword arguments:\n",
    "\n",
    "`num_words`: int. the maximum number of words to keep, based on word frequency. Only the most common `num_words-1` words will be kept. If set to `None`, all words are kept.\n",
    "    \n",
    "`filters`: str. Each element is a character that will be filtered from the texts. Defaults to all punctuation (inc. tabs and line breaks), except `'`.\n",
    "\n",
    "`lower`: bool. Whether to convert the texts to lowercase. Defaults to `True`.\n",
    "\n",
    "`split`: str. Separator for word splitting. Defaults to `' '`.\n",
    "    \n",
    "`char_level`: bool. if True, every character will be treated as a token. Defaults to `False`.\n",
    "\n",
    "`oov_token`: if given, it will be added to word_index and used to replace out-of-vocabulary words during sequence_to_text calls. Defaults to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Tokenizer to the text\n",
    "We can now tokenize our text using the `fit_on_texts` method. This method takes a list of strings to tokenize, as we have prepared with `sentence_strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Tokenizer vocabulary\n",
    "\n",
    "tokenizer.fit_on_texts(sentence_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_on_texts` method could also take a list of lists of strings, and in this case it would recognise each element of each sublist as an individual token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Tokenizer configuration\n",
    "Now that the Tokenizer has ingested the data, we can see what it has extracted from the text by viewing its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokenizer config as a python dict\n",
    "\n",
    "tokenizer_config = tokenizer.get_config()\n",
    "tokenizer_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the word_counts entry\n",
    "\n",
    "tokenizer_config['word_counts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the number of times each word appears in the corpus. As you can see, the word counts dictionaries in the config are serialized into plain JSON. The `loads()` method in the Python library `json` can be used to convert this JSON string into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word_counts as a python dictionary\n",
    "\n",
    "import json\n",
    "\n",
    "word_counts = json.loads(tokenizer_config['word_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index is derived from the `word_counts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the word_index entry\n",
    "\n",
    "tokenizer_config['word_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word_index and index_word as python dictionaries\n",
    "\n",
    "index_word = json.loads(tokenizer_config['index_word'])\n",
    "word_index = json.loads(tokenizer_config['word_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the sentences to tokens\n",
    "You can map each sentence to a sequence of integer tokens using the Tokenizer's `texts_to_sequences()` method. As was the case for the IMDb data set, the number corresponding to a word is that word's frequency rank in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 sentences\n",
    "\n",
    "sentence_strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "\n",
    "sentence_seq = tokenizer.texts_to_sequences(sentence_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The return type is a list\n",
    "\n",
    "type(sentence_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 tokenized sentences\n",
    "\n",
    "sentence_seq[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the mappings in the config\n",
    "\n",
    "print(word_index['chapter'], word_index['i'])\n",
    "print(word_index['three'], word_index['invalids'])\n",
    "print(word_index['sufferings'], word_index['of'], word_index['george'], word_index['and'], word_index['harris'])\n",
    "print(word_index['a'], word_index['victim'], word_index['to'], word_index['one'], word_index['hundred'], word_index['and'], word_index['seven'], word_index['fatal'], word_index['maladies'])\n",
    "print(word_index['useful'], word_index['prescriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the tokens to sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can map the tokens back to sentences using the Tokenizer's `sequences_to_texts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 tokenized sentences\n",
    "\n",
    "sentence_seq[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the token sequences back to sentences\n",
    "\n",
    "tokenizer.sequences_to_texts(sentence_seq)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the mappings in the config\n",
    "\n",
    "print(index_word['362'], index_word['8'])\n",
    "print(index_word['126'], index_word['3362'])\n",
    "print(index_word['2319'], index_word['6'], index_word['36'], index_word['3'], index_word['35'])\n",
    "print(index_word['5'], index_word['1779'], index_word['4'], index_word['43'], index_word['363'], index_word['3'], index_word['468'], index_word['3363'], index_word['2320'])\n",
    "print(index_word['2321'], index_word['3364'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any valid sequence of tokens can be converted to text\n",
    "\n",
    "tokenizer.sequences_to_texts([[92, 104, 241], [152, 169, 53, 2491]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word is not featured in the Tokenizer's word index, then it will be mapped to the value of the Tokenizer's `oov_token` property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize unrecognised words\n",
    "\n",
    "tokenizer.texts_to_sequences(['i would like goobleydoobly hobbledyho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the OOV token\n",
    "\n",
    "index_word['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading and resources\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
