{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T07:45:48.410623Z",
     "iopub.status.busy": "2021-03-31T07:45:48.410375Z",
     "iopub.status.idle": "2021-03-31T07:45:48.416898Z",
     "shell.execute_reply": "2021-03-31T07:45:48.415681Z",
     "shell.execute_reply.started": "2021-03-31T07:45:48.410591Z"
    }
   },
   "source": [
    "# Variational Autoencoders\n",
    "*1*.  \n",
    "- Structure of a VAE\n",
    "- Latent variable Model\n",
    "- VAE is a latent variable model in which the encoder and doecoder are NNs  \n",
    "\n",
    "*2*.  \n",
    "- Evidence lower bound(ELBO)\n",
    "- ELBO objective function that is used to fit a VAE\n",
    "\n",
    "*3*.  \n",
    "- Reparameterization trick, to estimate the ELBO\n",
    "\n",
    "*4*.  \n",
    "- ELBO can be estimated more precisely if part of it is evaluated analytically.\n",
    "- Provides a recipe for fitting a VAE\n",
    "\n",
    "VAEs used for \n",
    "- Anamoly detection, data compression, image denoising ad reducing dim in prep for some algo or model\n",
    "- These applications varu in their use of a trained VAEs encoder and decoder\n",
    "\n",
    "Similarity of VAEs and AEs...\n",
    "- They both use NNs for tasks like compression and reconstruction\n",
    "- a term in the ELBO resembles the reconstruction error of an autoencoder \n",
    "\n",
    "Differences\n",
    "- A VAE is an unsupervised generative model, AEs are not\n",
    "- An AE is generally `self supervised`, a VAE on the other hand describes the variability in the observations \n",
    "- Can be used to synthesise observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variables and LV Model\n",
    "\n",
    "A latent variable is a random variable that cannot be conditioned on for inference because its value is not known.\n",
    "\n",
    "- They do not need to correspond to a real quantities\n",
    "- Models that outwardly do not involve latent quantities are more conveniently expressed by imaging that they do\n",
    "- e.g. Gaussian mixture models, Observations can be generated by sampling a label from categorical distribution, then drawing from the Gaussian in the mixture that has that label\n",
    "- A latent variable model underlies the VAE: some latent variable Z is assumed to have distribution $p_{\\theta}$\n",
    "- The observation X is assumed ot be distributed according to the condiational distribution $p_{\\theta}(x|z)$\n",
    "- X may be either continuous or discrete \n",
    "- Our objective is to obtain a maximum likelihood estimate for $\\theta$ denoted $\\theta_{ML}$\n",
    "- Once $\\theta_{ML}$ is available, then the distribution of the observable given the latent variable $p_{\\theta ML} (M|L)$ \n",
    "- Marginal likelihood of an observation $p_{\\theta ML}(x)$ \n",
    "\n",
    "This model could be fit by maximising the marginal likelihood,\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x) = \\int p_{\\theta}(x|z)p_{\\theta}(z)dz,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If this likelihoood or its gradient can be efficiently evaluated or approximated, then maximising it wrt $\\theta$ i straightforward\n",
    "- The marginal likelihood may be intractable while the posterior is known or can be efficiently maximized (Expectation Maximization Algo used)\n",
    "\n",
    "A simple approach to estimating $p_\\theta(x)$ is to take samples $z_i$ ($i\\in I$) from $p_\\theta(z)$, then take the average of their $p_\\theta(x|z_i)$ values. The problem with this method is that if $z$ is high-dimensional, then a very large sample is required to estimate $p_\\theta(x)$ well.\n",
    "\n",
    "Variational inferene provides an alternative approach to fitting the model\n",
    "- The high level idea is this... approximate p_theta(z|x) \n",
    "- then use this approximation to estimate lower bound on log p_theta(x) \n",
    "- theta can then be updated based on thow lower bound\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
