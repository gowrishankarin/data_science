{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"TFP version:\", tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports and setting fixed random seed to have reproducibility\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Tutorials\n",
    "#### 1. [Univariate Distributions](#univariate_distributions)\n",
    "#### 2. [Multivariate Distributions](#multivariate_distributions)\n",
    "#### 3. [The Independent Distribution](#the_independent_distribution)\n",
    "#### 4. [Sampling and log probs](#sampling_and_log_probs)\n",
    "#### 5. [Trainable Distributions](#trainable_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Univariate distributions\n",
    "<a id='univariate_distributions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a normal distribution from Tensorflow Distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the chosen distribution...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... or sample multiple times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain value of probability's density\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain value of logprobability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that this really is the log of the probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram, approximating the density\n",
    "\n",
    "plt.hist(normal.sample(10000), bins=50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the exponential distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bernoulli distribution (discrete)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A word of caution on discrete distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bernoulli prob and see that 0.5 and -1 do not give the correct probability!\n",
    "\n",
    "for k in [0,0.5,1,-1]:\n",
    "    print('prob result {} for k = {} '.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the scores to see what is occurring under the hood\n",
    "\n",
    "def my_bernoulli(p_success, k):\n",
    "    return np.power(p_success,k)*np.power((1-p_success),(1-k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate it as before\n",
    "\n",
    "for k in [0,0.5,1,-1]:\n",
    "    print('prob result {} for k = {} '.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with batch distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batched Bernoulli distribution\n",
    "\n",
    "bernoulli_batch = tfd.Bernoulli(probs=[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "bernoulli_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from it, noting the shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a batch shape with higher rank\n",
    "\n",
    "probs = [[[0.5, 0.5], \n",
    "          [0.8, 0.3], \n",
    "          [0.25, 0.75]]]\n",
    "bernoulli_batch_2D = tfd.Bernoulli(probs=probs)\n",
    "bernoulli_batch_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from this batch of distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine probabilities from this batch distribution\n",
    "\n",
    "bernoulli_batch_2D.prob([[[1, 0], \n",
    "                         [0, 0], \n",
    "                         [1, 1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='multivariate_distributions'></a>\n",
    "## Multivariate Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic multivariate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 2D multivariate Gaussian with diagonal covariance matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot\n",
    "\n",
    "plt_sample = normal_diag.sample(10000)\n",
    "plt.scatter(plt_sample[:, 0], plt_sample[:, 1], marker='.', alpha=0.05)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batches of multivariate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three \"batches\" of multivariate normals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample for a plot -- notice the shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from the batched multivariate Gaussian\n",
    "\n",
    "fig, axs = (plt.subplots(1, 3, sharex=True, sharey=True, figsize=(10, 3)))\n",
    "titles = ['cov_diag=[1, 2]','cov_diag=[2, 1]', 'cov_diag=[2, 2]']\n",
    "\n",
    "for i, (ax, title) in enumerate(zip(axs,titles)):\n",
    "    samples = plt_sample_batch[:,i,:] #take the ith batch [samples x event_shape]\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], marker='.', alpha=0.05)\n",
    "    ax.set_title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***\n",
    "<a id='the_independent_distribution'></a>\n",
    "## The Independent Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by defining a batch of two univariate Gaussians, then\n",
    "# combine them into a bivariate Gaussian with independent components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Univariate density functions\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "t = np.linspace(-4, 4, 10000)\n",
    "densities = batch_of_normals.prob(np.repeat(t[:, np.newaxis], 2, axis=1)) # each column is a vector of densities for one distn\n",
    "\n",
    "sns.lineplot(t, densities[:, 0], label='loc={}, scale={}'.format(locs[0], scales[0]))\n",
    "sns.lineplot(t, densities[:, 1], label='loc={}, scale={}'.format(locs[1], scales[1]))\n",
    "plt.ylabel('Probability density')\n",
    "plt.xlabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check their batch_shape and event_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Independent to convert the batch shape to the event shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that dimension from batch_shape has shifted to event_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot showing joint density contours and marginal density functions\n",
    "\n",
    "samples = bivariate_normal_from_Independent.sample(10000)\n",
    "x1 = samples[:, 0]\n",
    "x2 = samples[:, 1]\n",
    "sns.jointplot(x1, x2, kind=\"kde\", space=0, color='b', xlim=[-4, 4], ylim=[-4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MultivariateNormalDiag to create the equivalent distribution\n",
    "# Note that diagonal covariance matrix => no correlation => independence (for the multivariate normal distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the joint density function of bivariate_normal_from_Independent\n",
    "# Refer back to bivariate_normal_from_Independent to show that the plot is the same\n",
    "#Â Summarise how Independent has been used\n",
    "\n",
    "samples = bivariate_normal_from_Multivariate.sample(10000)\n",
    "x1 = samples[:, 0]\n",
    "x2 = samples[:, 1]\n",
    "sns.jointplot(x1, x2, kind=\"kde\", space=0, color='b', xlim=[-4, 4], ylim=[-4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shifting batch dimensions to event dimensions using \n",
    "`reinterpreted_batch_ndims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Demonstrate use of reinterpreted_batch_ndims\n",
    "# By default all batch dims except the first are transferred to event dims\n",
    "\n",
    "loc_grid = [[-100., -100.],\n",
    "            [100., 100.],\n",
    "            [0., 0.]]\n",
    "scale_grid = [[1., 10.],\n",
    "              [1., 10.],\n",
    "              [1., 1.]]\n",
    "\n",
    "normals_batch_3by2_event_1 = tfd.Normal(loc=loc_grid, scale=scale_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight batch_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have a batch of 3 bivariate normal distributions,\n",
    "# each parametrised by a column of our original parameter grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate log_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Can reinterpret _all_ batch dimensions as event dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take log_probs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `Independent` to build a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to `newsgroups` data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, just load the dataset, fetch train/test splits, probably choose a subset of the data.\n",
    "\n",
    "Construct the class conditional feature distribution (with Independent, using the Naive Bayes assumption) and sample from it.\n",
    "\n",
    "We can just use the ML estimates for parameters, in later tutorials we will learn them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for retrieving the 20 newsgroups data set\n",
    "\n",
    "# Usenet was a forerunner to modern internet forums\n",
    "#Â Users could post and read articles\n",
    "# Newsgroup corresponded to a topic\n",
    "# Example topics in this data set: IBM computer hardware, baseball\n",
    "# Our objective is to use an article's contents to predict its newsgroup,\n",
    "# a 20-class classification problem.\n",
    "\n",
    "#Â 18000 newsgroups, posts on 20 topics\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â More information about the data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example article\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associated label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing boilerplate\n",
    "\n",
    "n_documents = len(newsgroups_data['data'])\n",
    "\n",
    "count_vectorizer = CountVectorizer(input='content', binary=True,\n",
    "                                   max_df=0.25, min_df=1.01/n_documents) # ignore common words, words that appear once\n",
    "binary_bag_of_words = count_vectorizer.fit_transform(newsgroups_data['data']) # input is a list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the fit has been successful\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict that will be useful later \n",
    "\n",
    "inv_vocabulary = {value:key for key, value in count_vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Naive Bayes classifier for `newsgroup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature vector $x$ is a list of indicators for whether a word appears in the article. $x_i$ is 1 if the $i$th word appears, and 0 otherwise. `inv_vocabulary` matches word indices $i$ to words.\n",
    "\n",
    "Each label $y$ is a value in $0, 1, \\ldots, 19$.\n",
    "\n",
    "The parts of a naive Bayes classifier for this problem can be summarised as:  \n",
    "\n",
    "\n",
    "- A probability distribution for the feature vector by class, $p(x|y = j)$ for each $j = 0, 1, \\ldots, 19$. These probability distributions are assumed to have independent components: we can factorize the joint probability as a product of marginal probabilities\n",
    "\\begin{equation}\n",
    "    p(x|y = j) = \\prod_{i=1}^d p(x_i|y = j)\n",
    "\\end{equation}\n",
    "These marginal probability distributions are Bernoulli distributions, each of which has a single parameter $\\theta_{ji} := p(x_i = 1|y = j)$. This parameter is the probability of observing word $i$ in an article of class $j$. \n",
    "\n",
    "- We will use the Laplace smoothed maximum likelihood estimate to compute these parameters. Laplace smoothing involves adding small counts to every feature for each class. Else, if a feature did not appear in the training set of a class, but then we observed it in our test data the log probability would be undefined.\n",
    "\n",
    "- A collection of class prior probabilities $p(y = j)$. These will be set by computing the class base rates in the training set.  \n",
    "\n",
    "\n",
    "- A function for computing the probability of class membership via Bayes' theorem:  \n",
    "\n",
    "\\begin{equation}\n",
    "    p(y = j|x) = \\frac{p(x|y = j)p(y = j)}{p(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the parameter estimates (adjusted fraction of documents in class that contain word)\n",
    "\n",
    "n_classes = newsgroups_data['target'].max() + 1\n",
    "y = newsgroups_data['target']\n",
    "n_words = binary_bag_of_words.shape[1]\n",
    "\n",
    "alpha = 1e-6 # parameters for Laplace smoothing\n",
    "\n",
    "theta = np.zeros([n_classes, n_words]) # stores parameter values - prob. word given class\n",
    "for c_k in range(n_classes): # 0, 1, ..., 19\n",
    "    class_mask = (y == c_k)\n",
    "    N = class_mask.sum() # number of articles in class\n",
    "    theta[c_k, :] = (binary_bag_of_words[class_mask, :].sum(axis=0) + alpha)/(N + alpha*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the most probable word in each class is reasonable\n",
    "\n",
    "most_probable_word_ix = theta.argmax(axis=1) # most probable word for each class\n",
    "\n",
    "for j, ix in enumerate(most_probable_word_ix):\n",
    "    print('Most probable word in class {} is \"{}\".'.format(newsgroups_data['target_names'][j],\n",
    "                                                           inv_vocabulary[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a distribution for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of words from each class\n",
    "\n",
    "n_samples = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a class\n",
    "\n",
    "chosen_class = 15\n",
    "newsgroups_data['target_names'][chosen_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicators for words that appear in the sample\n",
    "\n",
    "class_sample = sample[:, chosen_class, :]\n",
    "class_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inverse transform to test quality of fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='sampling_and_log_probs'></a>\n",
    "## Sampling and log probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Multivariate Distribution\n",
    "\n",
    "normal_distributions = tfd.MultivariateNormalDiag(loc=[[0.5, 1], [0.1, 0], [0, 0.2]],\n",
    "                                 scale_diag=[[2, 3], [1, 3], [4, 4]])\n",
    "normal_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Normal batched Distribution\n",
    "# We are broadcasting batch shapes of `loc` and `scal_diag` \n",
    "# against each other\n",
    "\n",
    "loc = [[[0.3, 1.5, 1.], [0.2, 0.4, 2.8]],\n",
    "        [[2., 2.3, 8], [1.4, 1, 1.3]]]\n",
    "scale_diag = [0.4, 1., 0.7]\n",
    "normal_distributions = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\n",
    "normal_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use independent to move part of the batch shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw some samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `[B, E]` shaped input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `[E]` shaped input (broadcasting over batch size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#`[S, B, E]` shaped input (broadcasting over samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `[S, b, e]` shaped input, where [b, e] is broadcastable over [B, E]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes example\n",
    "\n",
    "Lets now use what we have learned and continue the Naive Bayes classifier we were building last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function get_data which:\n",
    "#   1) Fetches the 20 newsgroup dataset\n",
    "#   2) Performs a word count on the articles and binarizes the result\n",
    "#   3) Returns the data as a numpy matrix with the labels\n",
    "\n",
    "def get_data(categories):\n",
    "    \n",
    "    newsgroups_train_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                               subset='train', categories=categories)\n",
    "    newsgroups_test_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                              subset='test', categories=categories)\n",
    "\n",
    "    n_documents = len(newsgroups_train_data['data'])\n",
    "    count_vectorizer = CountVectorizer(input='content', binary=True,max_df=0.25, min_df=1.01/n_documents)\n",
    "    \n",
    "    train_binary_bag_of_words = count_vectorizer.fit_transform(newsgroups_train_data['data'])\n",
    "    test_binary_bag_of_words = count_vectorizer.transform(newsgroups_test_data['data']) \n",
    "\n",
    "    return (train_binary_bag_of_words.todense(), newsgroups_train_data['target']),  (test_binary_bag_of_words.todense(), newsgroups_test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to conduct Laplace smoothing. This adds a base level of probability for a given feature\n",
    "# to occur in every class.\n",
    "\n",
    "def laplace_smoothing(labels, binary_data, n_classes):\n",
    "    # Compute the parameter estimates (adjusted fraction of documents in class that contain word)\n",
    "    n_words = binary_data.shape[1]\n",
    "    alpha = 1 # parameters for Laplace smoothing\n",
    "    theta = np.zeros([n_classes, n_words]) # stores parameter values - prob. word given class\n",
    "    for c_k in range(n_classes): # 0, 1, ..., 19\n",
    "        class_mask = (labels == c_k)\n",
    "        N = class_mask.sum() # number of articles in class\n",
    "        theta[c_k, :] = (binary_data[class_mask, :].sum(axis=0) + alpha)/(N + alpha*2)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a subset of the 20 newsgroup dataset\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = get_data(categories=categories)\n",
    "smoothed_counts = laplace_smoothing(labels=train_labels, binary_data=train_data, n_classes=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now make our NB classifier we need to build three functions:\n",
    "* Compute the class priors\n",
    "* Build our class conditional distributions\n",
    "* Put it all together and classify our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which computes the prior probability of every class based on frequency of occurence in \n",
    "# the dataset\n",
    "\n",
    "def class_priors(n_classes, labels):\n",
    "    counts = np.zeros(n_classes)\n",
    "    for c_k in range(n_classes):\n",
    "        counts[c_k] = np.sum(np.where(labels==c_k, 1, 0))\n",
    "    priors = counts / np.sum(counts)\n",
    "    print('The class priors are {}'.format(priors))\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do a function that given the feature occurence counts returns a Bernoulli distribution of \n",
    "# batch_shape=number of classes and event_shape=number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final function predict_sample which given the distribution, a test sample, and the class priors:\n",
    "#   1) Computes the class conditional probabilities given the sample\n",
    "#   2) Forms the joint likelihood\n",
    "#   3) Normalises the joint likelihood and returns the log prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting one example from our test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over our test data and classify.\n",
    "\n",
    "probabilities = []\n",
    "for sample, label in zip(test_data, test_labels):\n",
    "    probabilities.append(tf.exp(predict_sample(tf_dist, sample, priors)))\n",
    "\n",
    "probabilities = np.asarray(probabilities)\n",
    "predicted_classes = np.argmax(probabilities, axis =-1)\n",
    "print('f1 ', f1_score(test_labels, predicted_classes, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Bernoulli Naive Bayes classifier using sklearn with the same level of alpha smoothing. \n",
    "\n",
    "clf = BernoulliNB(alpha=1)\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "print('f1 from sklean ', f1_score(test_labels, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='trainable_distributions'></a>\n",
    "## Trainable Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an exponential distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "plt.hist(exponential.sample(5000), bins=100, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an exponential distribution with a trainable rate parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the loss and gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize\n",
    "\n",
    "def exponential_dist_optimisation(data, distribution):\n",
    "\n",
    "    # Keep results for plotting\n",
    "    train_loss_results = []\n",
    "    train_rate_results = []\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "\n",
    "    num_steps = 10\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        print(\"Step {:03d}: Loss: {:.3f}: Rate: {:.3f}\".format())\n",
    "        \n",
    "    return train_loss_results, train_rate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data and train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted value for the rate parameter\n",
    "\n",
    "pred_value = exp_train.rate.numpy()\n",
    "exact_value = exponential.rate.numpy()\n",
    "\n",
    "print(\"Exact rate: \", exact_value)\n",
    "print(\"Pred rate:  \", pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to see the convergence of the estimated and true parameters\n",
    "\n",
    "tensor_exact_value = tf.constant(exact_value, shape=[len(train_rate_results)])\n",
    "\n",
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Convergence')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Rate\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_rate_results, label='trainable rate variable')\n",
    "axes[1].plot(tensor_exact_value, label='exact rate')\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function get_data which:\n",
    "#   1) Fetches the 20 newsgroup dataset\n",
    "#   2) Performs a word count on the articles and binarizes the result\n",
    "#   3) Returns the data as a numpy matrix with the labels\n",
    "\n",
    "def get_data(categories):\n",
    "\n",
    "    newsgroups_train_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                               subset='train', categories=categories)\n",
    "    newsgroups_test_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                              subset='test', categories=categories)\n",
    "\n",
    "    n_documents = len(newsgroups_train_data['data'])\n",
    "    count_vectorizer = CountVectorizer(input='content', binary=True,max_df=0.25, min_df=1.01/n_documents) \n",
    "    train_binary_bag_of_words = count_vectorizer.fit_transform(newsgroups_train_data['data']) \n",
    "    test_binary_bag_of_words = count_vectorizer.transform(newsgroups_test_data['data']) \n",
    "\n",
    "    return (train_binary_bag_of_words.todense(), newsgroups_train_data['target']),  (test_binary_bag_of_words.todense(), newsgroups_test_data['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to conduct laplace smoothing. This adds a base level of probability for a given feature\n",
    "# to occur in every class.\n",
    "\n",
    "def laplace_smoothing(labels, binary_data, n_classes):\n",
    "    # Compute the parameter estimates (adjusted fraction of documents in class that contain word)\n",
    "    n_words = binary_data.shape[1]\n",
    "    alpha = 1 # parameters for Laplace smoothing\n",
    "    theta = np.zeros([n_classes, n_words]) # stores parameter values - prob. word given class\n",
    "    for c_k in range(n_classes): # 0, 1, ..., 19\n",
    "        class_mask = (labels == c_k)\n",
    "        N = class_mask.sum() # number of articles in class\n",
    "        theta[c_k, :] = (binary_data[class_mask, :].sum(axis=0) + alpha)/(N + alpha*2)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do a function that given the feature occurence counts returns a Bernoulli distribution of \n",
    "# batch_shape=number of classes and event_shape=number of features.\n",
    "\n",
    "def make_distributions(probs):\n",
    "    batch_of_bernoullis = tfd.Bernoulli(probs=probs) #Â shape (n_classes, n_words)\n",
    "    dist = tfd.Independent(batch_of_bernoullis, reinterpreted_batch_ndims=1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which computes the prior probability of every class based on frequency of occurence in \n",
    "# the dataset\n",
    "\n",
    "def class_priors(n_classes, labels):\n",
    "    counts = np.zeros(n_classes)\n",
    "    for c_k in range(n_classes):\n",
    "        counts[c_k] = np.sum(np.where(labels==c_k, 1, 0))\n",
    "    priors = counts / np.sum(counts)\n",
    "    print('The class priors are {}'.format(priors))\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final function predict_sample which given the distribution, a test sample, and the class priors:\n",
    "#   1) Computes the class conditional probabilities given the sample\n",
    "#   2) Forms the joint likelihood\n",
    "#   3) Normalises the joint likelihood and returns the log prob\n",
    "\n",
    "def predict_sample(dist, sample, priors):\n",
    "    cond_probs = dist.log_prob(sample)\n",
    "    joint_likelihood = tf.add(np.log(priors), cond_probs)\n",
    "    norm_factor = tf.math.reduce_logsumexp(joint_likelihood, axis=-1, keepdims=True)\n",
    "    log_prob = joint_likelihood - norm_factor\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we learn the distribution using gradient tape\n",
    "\n",
    "def make_distribution_withGT(data, labels, nb_classes):\n",
    "\n",
    "    class_data = []\n",
    "    train_vars = []\n",
    "    distributions = []\n",
    "    for c in range(nb_classes):\n",
    "        train_vars.append(tf.Variable(initial_value=np.random.uniform(low=0.01, high =0.1, size=data.shape[-1])))\n",
    "        distributions.append(tfd.Bernoulli(probs=train_vars[c]))\n",
    "        class_mask = (labels == c)\n",
    "        class_data.append(data[class_mask, :])\n",
    "\n",
    "    for c_num in range(0,nb_classes):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        print('\\n%-------------------%')\n",
    "        print('Class ', c_num)\n",
    "        print('%-------------------%')\n",
    "\n",
    "        for i in range(0,100):\n",
    "            \n",
    "\n",
    "    dist = tfd.Bernoulli(probs=train_vars)\n",
    "    dist = tfd.Independent(dist,reinterpreted_batch_ndims=1)\n",
    "\n",
    "    print(dist)\n",
    "\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same Naive Bayes classifier we did last tutorial\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = get_data(categories)\n",
    "\n",
    "smoothed_counts = laplace_smoothing(labels=train_labels, binary_data=train_data, n_classes=len(categories))\n",
    "\n",
    "priors = class_priors(n_classes=len(categories), labels=train_labels)\n",
    "tf_dist = make_distributions(smoothed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the distributions with gradient tape\n",
    "\n",
    "GT_dist = make_distribution_withGT(data=train_data, labels=train_labels, nb_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two results\n",
    "\n",
    "for dist in [GT_dist,tf_dist]:\n",
    "    probabilities = []\n",
    "    for sample, label in zip(test_data, test_labels):\n",
    "        probabilities.append(predict_sample(dist, sample, priors))\n",
    "\n",
    "    probabilities = np.asarray(probabilities)\n",
    "    predicted_classes = np.argmax(probabilities, axis =-1)\n",
    "    print('f1 ', f1_score(test_labels, predicted_classes, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
