{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found a different version 0.1.0 of dataset imdb_reviews in data_dir /Users/shankar/tensorflow_datasets. Using currently defined version 1.0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/shankar/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
      "Shuffling and writing examples to /Users/shankar/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete49K60P/imdb_reviews-train.tfrecord\n",
      "Shuffling and writing examples to /Users/shankar/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete49K60P/imdb_reviews-test.tfrecord\n",
      "Shuffling and writing examples to /Users/shankar/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete49K60P/imdb_reviews-unsupervised.tfrecord\n",
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/shankar/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "splits = ['train[:60%]', 'train[-40%:]', 'test']\n",
    "\n",
    "splits, info = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    with_info=True,\n",
    "    split=splits,\n",
    "    as_supervised=True\n",
    ")\n",
    "train_data, validation_data, test_data = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has a total of:\n",
      "2 classes\n",
      "25000 movie reviews for training\n",
      "25000 movie reviews for testing\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "print('The dataset has a total of:')\n",
    "print(f\"{num_classes} classes\")\n",
    "print(f\"{num_train_examples} movie reviews for training\")\n",
    "print(f\"{num_test_examples} movie reviews for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "Label: negative\n"
     ]
    }
   ],
   "source": [
    "for review, label in train_data.take(1):\n",
    "    review = review.numpy()\n",
    "    label = label.numpy()\n",
    "    \n",
    "    print(f\"Movie Review: {review}\")\n",
    "    print(f\"Label: {class_names[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_batches = train_data.shuffle(num_train_examples // 4).batch(batch_size).prefetch(1)\n",
    "validation_batches = validation_data.batch(batch_size).prefetch(1)\n",
    "test_batches = test_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    hub_layer,\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 0.7922 - accuracy: 0.5173 - val_loss: 0.7088 - val_accuracy: 0.5608\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.6616 - accuracy: 0.6093 - val_loss: 0.6336 - val_accuracy: 0.6477\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.6063 - accuracy: 0.6791 - val_loss: 0.5895 - val_accuracy: 0.6912\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.5531 - accuracy: 0.7259 - val_loss: 0.5367 - val_accuracy: 0.7408\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.4998 - accuracy: 0.7740 - val_loss: 0.4906 - val_accuracy: 0.7760\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.4527 - accuracy: 0.8031 - val_loss: 0.4554 - val_accuracy: 0.7977\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.4123 - accuracy: 0.8265 - val_loss: 0.4244 - val_accuracy: 0.8140\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.3764 - accuracy: 0.8463 - val_loss: 0.3985 - val_accuracy: 0.8258\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.3446 - accuracy: 0.8624 - val_loss: 0.3780 - val_accuracy: 0.8377\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.3182 - accuracy: 0.8734 - val_loss: 0.3607 - val_accuracy: 0.8443\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.2925 - accuracy: 0.8884 - val_loss: 0.3486 - val_accuracy: 0.8490\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.2706 - accuracy: 0.8977 - val_loss: 0.3358 - val_accuracy: 0.8554\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.2511 - accuracy: 0.9059 - val_loss: 0.3280 - val_accuracy: 0.8584\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.2341 - accuracy: 0.9149 - val_loss: 0.3208 - val_accuracy: 0.8625\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.2172 - accuracy: 0.9215 - val_loss: 0.3174 - val_accuracy: 0.8641\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.2029 - accuracy: 0.9288 - val_loss: 0.3130 - val_accuracy: 0.8671\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.1886 - accuracy: 0.9339 - val_loss: 0.3088 - val_accuracy: 0.8699\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.1760 - accuracy: 0.9396 - val_loss: 0.3126 - val_accuracy: 0.8682\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.1645 - accuracy: 0.9434 - val_loss: 0.3093 - val_accuracy: 0.8700\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.1558 - accuracy: 0.9490 - val_loss: 0.3090 - val_accuracy: 0.8708\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=20,\n",
    "    validation_data=validation_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.324\n",
      "accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate(test_batches, verbose=0)\n",
    "for metric, value in zip(model.metrics_names, eval_results):\n",
    "    print(metric + ': {:.3}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
